{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Patrick/vllmenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-10-08 09:41:51,297\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import transformers\n",
    "import accelerate\n",
    "import vllm\n",
    "import bitsandbytes\n",
    "from vllm import LLM, SamplingParams\n",
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib\n",
    "from collections import Counter\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "#matplotlib.use('TkAgg')\n",
    "#from awq import AutoAWQForCausalLM\n",
    "#from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>How can I improve my time management skills?</td>\n",
       "      <td>generic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>What are the most effective ways to deal with ...</td>\n",
       "      <td>generic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>What are the main differences between Python a...</td>\n",
       "      <td>generic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>How can I increase my productivity while worki...</td>\n",
       "      <td>generic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Can you explain the basics of quantum computing?</td>\n",
       "      <td>generic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>76</td>\n",
       "      <td>Write a script for a YouTube video exploring t...</td>\n",
       "      <td>writing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>77</td>\n",
       "      <td>Compose an engaging travel blog post about a r...</td>\n",
       "      <td>writing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>78</td>\n",
       "      <td>Write a captivating movie review for a recentl...</td>\n",
       "      <td>writing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>79</td>\n",
       "      <td>Structure a podcast script for an episode disc...</td>\n",
       "      <td>writing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>80</td>\n",
       "      <td>Write a symphony concert review, discussing th...</td>\n",
       "      <td>writing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    question_id                                               text category\n",
       "0             1       How can I improve my time management skills?  generic\n",
       "1             2  What are the most effective ways to deal with ...  generic\n",
       "2             3  What are the main differences between Python a...  generic\n",
       "3             4  How can I increase my productivity while worki...  generic\n",
       "4             5   Can you explain the basics of quantum computing?  generic\n",
       "..          ...                                                ...      ...\n",
       "75           76  Write a script for a YouTube video exploring t...  writing\n",
       "76           77  Compose an engaging travel blog post about a r...  writing\n",
       "77           78  Write a captivating movie review for a recentl...  writing\n",
       "78           79  Structure a podcast script for an episode disc...  writing\n",
       "79           80  Write a symphony concert review, discussing th...  writing\n",
       "\n",
       "[80 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Multitask Benchmark datenset json\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "file_path = \"./projects/question.jsonl\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "    #for line in f:\n",
    "    #    data = json.loads(line)\n",
    "    #    print(data)\n",
    "df_mtconversation = pd.DataFrame(data)\n",
    "df_mtconversation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['generic', 'knowledge', 'roleplay', 'common-sense', 'fermi',\n",
       "       'counterfactual', 'coding', 'math', 'writing'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "file_path = \"./projects/question.jsonl\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "    #for line in f:\n",
    "    #    data = json.loads(line)\n",
    "    #    print(data)\n",
    "df_mtconversation = pd.DataFrame(data)\n",
    "df_mtconversation\n",
    "\n",
    "# Categories:\n",
    "print(df_mtconversation.category.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Develop a C++ program that reads a text file line by line and counts the number of occurrences of a specific word in the file.',\n",
       "       'Implement a Python function to find the longest common subsequence of two input strings using dynamic programming.',\n",
       "       'Implement a regular expression in Python to validate an email address.',\n",
       "       'Write a program to find the nth Fibonacci number using dynamic programming.',\n",
       "       'Implement a binary search algorithm to find a specific element in a sorted array.',\n",
       "       'Implement a queue data structure using two stacks in Python.',\n",
       "       'Implement a program to find the common elements in two arrays without using any extra data structures.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Coding texts\n",
    "\n",
    "coding_texts = df[df['category'] == 'coding']['text']\n",
    "coding_texts.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpu_power(gpu_index=1): #measuring for gpu1\n",
    "    \"\"\"Fetches the current power consumption of the GPU using nvidia-smi.\"\"\"\n",
    "    result = subprocess.run(['nvidia-smi', '--query-gpu=power.draw', '--format=csv,noheader,nounits', '-i',str(gpu_index)], \n",
    "                            stdout=subprocess.PIPE, text=True)\n",
    "    power = float(result.stdout.strip())  # Power in watts\n",
    "    return power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hier mit simplem MT_Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['generic' 'knowledge' 'roleplay' 'common-sense' 'fermi' 'counterfactual'\n",
      " 'coding' 'math' 'writing']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Develop a C++ program that reads a text file line by line and counts the number of occurrences of a specific word in the file.',\n",
       "       'Implement a Python function to find the longest common subsequence of two input strings using dynamic programming.',\n",
       "       'Implement a regular expression in Python to validate an email address.',\n",
       "       'Write a program to find the nth Fibonacci number using dynamic programming.',\n",
       "       'Implement a binary search algorithm to find a specific element in a sorted array.',\n",
       "       'Implement a queue data structure using two stacks in Python.',\n",
       "       'Implement a program to find the common elements in two arrays without using any extra data structures.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "file_path = \"./projects/question.jsonl\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "    #for line in f:\n",
    "    #    data = json.loads(line)\n",
    "    #    print(data)\n",
    "df_mtconversation = pd.DataFrame(data)\n",
    "df_mtconversation\n",
    "\n",
    "\n",
    "print(df_mtconversation.category.unique())\n",
    "coding_texts = df_mtconversation[df_mtconversation['category'] == 'coding']['text']\n",
    "coding_texts.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wie oft soll jeder einzelne Eingabeprompt genutzt werden? wichtig fÃ¼r Mean, std..\n",
    "\n",
    "bootstrapping = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MT_Bench mit normalem Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Patrick/vllmenv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing category: generic\n",
      "Processing category: knowledge\n",
      "Processing category: roleplay\n",
      "Processing category: common-sense\n",
      "Processing category: fermi\n",
      "Processing category: counterfactual\n",
      "Processing category: coding\n",
      "Processing category: math\n",
      "Processing category: writing\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\",\n",
    "                                             #\"tiiuae/falcon-mamba-7b\",\n",
    "                                             device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained (\"facebook/opt-125m\")\n",
    "\n",
    "\n",
    "def get_gpu_power(gpu_index=1): #measuring for gpu1\n",
    "    \"\"\"Fetches the current power consumption of the GPU using nvidia-smi.\"\"\"\n",
    "    result = subprocess.run(['nvidia-smi', '--query-gpu=power.draw', '--format=csv,noheader,nounits', '-i',str(gpu_index)], \n",
    "                            stdout=subprocess.PIPE, text=True)\n",
    "    power = float(result.stdout.strip())  # Power in watts\n",
    "    return power\n",
    "\n",
    "\n",
    "# Load dataset once and keep it ready for all experiments\n",
    "def load_dataset(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Filter dataset by category\n",
    "def filter_texts_by_category(df, category):\n",
    "    return df[df['category'] == category]['text'].values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# Run the bootstrapping experiment for each text in a given category\n",
    "def run_experiment_for_texts(texts, bootstrapping):\n",
    "\n",
    "\n",
    "    #metriken:\n",
    "    latencies = []\n",
    "    energy_per_token = []\n",
    "    throughputs= []\n",
    "    generated_texts= []\n",
    "\n",
    "\n",
    "    for i, text in enumerate(texts):\n",
    "        \"\"\"\n",
    "        Texts: einzelne Prompts die zu einer Kategorie gehÃ¶ren: z.B. \n",
    "        coding\n",
    "\n",
    "        ['Develop a C++ program that reads a text file line by line and counts the number of occurrences of a specific word in the file.',\n",
    "       'Implement a Python function to find the longest common subsequence of two input strings using dynamic programming.',\n",
    "       'Implement a regular expression in Python to validate an email address.',\n",
    "       'Write a program to find the nth Fibonacci number using dynamic programming.',\n",
    "       'Implement a binary search algorithm to find a specific element in a sorted array.',\n",
    "       'Implement a queue data structure using two stacks in Python.',\n",
    "       'Implement a program to find the common elements in two arrays without using any extra data structures.']\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\") #einzelner prompt \n",
    "        \n",
    "        text_latencies = []                                 #latency fÃ¼r einen prompt der 10x durchlaufen wird   ------ darÃ¼ber am ende Mean berechnen\n",
    "        text_energy_per_token = []                          #energy per token eines einzelnen prompts der 10x durchlaufen wird ------ darÃ¼ber am ende Mean berechnen\n",
    "        text_throughput = []                                  #throughput ------ darÃ¼ber am ende Mean berechnen\n",
    "        text_generated = []                                   #generated text------ darÃ¼ber am ende Mean berechnen\n",
    "\n",
    "        for _ in range(bootstrapping):                           # einzelner prompt wird 10 mal durchlaufen\n",
    "            power_start = get_gpu_power()                       #power consumption\n",
    "            start_time = time.time()                            #start time\n",
    "\n",
    "            # Generate output from the model\n",
    "            output = model.generate(inputs['input_ids'], max_new_tokens=200, do_sample=True) #generating output\n",
    "\n",
    "            end_time = time.time()                              #end time\n",
    "            power_end = get_gpu_power()                        #power consumption           \n",
    "\n",
    "            # Measure latency\n",
    "            latency = end_time - start_time\n",
    "            text_latencies.append(latency)\n",
    "\n",
    "\n",
    "            # Calculate energy consumption (average power * time)\n",
    "            avg_power = (power_start + power_end) / 2\n",
    "            energy = avg_power * latency\n",
    "            output_tokens = len(output[0])\n",
    "            energy_token = energy / output_tokens if output_tokens > 0 else 0\n",
    "            text_energy_per_token.append(energy_token)\n",
    "\n",
    "\n",
    "            #througput\n",
    "            throughput = output_tokens / latency\n",
    "            text_throughput.append(throughput)\n",
    "\n",
    "            # Decode the generated output text\n",
    "            generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            # Filter out the repetitive question from the generated text\n",
    "            filtered_generated_text = generated_text.replace(text, \"\").strip()\n",
    "            text_generated.append(filtered_generated_text)\n",
    "\n",
    "\n",
    "        latencies.append(text_latencies)\n",
    "        energy_per_token.append(text_energy_per_token)\n",
    "        throughputs.append(text_throughput)\n",
    "        generated_texts.append(text_generated)\n",
    "\n",
    "    return latencies, energy_per_token, throughputs, generated_texts\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Store and collect metrics for each category\n",
    "def collect_metrics_for_categories(df, categories, bootstrapping):\n",
    "    category_metrics = {}\n",
    "\n",
    "    for category in categories:                                                              #fÃ¼r jede Kategorie         \n",
    "        print(f\"Processing category: {category}\")\n",
    "        texts = filter_texts_by_category(df, category)                                      #df nach Kategorie filtern\n",
    "        latencies, energy_per_token, throughputs, generated_texts= run_experiment_for_texts(texts, bootstrapping)    #hier wird die funktion run_experiment_for_texts aufgerufen\n",
    "\n",
    "        # Store metrics for each category\n",
    "        category_metrics[category] = {\n",
    "            \"latencies\": latencies,\n",
    "            \"energy_per_token\": energy_per_token,\n",
    "            \"throughput\" : throughputs,\n",
    "            \"generated_texts\": generated_texts\n",
    "        }\n",
    "\n",
    "    return category_metrics\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"./projects/question.jsonl\"\n",
    "bootstrapping = 10  # Number of iterations for each prompt\n",
    "\n",
    "df_mtconversation = load_dataset(file_path)\n",
    "\n",
    "# Define the categories to test\n",
    "categories = ['generic', 'knowledge', 'roleplay', 'common-sense', 'fermi', 'counterfactual', 'coding', 'math', 'writing']   #sind alle kategorien die es gibt im MT_Bench datenset\n",
    "\n",
    "# Collect metrics for each category\n",
    "metrics = collect_metrics_for_categories(df_mtconversation, categories, bootstrapping)\n",
    "\n",
    "# Plot results for comparison\n",
    "#plot_energy_vs_latency(metrics, categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the energy consumption per token comparison\n",
    "def plot_energy_vs_latency(metrics, categories):\n",
    "    for category in categories:\n",
    "        category_data = metrics[category]\n",
    "        energy_per_token = category_data[\"energy_per_token\"]\n",
    "        latencies = category_data[\"latencies\"]\n",
    "        response_sim = category_data[\"response_similiarity\"]\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "\n",
    "        # Plot energy per token\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot([sum(x)/len(x) for x in energy_per_token], marker='o', color='blue', label='Energy per Token (J)')\n",
    "        plt.title(f\"Energy per Token for {category}\")\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Energy (J)')\n",
    "        plt.legend()\n",
    "\n",
    "        # Plot latencies\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot([sum(x)/len(x) for x in latencies], marker='o', color='green', label='latency')\n",
    "        plt.title(f\"latency for {category}\")\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('simScore')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_energy_vs_latency(metrics, categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MT_Bench mit quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Patrick/vllmenv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",  # Specify the quantization type\n",
    "    bnb_4bit_use_double_quant=True,  # Use double quantization if needed\n",
    "    bnb_4bit_compute_dtype=torch.float16  # Specify computation dtype\n",
    ")\n",
    "\n",
    "\n",
    "# Configure 4-bit quantization\n",
    "#quant_config = { \"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\" }\n",
    "# Load the model and tokenizer with quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\",\n",
    "                                             #\"tiiuae/falcon-mamba-7b\",\n",
    "                                             quantization_config=quant_config,\n",
    "                                             device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained (\"facebook/opt-125m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing category: generic\n",
      "Processing category: knowledge\n",
      "Processing category: roleplay\n",
      "Processing category: common-sense\n",
      "Processing category: fermi\n",
      "Processing category: counterfactual\n",
      "Processing category: coding\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "# Load a pre-trained sentence transformer model\n",
    "modelSimiliarity = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "def calculate_semantic_similarity(response_1, response_2):\n",
    "    # Get embeddings for both responses\n",
    "    embedding_1 = modelSimiliarity.encode(response_1, convert_to_tensor=True)\n",
    "    embedding_2 = modelSimiliarity.encode(response_2, convert_to_tensor=True)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    cosine_sim = util.pytorch_cos_sim(embedding_1, embedding_2)\n",
    "    return cosine_sim.item()\n",
    "\n",
    "def get_gpu_power(gpu_index=1): #measuring for gpu1\n",
    "    \"\"\"Fetches the current power consumption of the GPU using nvidia-smi.\"\"\"\n",
    "    result = subprocess.run(['nvidia-smi', '--query-gpu=power.draw', '--format=csv,noheader,nounits', '-i',str(gpu_index)], \n",
    "                            stdout=subprocess.PIPE, text=True)\n",
    "    power = float(result.stdout.strip())  # Power in watts\n",
    "    return power\n",
    "\n",
    "\n",
    "# Load dataset once and keep it ready for all experiments\n",
    "def load_dataset(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Filter dataset by category\n",
    "def filter_texts_by_category(df, category):\n",
    "    return df[df['category'] == category]['text'].values\n",
    "\n",
    "# Run the bootstrapping experiment for each text in a given category\n",
    "def run_experiment_for_texts(texts, bootstrapping):\n",
    "    latencies = []\n",
    "    energy_per_token = []\n",
    "    throughputs= []\n",
    "    generated_texts= []\n",
    "\n",
    "    for i, text in enumerate(texts):\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "        \n",
    "        text_latencies = []\n",
    "        text_energy_per_token = []\n",
    "        text_throughput = []\n",
    "        text_generated = []\n",
    "        response_similiarity = []\n",
    "\n",
    "        for _ in range(bootstrapping):\n",
    "            power_start = get_gpu_power()\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Generate output from the model\n",
    "            output = model.generate(inputs['input_ids'], max_new_tokens=200, do_sample=True)\n",
    "\n",
    "            end_time = time.time()\n",
    "            power_end = get_gpu_power()\n",
    "\n",
    "            # Measure latency\n",
    "            latency = end_time - start_time\n",
    "            text_latencies.append(latency)\n",
    "\n",
    "\n",
    "            # Calculate energy consumption (average power * time)\n",
    "            avg_power = (power_start + power_end) / 2\n",
    "            energy = avg_power * latency\n",
    "            output_tokens = len(output[0])\n",
    "            energy_token = energy / output_tokens if output_tokens > 0 else 0\n",
    "            text_energy_per_token.append(energy_token)\n",
    "\n",
    "\n",
    "            #througput\n",
    "            throughput = output_tokens / latency\n",
    "            text_throughput.append(throughput)\n",
    "\n",
    "            # Decode the generated output text\n",
    "            generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            # Filter out the repetitive question from the generated text\n",
    "            filtered_generated_text = generated_text.replace(text, \"\").strip()\n",
    "            text_generated.append(filtered_generated_text)\n",
    "\n",
    "        # measuring the similiarity    \n",
    "        n = len(text_generated)\n",
    "        similarity_scores = []  # To store similarity scores\n",
    "\n",
    "        for i in range(n - 1):  # Outer loop goes up to n-2\n",
    "            for j in range(i + 1, n):  # Inner loop starts from i+1 and goes to n-1\n",
    "                similarity = calculate_semantic_similarity(text_generated[i], text_generated[j])\n",
    "                similarity_scores.append(similarity)  # Store only the pairwise similarity scores\n",
    "\n",
    "        # Step 2: Calculate average and variance of similarities\n",
    "        average_similarity_Encoder = np.mean(similarity_scores)\n",
    "        std_similarity_Encoder = np.std(similarity_scores)\n",
    "        response_similiarity.append((average_similarity_Encoder, std_similarity_Encoder))\n",
    "\n",
    "\n",
    "        latencies.append(text_latencies)\n",
    "        energy_per_token.append(text_energy_per_token)\n",
    "        throughputs.append(text_throughput)\n",
    "        generated_texts.append(text_generated)\n",
    "\n",
    "    return latencies, energy_per_token, throughputs, generated_texts, response_similiarity\n",
    "\n",
    "# Store and collect metrics for each category\n",
    "def collect_metrics_for_categories(df, categories, bootstrapping):\n",
    "    category_metrics = {}\n",
    "\n",
    "    for category in categories:\n",
    "        print(f\"Processing category: {category}\")\n",
    "        texts = filter_texts_by_category(df, category)\n",
    "        latencies, energy_per_token, throughputs, generated_texts, response_similiarity = run_experiment_for_texts(texts, bootstrapping)\n",
    "\n",
    "        # Store metrics for each category\n",
    "        category_metrics[category] = {\n",
    "            \"latencies\": latencies,\n",
    "            \"energy_per_token\": energy_per_token,\n",
    "            \"throughput\" : throughputs,\n",
    "            \"generated_texts\": generated_texts,\n",
    "            \"response_similiarity\": response_similiarity\n",
    "        }\n",
    "\n",
    "    return category_metrics\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"./projects/question.jsonl\"\n",
    "bootstrapping = 10  # Number of iterations for each prompt\n",
    "\n",
    "df_mtconversation = load_dataset(file_path)\n",
    "\n",
    "# Define the categories to test\n",
    "categories = ['generic', 'knowledge', 'roleplay', 'common-sense', 'fermi', 'counterfactual', 'coding', 'math', 'writing']\n",
    "\n",
    "# Collect metrics for each category\n",
    "metricsquant = collect_metrics_for_categories(df_mtconversation, categories, bootstrapping)\n",
    "\n",
    "# Plot results for comparison\n",
    "#plot_energy_vs_latency(metrics, categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the energy consumption per token comparison\n",
    "def plot_energy_vs_latency(metrics, categories):\n",
    "    for category in categories:\n",
    "        category_data = metrics[category]\n",
    "        energy_per_token = category_data[\"energy_per_token\"]\n",
    "        latencies = category_data[\"latencies\"]\n",
    "        response_sim = category_data[\"response_similiarity\"]\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "\n",
    "        # Plot energy per token\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot([sum(x)/len(x) for x in energy_per_token], marker='o', color='blue', label='Energy per Token (J)')\n",
    "        plt.title(f\"Energy per Token for {category}\")\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Energy (J)')\n",
    "        plt.legend()\n",
    "\n",
    "        # Plot latencies\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot([sum(x)/len(x) for x in latencies], marker='o', color='green', label='latency')\n",
    "        plt.title(f\"latency for {category}\")\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('simScore')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MT_Bench with vLLM\n",
    "\n",
    "\n",
    "### Das funktioniert noch nicht mit vLLM!!!  out of memory error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Patrick/vllmenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-09-26 16:22:31,750\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import transformers\n",
    "import accelerate\n",
    "import vllm\n",
    "import bitsandbytes\n",
    "from vllm import LLM, SamplingParams\n",
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib\n",
    "from collections import Counter\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "#matplotlib.use('TkAgg')\n",
    "#from awq import AutoAWQForCausalLM\n",
    "#from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Patrick/vllmenv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-26 16:22:36 llm_engine.py:223] Initializing an LLM engine (v0.6.1.post2) with config: model='facebook/opt-125m', speculative_config=None, tokenizer='facebook/opt-125m', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=facebook/opt-125m, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)\n",
      "INFO 09-26 16:22:37 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 09-26 16:22:37 selector.py:116] Using XFormers backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Patrick/vllmenv/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/home/Patrick/vllmenv/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/opt-125m\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#llm = LLM(\"tiiuae/falcon-7b\",dtype = torch.float16 ,trust_remote_code=True)\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfacebook/opt-125m\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Prepare sampling parameters and prompt\u001b[39;00m\n\u001b[1;32m     13\u001b[0m sampling_params \u001b[38;5;241m=\u001b[39m SamplingParams(temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m)\n",
      "File \u001b[0;32m~/vllmenv/lib/python3.10/site-packages/vllm/entrypoints/llm.py:178\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_context_len_to_capture, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere is no need to pass vision-related arguments anymore.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    156\u001b[0m engine_args \u001b[38;5;241m=\u001b[39m EngineArgs(\n\u001b[1;32m    157\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    158\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    177\u001b[0m )\n\u001b[0;32m--> 178\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m \u001b[43mLLMEngine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUsageContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[0;32m~/vllmenv/lib/python3.10/site-packages/vllm/engine/llm_engine.py:550\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    548\u001b[0m executor_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_executor_cls(engine_config)\n\u001b[1;32m    549\u001b[0m \u001b[38;5;66;03m# Create the LLM engine.\u001b[39;00m\n\u001b[0;32m--> 550\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mengine_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m engine\n",
      "File \u001b[0;32m~/vllmenv/lib/python3.10/site-packages/vllm/engine/llm_engine.py:317\u001b[0m, in \u001b[0;36mLLMEngine.__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, load_config, lora_config, speculative_config, decoding_config, observability_config, prompt_adapter_config, executor_class, log_stats, usage_context, stat_loggers, input_registry)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_registry \u001b[38;5;241m=\u001b[39m input_registry\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_processor \u001b[38;5;241m=\u001b[39m input_registry\u001b[38;5;241m.\u001b[39mcreate_input_processor(\n\u001b[1;32m    315\u001b[0m     model_config)\n\u001b[0;32m--> 317\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_executor \u001b[38;5;241m=\u001b[39m \u001b[43mexecutor_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspeculative_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspeculative_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_adapter_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_adapter_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobservability_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservability_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39membedding_mode:\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_kv_caches()\n",
      "File \u001b[0;32m~/vllmenv/lib/python3.10/site-packages/vllm/executor/executor_base.py:47\u001b[0m, in \u001b[0;36mExecutorBase.__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, load_config, lora_config, speculative_config, prompt_adapter_config, observability_config)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt_adapter_config \u001b[38;5;241m=\u001b[39m prompt_adapter_config\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservability_config \u001b[38;5;241m=\u001b[39m observability_config\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_executor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vllmenv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py:39\u001b[0m, in \u001b[0;36mGPUExecutor._init_executor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_config\u001b[38;5;241m.\u001b[39mworld_size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, (\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPUExecutor only supports single GPU.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_worker \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_worker()\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_worker\u001b[38;5;241m.\u001b[39mload_model()\n",
      "File \u001b[0;32m~/vllmenv/lib/python3.10/site-packages/vllm/worker/worker.py:171\u001b[0m, in \u001b[0;36mWorker.init_device\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    169\u001b[0m     gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m    170\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m--> 171\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_gpu_memory \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmem_get_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    174\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot support device type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_config\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/vllmenv/lib/python3.10/site-packages/torch/cuda/memory.py:685\u001b[0m, in \u001b[0;36mmem_get_info\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    683\u001b[0m     device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mcurrent_device()\n\u001b[1;32m    684\u001b[0m device \u001b[38;5;241m=\u001b[39m _get_device_index(device)\n\u001b[0;32m--> 685\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudaMemGetInfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def get_gpu_power(gpu_index=1): #measuring for gpu1\n",
    "    \"\"\"Fetches the current power consumption of the GPU using nvidia-smi.\"\"\"\n",
    "    result = subprocess.run(['nvidia-smi', '--query-gpu=power.draw', '--format=csv,noheader,nounits', '-i',str(gpu_index)], \n",
    "                            stdout=subprocess.PIPE, text=True)\n",
    "    power = float(result.stdout.strip())  # Power in watts\n",
    "    return power\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained (\"facebook/opt-125m\")\n",
    "#llm = LLM(\"tiiuae/falcon-7b\",dtype = torch.float16 ,trust_remote_code=True)\n",
    "llm = LLM(\"facebook/opt-125m\",trust_remote_code=True)\n",
    "# Prepare sampling parameters and prompt\n",
    "sampling_params = SamplingParams(temperature=0.5, max_tokens=200)\n",
    "#prompt = \"Generate a python code that accepts a list of numbers and returns the sum.\"\n",
    "\n",
    "\n",
    "\n",
    "def get_gpu_power(gpu_index=1): #measuring for gpu1\n",
    "    \"\"\"Fetches the current power consumption of the GPU using nvidia-smi.\"\"\"\n",
    "    result = subprocess.run(['nvidia-smi', '--query-gpu=power.draw', '--format=csv,noheader,nounits', '-i',str(gpu_index)], \n",
    "                            stdout=subprocess.PIPE, text=True)\n",
    "    power = float(result.stdout.strip())  # Power in watts\n",
    "    return power\n",
    "\n",
    "\n",
    "# Load dataset once and keep it ready for all experiments\n",
    "def load_dataset(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Filter dataset by category\n",
    "def filter_texts_by_category(df, category):\n",
    "    return df[df['category'] == category]['text'].values\n",
    "\n",
    "# Run the bootstrapping experiment for each text in a given category\n",
    "def run_experiment_for_texts(texts, bootstrapping):\n",
    "    latencies = []\n",
    "    energy_per_token = []\n",
    "    throughputs= []\n",
    "    generated_texts= []\n",
    "\n",
    "    for i, text in enumerate(texts):\n",
    "        #inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "        \n",
    "        text_latencies = []\n",
    "        text_energy_per_token = []\n",
    "        text_throughput = []\n",
    "        text_generated = []\n",
    "\n",
    "\n",
    "        for _ in range(bootstrapping):\n",
    "            power_start = get_gpu_power()\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "            #prompts = inputs['input_ids'].tolist()\n",
    "            # Generate output from the model\n",
    "            response = llm.generate(text, sampling_params)\n",
    "\n",
    "\n",
    "\n",
    "            end_time = time.time()\n",
    "            power_end = get_gpu_power()\n",
    "            # Measure latency\n",
    "            latency = end_time - start_time\n",
    "            text_latencies.append(latency)\n",
    "\n",
    "\n",
    "            # Calculate energy consumption (average power * time)\n",
    "            avg_power = (power_start + power_end) / 2\n",
    "            energy = avg_power * latency\n",
    "            output_tokens = len(output[0])\n",
    "            energy_token = energy / output_tokens if output_tokens > 0 else 0\n",
    "            text_energy_per_token.append(energy_token)\n",
    "\n",
    "\n",
    "            #througput\n",
    "            throughput = output_tokens / latency\n",
    "            text_throughput.append(throughput)\n",
    "\n",
    "            # Decode the generated output text\n",
    "            generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            # Filter out the repetitive question from the generated text\n",
    "            filtered_generated_text = generated_text.replace(text, \"\").strip()\n",
    "            text_generated.append(filtered_generated_text)\n",
    "\n",
    "        latencies.append(text_latencies)\n",
    "        energy_per_token.append(text_energy_per_token)\n",
    "        throughputs.append(text_throughput)\n",
    "        generated_texts.append(text_generated)\n",
    "\n",
    "    return latencies, energy_per_token, throughputs, generated_texts\n",
    "\n",
    "# Store and collect metrics for each category\n",
    "def collect_metrics_for_categories(df, categories, bootstrapping):\n",
    "    category_metrics = {}\n",
    "\n",
    "    for category in categories:\n",
    "        print(f\"Processing category: {category}\")\n",
    "        texts = filter_texts_by_category(df, category)\n",
    "        latencies, energy_per_token, throughputs, generated_texts = run_experiment_for_texts(texts, bootstrapping)\n",
    "\n",
    "        # Store metrics for each category\n",
    "        category_metrics[category] = {\n",
    "            \"latencies\": latencies,\n",
    "            \"energy_per_token\": energy_per_token,\n",
    "            \"throughput\" : throughputs,\n",
    "            \"generated_texts\": generated_texts\n",
    "        }\n",
    "\n",
    "    return category_metrics\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"./projects/question.jsonl\"\n",
    "bootstrapping = 10  # Number of iterations for each prompt\n",
    "\n",
    "df_mtconversation = load_dataset(file_path)\n",
    "\n",
    "# Define the categories to test\n",
    "categories = ['generic', 'knowledge', 'roleplay', 'common-sense', 'fermi', 'counterfactual', 'coding', 'math', 'writing']\n",
    "\n",
    "# Collect metrics for each category\n",
    "metricsquant = collect_metrics_for_categories(df_mtconversation, categories, bootstrapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MMLU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Science, Technology, Engineering, Mathematics = stem\n",
    "stem = [\"clinical_knowledge\",\n",
    "\"medical_genetics\", \n",
    "\"high_school_physics\",\n",
    "\"virology\",\n",
    "\"high_school_biology\",\n",
    "\"abstract_algebra\",\n",
    "\"professional_medicine\",\n",
    "\"nutrition\",\n",
    "\"machine_learning\",\n",
    "\"anatomy\",\n",
    "\"college_medicine\",\n",
    "\"college_chemistry\",\n",
    "\"elementary_mathematics\",\n",
    "\"human_aging\",\n",
    "\"college_mathematics\",\n",
    "\"high_school_statistics\",\n",
    "\"high_school_mathematics\",\n",
    "\"high_school_computer_science\",\n",
    "\"conceptual_physics\",\n",
    "\"high_school_chemistry\",\n",
    "\"college_physics\",\n",
    "\"electrical_engineering\",\n",
    "\"astronomy\",\n",
    "\"college_biology\",\n",
    "\"computer_security\"]\n",
    "\n",
    "humanities= [\"high_school_european_history\",\n",
    "\"high_school_us_history\",\n",
    "\"high_school_world_history\",\n",
    "\"philosophy\",\n",
    "\"global_facts\",\n",
    "\"security_studies\",\n",
    "\"prehistory\",\n",
    "\"high_school_government_and_politics\",\n",
    "\"logical_fallacies\",\n",
    "\"international_law\",\n",
    "\"jurisprudence\",\n",
    "\"world_religions\",\n",
    "\"us_foreign_policy\",\n",
    "\"moral_scenarios\",\n",
    "\"moral_disputes\"\n",
    "]\n",
    "\n",
    "sociology = [\"sociology\",\n",
    "\"professional_psychology\",\n",
    "\"high_school_psychology\",\n",
    "\"human_sexuality\"]\n",
    "\n",
    "others = [\"business_ethics\",\n",
    "\"high_school_microeconomics\",\n",
    "\"econometrics\",\n",
    "\"professional_accounting\",\n",
    "\"public_relations\",\n",
    "\"marketing\",\n",
    "\"professional_law\",\n",
    "\"management\",\n",
    "\"miscellaneous\",\n",
    "\"high_school_macroeconomics\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "def convert_to_dataframe(categories):\n",
    "    category_dataframes = {}  # Dictionary to store DataFrames for each category\n",
    "    \n",
    "    for category in categories:\n",
    "        print(\"Loading Data for category: \", category)\n",
    "        \n",
    "        # Load the dataset for the given category\n",
    "        mmlu_dataset = load_dataset(\"lukaemon/mmlu\", category, split='validation', trust_remote_code=True)\n",
    "        \n",
    "        # Create a DataFrame for the current category\n",
    "        df_category = pd.DataFrame({\n",
    "            'input': mmlu_dataset['input'],  # The question or prompt\n",
    "            'A': mmlu_dataset['A'],          # Option A\n",
    "            'B': mmlu_dataset['B'],          # Option B\n",
    "            'C': mmlu_dataset['C'],          # Option C\n",
    "            'D': mmlu_dataset['D'],          # Option D\n",
    "            'target': mmlu_dataset['target'] # The correct answer (e.g., 'A', 'B', 'C', 'D')\n",
    "        })\n",
    "        \n",
    "        # Store the DataFrame in the dictionary, with the category as the key\n",
    "        category_dataframes[category] = df_category\n",
    "    \n",
    "    return category_dataframes\n",
    "\n",
    "# Access a DataFrame for a specific category\n",
    "#print(category_dfs.head())  # Example for checking 'high_school_biology'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "stem, humanities ..etc\n",
    "\"\"\"\n",
    "\n",
    "# Example list of categories\n",
    "categories = ['high_school_biology', 'abstract_algebra', 'professional_medicine', 'nutrition']\n",
    "\n",
    "# Call the function and get the dictionary of DataFrames\n",
    "category_dfs = convert_to_dataframe(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from datasets import load_dataset\n",
    "\n",
    "#cataegory = \"sociology\"\n",
    "# Load the MMLU dataset\n",
    "#def load_mmlu_dataset():\n",
    "#    mmlu_dataset = load_dataset(\"lukaemon/mmlu\",category, split='validation',trust_remote_code=True)\n",
    "#    return mmlu_dataset\n",
    "\n",
    "\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\", device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")\n",
    "\n",
    "def get_gpu_power(gpu_index=1): #measuring for gpu1\n",
    "    \"\"\"Fetches the current power consumption of the GPU using nvidia-smi.\"\"\"\n",
    "    result = subprocess.run(['nvidia-smi', '--query-gpu=power.draw', '--format=csv,noheader,nounits', '-i',str(gpu_index)], \n",
    "                            stdout=subprocess.PIPE, text=True)\n",
    "    power = float(result.stdout.strip())  # Power in watts\n",
    "    return power\n",
    "\n",
    "def convert_to_dataframe(categories):\n",
    "    category_dataframes = {}  # Dictionary to store DataFrames for each category\n",
    "    \n",
    "    for category in categories:\n",
    "        print(\"Loading Data for category: \", category)\n",
    "        \n",
    "        # Load the dataset for the given category\n",
    "        mmlu_dataset = load_dataset(\"lukaemon/mmlu\",category, split='validation',trust_remote_code=True)\n",
    "        #print(type(mmlu_dataset))\n",
    "        # Create a DataFrame for the current category\n",
    "        df_category = pd.DataFrame({\n",
    "            'input': mmlu_dataset['input'],  # The question or prompt\n",
    "            'A': mmlu_dataset['A'],          # Option A\n",
    "            'B': mmlu_dataset['B'],          # Option B\n",
    "            'C': mmlu_dataset['C'],          # Option C\n",
    "            'D': mmlu_dataset['D'],          # Option D\n",
    "            'target': mmlu_dataset['target'] # The correct answer (e.g., 'A', 'B', 'C', 'D')\n",
    "        })\n",
    "        \n",
    "        # Store the DataFrame in the dictionary, with the category as the key\n",
    "        category_dataframes[category] = df_category\n",
    "    print(\"loading_ data finish\")\n",
    "    return category_dataframes\n",
    "\n",
    "# Filter dataset by category\n",
    "#def filter_texts_by_category(df, category):\n",
    "#    return df[df['category'] == category]['text'].values\n",
    "\n",
    "# Filter dataset by subject category (e.g., 'high_school_biology', 'abstract_algebra')\n",
    "def filter_dict_by_category(df, category):\n",
    "    return df[category] \n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "# Run the bootstrapping experiment for multiple-choice questions in a given category\n",
    "def run_experiment_for_texts(datadictionary,categories, bootstrapping ):\n",
    "    category_latencies = []\n",
    "    category_energy_per_token = []\n",
    "    \n",
    "    \n",
    "    #print(categories)\n",
    "    category_accuracy= []\n",
    "\n",
    "\n",
    "\n",
    "    # task in one category\n",
    "    for category in categories:\n",
    "        data = datadictionary[category]#filter_dict_by_category(datadictionary, category)\n",
    "        question_text = data['input'].values\n",
    "        choices = [data['A'].values, data['B'].values, data['C'].values, data['D'].values]\n",
    "        correct_answer = data['target'].values\n",
    "        \n",
    "        task_accuracy= []\n",
    "        task_latencies = []\n",
    "        task_energy_per_token = []\n",
    "        #print(\" type question_text: \", question_text)\n",
    "        #print(\"len question_text: \", len(question_text))\n",
    "        print(\"processing category: \", category)\n",
    "\n",
    "\n",
    "        # Prompts of one tasks\n",
    "        for i, tasks in enumerate (question_text):\n",
    "\n",
    "\n",
    "\n",
    "            print(\"i : \", i)\n",
    "            # Concatenate question with options for LLM input\n",
    "            full_input = f\"Question: {question_text[i]}\\nA) {choices[0][i]}\\nB) ,{choices[1][i]}\\nC) ,{choices[2][i]}\\nD) ,{choices[3][i]}\"\n",
    "            #print(\"full input: \", full_input)\n",
    "            inputs = tokenizer(full_input, return_tensors=\"pt\").to(\"cuda\")  # Prepare input tensors\n",
    "\n",
    "            text_latencies = []\n",
    "            text_energy_per_token = []\n",
    "            correct_predictions = 0  # To calculate accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # Prompt Bootstrapping \n",
    "            for _ in range(bootstrapping):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                print(_)\n",
    "                power_start = get_gpu_power()  # Assuming a function to get GPU power\n",
    "                start_time = time.time()\n",
    "\n",
    "                # Generate the model's response\n",
    "                output = model.generate(inputs['input_ids'], max_new_tokens=200, do_sample=False)  # Adjust tokens if necessary\n",
    "\n",
    "                end_time = time.time()\n",
    "                power_end = get_gpu_power()\n",
    "\n",
    "                # Measure latency\n",
    "                latency = end_time - start_time\n",
    "                text_latencies.append(latency)\n",
    "\n",
    "                # Calculate energy consumption\n",
    "                avg_power = (power_start + power_end) / 2\n",
    "                energy = avg_power * latency\n",
    "\n",
    "                # Token count from output (assuming a tensor output)\n",
    "                output_tokens = output[0].shape[0]\n",
    "                energy_token = energy / output_tokens if output_tokens > 0 else 0\n",
    "                text_energy_per_token.append(energy_token)\n",
    "\n",
    "                # Decode the model's generated answer (you might need to adjust based on model output format)\n",
    "                generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "                # Check if the model's generated answer matches the correct answer\n",
    "                if correct_answer[i] in generated_text:\n",
    "                    correct_predictions += 1\n",
    "\n",
    "            task_latencies.append(np.mean(text_latencies))\n",
    "            task_energy_per_token.append(np.mean(text_energy_per_token))\n",
    "            accuracy = correct_predictions / bootstrapping \n",
    "            task_accuracy.append(accuracy)\n",
    "    category_accuracy.append(np.array(task_accuracy)/len(question_text))\n",
    "    return task_latencies, task_energy_per_token, category_accuracy\n",
    "\n",
    "\n",
    "# Store and collect metrics for each category\n",
    "# Collect metrics for each category\n",
    "def collect_metrics_for_categories(df, categories, bootstrapping):\n",
    "    category_metrics = {}\n",
    "\n",
    "    \"\"\"    for category in categories:\n",
    "            print(f\"Processing category: {category}\")\n",
    "            texts = filter_texts_by_category(df, category)\n",
    "\n",
    "            if texts.empty:\n",
    "                print(f\"No texts found for category {category}\")\n",
    "                continue\n",
    "    \"\"\"\n",
    "    latencies, energy_per_token, accuracy = run_experiment_for_texts(data_dict, categories, bootstrapping)\n",
    "\n",
    "    # Store metrics for each category\n",
    "    category_metrics[category] = {\n",
    "        \"latencies\": latencies,\n",
    "        \"energy_per_token\": energy_per_token,\n",
    "        \"accuracy\": accuracy\n",
    "    }\n",
    "\n",
    "    return category_metrics\n",
    "\n",
    "# Plot the energy consumption per token comparison\n",
    "def plot_energy_vs_latency(metrics, categories):\n",
    "    for category in categories:\n",
    "        category_data = metrics[category]\n",
    "        energy_per_token = category_data[\"energy_per_token\"]\n",
    "        latencies = category_data[\"latencies\"]\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "\n",
    "        # Plot energy per token\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot([sum(x)/len(x) for x in energy_per_token], marker='o', color='blue', label='Energy per Token (J)')\n",
    "        plt.title(f\"Energy per Token for {category}\")\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Energy (J)')\n",
    "        plt.legend()\n",
    "\n",
    "        # Plot latencies\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot([sum(x)/len(x) for x in latencies], marker='o', color='green', label='Latency (s)')\n",
    "        plt.title(f\"Latency for {category}\")\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Latency (s)')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    #plot_energy_vs_latency(metrics, categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Processing data\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "categories = stem\n",
    "categories = sociology  #sociology is shorter / less prompts\n",
    "#mmlu_dataset = load_mmlu_dataset()\n",
    "data_dict = convert_to_dataframe(categories)\n",
    "print(\"data_dict geladen\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run Experiments\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "bootstrapping = 3  # Number of iterations for each prompt\n",
    "\n",
    "# Collect metrics for each category\n",
    "metrics = collect_metrics_for_categories(data_dict, categories, bootstrapping)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
