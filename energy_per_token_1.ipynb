{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import transformers\n",
    "import accelerate\n",
    "import vllm\n",
    "import bitsandbytes\n",
    "from vllm import LLM, SamplingParams\n",
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib\n",
    "from collections import Counter\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "#matplotlib.use('TkAgg')\n",
    "#from awq import AutoAWQForCausalLM\n",
    "#from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>How can I improve my time management skills?</td>\n",
       "      <td>generic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>What are the most effective ways to deal with ...</td>\n",
       "      <td>generic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>What are the main differences between Python a...</td>\n",
       "      <td>generic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>How can I increase my productivity while worki...</td>\n",
       "      <td>generic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Can you explain the basics of quantum computing?</td>\n",
       "      <td>generic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>76</td>\n",
       "      <td>Write a script for a YouTube video exploring t...</td>\n",
       "      <td>writing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>77</td>\n",
       "      <td>Compose an engaging travel blog post about a r...</td>\n",
       "      <td>writing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>78</td>\n",
       "      <td>Write a captivating movie review for a recentl...</td>\n",
       "      <td>writing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>79</td>\n",
       "      <td>Structure a podcast script for an episode disc...</td>\n",
       "      <td>writing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>80</td>\n",
       "      <td>Write a symphony concert review, discussing th...</td>\n",
       "      <td>writing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    question_id                                               text category\n",
       "0             1       How can I improve my time management skills?  generic\n",
       "1             2  What are the most effective ways to deal with ...  generic\n",
       "2             3  What are the main differences between Python a...  generic\n",
       "3             4  How can I increase my productivity while worki...  generic\n",
       "4             5   Can you explain the basics of quantum computing?  generic\n",
       "..          ...                                                ...      ...\n",
       "75           76  Write a script for a YouTube video exploring t...  writing\n",
       "76           77  Compose an engaging travel blog post about a r...  writing\n",
       "77           78  Write a captivating movie review for a recentl...  writing\n",
       "78           79  Structure a podcast script for an episode disc...  writing\n",
       "79           80  Write a symphony concert review, discussing th...  writing\n",
       "\n",
       "[80 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Multitask Benchmark datenset json\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "file_path = \"./question.jsonl\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "    #for line in f:\n",
    "    #    data = json.loads(line)\n",
    "    #    print(data)\n",
    "df_mtconversation = pd.DataFrame(data)\n",
    "df_mtconversation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['generic' 'knowledge' 'roleplay' 'common-sense' 'fermi' 'counterfactual'\n",
      " 'coding' 'math' 'writing']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "file_path = \"./question.jsonl\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "    #for line in f:\n",
    "    #    data = json.loads(line)\n",
    "    #    print(data)\n",
    "df_mtconversation = pd.DataFrame(data)\n",
    "df_mtconversation\n",
    "\n",
    "# Categories:\n",
    "print(df_mtconversation.category.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Develop a C++ program that reads a text file line by line and counts the number of occurrences of a specific word in the file.',\n",
       "       'Implement a Python function to find the longest common subsequence of two input strings using dynamic programming.',\n",
       "       'Implement a regular expression in Python to validate an email address.',\n",
       "       'Write a program to find the nth Fibonacci number using dynamic programming.',\n",
       "       'Implement a binary search algorithm to find a specific element in a sorted array.',\n",
       "       'Implement a queue data structure using two stacks in Python.',\n",
       "       'Implement a program to find the common elements in two arrays without using any extra data structures.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Coding texts\n",
    "\n",
    "coding_texts = df_mtconversation[df_mtconversation['category'] == 'coding']['text']\n",
    "coding_texts.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpu_power(gpu_index=1): #measuring for gpu1\n",
    "    \"\"\"Fetches the current power consumption of the GPU using nvidia-smi.\"\"\"\n",
    "    result = subprocess.run(['nvidia-smi', '--query-gpu=power.draw', '--format=csv,noheader,nounits', '-i',str(gpu_index)], \n",
    "                            stdout=subprocess.PIPE, text=True)\n",
    "    power = float(result.stdout.strip())  # Power in watts\n",
    "    return power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hier mit simplem MT_Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['generic' 'knowledge' 'roleplay' 'common-sense' 'fermi' 'counterfactual'\n",
      " 'coding' 'math' 'writing']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Develop a C++ program that reads a text file line by line and counts the number of occurrences of a specific word in the file.',\n",
       "       'Implement a Python function to find the longest common subsequence of two input strings using dynamic programming.',\n",
       "       'Implement a regular expression in Python to validate an email address.',\n",
       "       'Write a program to find the nth Fibonacci number using dynamic programming.',\n",
       "       'Implement a binary search algorithm to find a specific element in a sorted array.',\n",
       "       'Implement a queue data structure using two stacks in Python.',\n",
       "       'Implement a program to find the common elements in two arrays without using any extra data structures.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "file_path = \"./question.jsonl\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "    #for line in f:\n",
    "    #    data = json.loads(line)\n",
    "    #    print(data)\n",
    "df_mtconversation = pd.DataFrame(data)\n",
    "df_mtconversation\n",
    "\n",
    "\n",
    "print(df_mtconversation.category.unique())\n",
    "coding_texts = df_mtconversation[df_mtconversation['category'] == 'coding']['text']\n",
    "coding_texts.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wie oft soll jeder einzelne Eingabeprompt genutzt werden? wichtig für Mean, std..\n",
    "\n",
    "bootstrapping = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MT_Bench mit normalem Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"tiiuae/falcon-7b\" \n",
    "            #\"MathLLMs/MathCoder-L-7B\" \n",
    "            #\"ProbeMedicalYonseiMAILab/medllama3-v20\" \n",
    "            #\"NTQAI/Nxcode-CQ-7B-orpo\" \n",
    "            #\"meta-llama/Llama-3.1-8B\" \n",
    "            #\"facebook/opt-125m\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "306fffc7a0ae4f4f809872f5699b3c7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julius/energy_per_token/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing category: generic\n",
      "Processing prompt (1/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (2/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (3/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (4/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (5/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (6/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (7/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (8/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (9/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (10/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing category: knowledge\n",
      "Processing prompt (1/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (2/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (3/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (4/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (5/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (6/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (7/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (8/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (9/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (10/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing category: roleplay\n",
      "Processing prompt (1/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (2/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (3/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (4/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (5/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (6/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (7/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (8/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (9/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (10/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing category: common-sense\n",
      "Processing prompt (1/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (2/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (3/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (4/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (5/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (6/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (7/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (8/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (9/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (10/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing category: fermi\n",
      "Processing prompt (1/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (2/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (3/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (4/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (5/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (6/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (7/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (8/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (9/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (10/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing category: counterfactual\n",
      "Processing prompt (1/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (2/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (3/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (4/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (5/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (6/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (7/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (8/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (9/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (10/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing category: coding\n",
      "Processing prompt (1/7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (2/7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (3/7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (4/7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (5/7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (6/7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (7/7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing category: math\n",
      "Processing prompt (1/3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (2/3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (3/3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing category: writing\n",
      "Processing prompt (1/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (2/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (3/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (4/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (5/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (6/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (7/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (8/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (9/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt (10/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             #\"tiiuae/falcon-mamba-7b\",\n",
    "                                             device_map=\"auto\", token=access_token)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "def get_gpu_power(gpu_index=1): #measuring for gpu1\n",
    "    \"\"\"Fetches the current power consumption of the GPU using nvidia-smi.\"\"\"\n",
    "    result = subprocess.run(['nvidia-smi', '--query-gpu=power.draw', '--format=csv,noheader,nounits', '-i',str(gpu_index)], \n",
    "                            stdout=subprocess.PIPE, text=True)\n",
    "    power = float(result.stdout.strip())  # Power in watts\n",
    "    return power\n",
    "\n",
    "\n",
    "# Load dataset once and keep it ready for all experiments\n",
    "def load_dataset(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Filter dataset by category\n",
    "def filter_texts_by_category(df, category):\n",
    "    return df[df['category'] == category]['text'].values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# Run the bootstrapping experiment for each text in a given category\n",
    "def run_experiment_for_texts(texts, bootstrapping):\n",
    "\n",
    "\n",
    "    #metriken:\n",
    "    latencies = []\n",
    "    energy_per_token = []\n",
    "    throughputs= []\n",
    "    generated_texts= []\n",
    "\n",
    "\n",
    "    for i, text in enumerate(texts):\n",
    "        \"\"\"\n",
    "        Texts: einzelne Prompts die zu einer Kategorie gehören: z.B. \n",
    "        coding\n",
    "\n",
    "        ['Develop a C++ program that reads a text file line by line and counts the number of occurrences of a specific word in the file.',\n",
    "       'Implement a Python function to find the longest common subsequence of two input strings using dynamic programming.',\n",
    "       'Implement a regular expression in Python to validate an email address.',\n",
    "       'Write a program to find the nth Fibonacci number using dynamic programming.',\n",
    "       'Implement a binary search algorithm to find a specific element in a sorted array.',\n",
    "       'Implement a queue data structure using two stacks in Python.',\n",
    "       'Implement a program to find the common elements in two arrays without using any extra data structures.']\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, return_attention_mask=True).to(\"cuda\") #einzelner prompt \n",
    "        \n",
    "        text_latencies = []                                 #latency für einen prompt der 10x durchlaufen wird   ------ darüber am ende Mean berechnen\n",
    "        text_energy_per_token = []                          #energy per token eines einzelnen prompts der 10x durchlaufen wird ------ darüber am ende Mean berechnen\n",
    "        text_throughput = []                                  #throughput ------ darüber am ende Mean berechnen\n",
    "        text_generated = []                                   #generated text------ darüber am ende Mean berechnen\n",
    "\n",
    "        print(f\"Processing prompt ({i+1}/{len(texts)})\")\n",
    "        \n",
    "        for _ in range(bootstrapping):                           # einzelner prompt wird 10 mal durchlaufen\n",
    "            power_start = get_gpu_power()                       #power consumption\n",
    "            start_time = time.time()                            #start time\n",
    "\n",
    "            # Measure the number of input tokens\n",
    "            input_tokens = inputs['input_ids'].shape[1]  # Number of tokens in the input prompt\n",
    "\n",
    "                \n",
    "            # Generate output from the model\n",
    "            output = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_new_tokens=200, do_sample=True) #generating output\n",
    "           \n",
    "            end_time = time.time()                              #end time\n",
    "            power_end = get_gpu_power()                        #power consumption           \n",
    "\n",
    "            # Measure latency\n",
    "            latency = end_time - start_time\n",
    "            text_latencies.append(latency)\n",
    "\n",
    "\n",
    "            # Calculate energy consumption (average power * time)\n",
    "            avg_power = (power_start + power_end) / 2\n",
    "            energy = avg_power * latency\n",
    "            total_output_tokens = len(output[0])\n",
    "\n",
    "            # Calculate the number of newly generated tokens\n",
    "            generated_tokens = total_output_tokens - input_tokens\n",
    "\n",
    "            energy_token = energy / generated_tokens if generated_tokens > 0 else 0\n",
    "            text_energy_per_token.append(energy_token)\n",
    "\n",
    "\n",
    "            #througput\n",
    "            throughput = generated_tokens / latency\n",
    "            text_throughput.append(throughput)\n",
    "\n",
    "            # Decode the generated output text\n",
    "            generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "            # Filter out the repetitive question from the generated text\n",
    "            filtered_generated_text = generated_text.replace(text, \"\").strip()\n",
    "            text_generated.append(filtered_generated_text)\n",
    "\n",
    "\n",
    "        latencies.append(text_latencies)\n",
    "        energy_per_token.append(text_energy_per_token)\n",
    "        throughputs.append(text_throughput)\n",
    "        generated_texts.append(text_generated)\n",
    "\n",
    "    return latencies, energy_per_token, throughputs, generated_texts\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Store and collect metrics for each category\n",
    "def collect_metrics_for_categories(df, categories, bootstrapping):\n",
    "    category_metrics = {}\n",
    "\n",
    "    for category in categories:                                                              #für jede Kategorie         \n",
    "        print(f\"Processing category: {category}\")\n",
    "        texts = filter_texts_by_category(df, category)                                      #df nach Kategorie filtern\n",
    "        latencies, energy_per_token, throughputs, generated_texts= run_experiment_for_texts(texts, bootstrapping)    #hier wird die funktion run_experiment_for_texts aufgerufen\n",
    "\n",
    "        # Store metrics for each category\n",
    "        category_metrics[category] = {\n",
    "            \"latencies\": latencies,\n",
    "            \"energy_per_token\": energy_per_token,\n",
    "            \"throughput\" : throughputs,\n",
    "            \"generated_texts\": generated_texts\n",
    "        }\n",
    "\n",
    "    return category_metrics\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"./question.jsonl\"\n",
    "# bootstrapping = 10  # Number of iterations for each prompt\n",
    "\n",
    "df_mtconversation = load_dataset(file_path)\n",
    "\n",
    "# Define the categories to test\n",
    "categories = ['generic', 'knowledge', 'roleplay', 'common-sense', 'fermi', 'counterfactual', 'coding', 'math', 'writing']   #sind alle kategorien die es gibt im MT_Bench datenset\n",
    "\n",
    "# Collect metrics for each category\n",
    "metrics = collect_metrics_for_categories(df_mtconversation, categories, bootstrapping)\n",
    "\n",
    "# Plot results for comparison\n",
    "#plot_energy_vs_latency(metrics, categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{model_name.replace('/','-').replace('.', '_')}_bootstrapping={bootstrapping}_metrics.json\", \"w\") as json_file:\n",
    "    json.dump(metrics, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the energy consumption per token comparison\n",
    "def plot_energy_vs_latency(metrics, categories, model):\n",
    "    for category in categories:\n",
    "        category_data = metrics[category]\n",
    "        energy_per_token = category_data[\"energy_per_token\"]\n",
    "        latencies = category_data[\"latencies\"]\n",
    "        #response_sim = category_data[\"response_similiarity\"]\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "\n",
    "        # Plot energy per token\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot([sum(x)/len(x) for x in energy_per_token], marker='o', color='blue', label='Energy per Token (J)')\n",
    "        plt.title(f\"{model} Energy per Token for {category}\")\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Energy (J)')\n",
    "        plt.legend()\n",
    "\n",
    "        # Plot latencies\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot([sum(x)/len(x) for x in latencies], marker='o', color='green', label='latency')\n",
    "        plt.title(f\"{model} Latency for {category}\")\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('simScore')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{model.replace('/','-').replace('.', '_')}_{category}\", dpi=300)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_energy_vs_latency(metrics, categories, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MT_Bench mit quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"facebook/opt-125m\" # \"meta-llama/Llama-3.1-8B\" \"tiiuae/falcon-7b\" #\n",
    "quantization = \"nf4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julius/energy_per_token/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=quantization,  # Specify the quantization type\n",
    "    bnb_4bit_use_double_quant=True,  # Use double quantization if needed\n",
    "    bnb_4bit_compute_dtype=torch.float16  # Specify computation dtype\n",
    ")\n",
    "\n",
    "\n",
    "# Configure 4-bit quantization\n",
    "#quant_config = { \"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\" }\n",
    "# Load the model and tokenizer with quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             #\"tiiuae/falcon-mamba-7b\",\n",
    "                                             quantization_config=quant_config,\n",
    "                                             device_map=\"auto\", token=access_token)\n",
    "tokenizer = AutoTokenizer.from_pretrained (model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing category: generic\n",
      "Processing category: knowledge\n",
      "Processing category: roleplay\n",
      "Processing category: common-sense\n",
      "Processing category: fermi\n",
      "Processing category: counterfactual\n",
      "Processing category: coding\n",
      "Processing category: math\n",
      "Processing category: writing\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "# Load a pre-trained sentence transformer model\n",
    "modelSimiliarity = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "def calculate_semantic_similarity(response_1, response_2):\n",
    "    # Get embeddings for both responses\n",
    "    embedding_1 = modelSimiliarity.encode(response_1, convert_to_tensor=True)\n",
    "    embedding_2 = modelSimiliarity.encode(response_2, convert_to_tensor=True)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    cosine_sim = util.pytorch_cos_sim(embedding_1, embedding_2)\n",
    "    return cosine_sim.item()\n",
    "\n",
    "def get_gpu_power(gpu_index=1): #measuring for gpu1\n",
    "    \"\"\"Fetches the current power consumption of the GPU using nvidia-smi.\"\"\"\n",
    "    result = subprocess.run(['nvidia-smi', '--query-gpu=power.draw', '--format=csv,noheader,nounits', '-i',str(gpu_index)], \n",
    "                            stdout=subprocess.PIPE, text=True)\n",
    "    power = float(result.stdout.strip())  # Power in watts\n",
    "    return power\n",
    "\n",
    "\n",
    "# Load dataset once and keep it ready for all experiments\n",
    "def load_dataset(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Filter dataset by category\n",
    "def filter_texts_by_category(df, category):\n",
    "    return df[df['category'] == category]['text'].values\n",
    "\n",
    "# Run the bootstrapping experiment for each text in a given category\n",
    "def run_experiment_for_texts(texts, bootstrapping):\n",
    "    latencies = []\n",
    "    energy_per_token = []\n",
    "    throughputs= []\n",
    "    generated_texts= []\n",
    "\n",
    "    for i, text in enumerate(texts):\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, return_attention_mask=True).to(\"cuda\")\n",
    "        \n",
    "        text_latencies = []\n",
    "        text_energy_per_token = []\n",
    "        text_throughput = []\n",
    "        text_generated = []\n",
    "        response_similiarity = []\n",
    "\n",
    "        for _ in range(bootstrapping):\n",
    "            power_start = get_gpu_power()\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Generate output from the model\n",
    "            output = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_new_tokens=200, do_sample=True)\n",
    "\n",
    "            end_time = time.time()\n",
    "            power_end = get_gpu_power()\n",
    "\n",
    "            # Measure latency\n",
    "            latency = end_time - start_time\n",
    "            text_latencies.append(latency)\n",
    "\n",
    "\n",
    "            # Calculate energy consumption (average power * time)\n",
    "            avg_power = (power_start + power_end) / 2\n",
    "            energy = avg_power * latency\n",
    "            output_tokens = len(output[0])\n",
    "            energy_token = energy / output_tokens if output_tokens > 0 else 0\n",
    "            text_energy_per_token.append(energy_token)\n",
    "\n",
    "\n",
    "            #througput\n",
    "            throughput = output_tokens / latency\n",
    "            text_throughput.append(throughput)\n",
    "\n",
    "            # Decode the generated output text\n",
    "            generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            # Filter out the repetitive question from the generated text\n",
    "            filtered_generated_text = generated_text.replace(text, \"\").strip()\n",
    "            text_generated.append(filtered_generated_text)\n",
    "\n",
    "        # measuring the similiarity    \n",
    "        n = len(text_generated)\n",
    "        similarity_scores = []  # To store similarity scores\n",
    "\n",
    "        for i in range(n - 1):  # Outer loop goes up to n-2\n",
    "            for j in range(i + 1, n):  # Inner loop starts from i+1 and goes to n-1\n",
    "                similarity = calculate_semantic_similarity(text_generated[i], text_generated[j])\n",
    "                similarity_scores.append(similarity)  # Store only the pairwise similarity scores\n",
    "\n",
    "        # Step 2: Calculate average and variance of similarities\n",
    "        average_similarity_Encoder = np.mean(similarity_scores)\n",
    "        std_similarity_Encoder = np.std(similarity_scores)\n",
    "        response_similiarity.append((average_similarity_Encoder, std_similarity_Encoder))\n",
    "\n",
    "\n",
    "        latencies.append(text_latencies)\n",
    "        energy_per_token.append(text_energy_per_token)\n",
    "        throughputs.append(text_throughput)\n",
    "        generated_texts.append(text_generated)\n",
    "\n",
    "    return latencies, energy_per_token, throughputs, generated_texts, response_similiarity\n",
    "\n",
    "# Store and collect metrics for each category\n",
    "def collect_metrics_for_categories(df, categories, bootstrapping):\n",
    "    category_metrics = {}\n",
    "\n",
    "    for category in categories:\n",
    "        print(f\"Processing category: {category}\")\n",
    "        texts = filter_texts_by_category(df, category)\n",
    "        latencies, energy_per_token, throughputs, generated_texts, response_similiarity = run_experiment_for_texts(texts, bootstrapping)\n",
    "\n",
    "        # Store metrics for each category\n",
    "        category_metrics[category] = {\n",
    "            \"latencies\": latencies,\n",
    "            \"energy_per_token\": energy_per_token,\n",
    "            \"throughput\" : throughputs,\n",
    "            \"generated_texts\": generated_texts,\n",
    "            \"response_similiarity\": response_similiarity\n",
    "        }\n",
    "\n",
    "    return category_metrics\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"./question.jsonl\"\n",
    "bootstrapping = 10  # Number of iterations for each prompt\n",
    "\n",
    "df_mtconversation = load_dataset(file_path)\n",
    "\n",
    "# Define the categories to test\n",
    "categories = ['generic', 'knowledge', 'roleplay', 'common-sense', 'fermi', 'counterfactual', 'coding', 'math', 'writing']\n",
    "\n",
    "# Collect metrics for each category\n",
    "metricsquant = collect_metrics_for_categories(df_mtconversation, categories, bootstrapping)\n",
    "\n",
    "# Plot results for comparison\n",
    "#plot_energy_vs_latency(metrics, categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save metrics to JSON file\n",
    "with open(f\"{model_name.replace('/','-').replace('.', '_')}_{quantization}_metrics.json\", \"w\") as json_file:\n",
    "    json.dump(metricsquant, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the energy consumption per token comparison\n",
    "def plot_energy_vs_latency(metrics, categories, model, quantization):\n",
    "    for category in categories:\n",
    "        category_data = metrics[category]\n",
    "        energy_per_token = category_data[\"energy_per_token\"]\n",
    "        latencies = category_data[\"latencies\"]\n",
    "        response_sim = category_data[\"response_similiarity\"]\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "\n",
    "        # Plot energy per token\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot([sum(x)/len(x) for x in energy_per_token], marker='o', color='blue', label='Energy per Token (J)')\n",
    "        plt.title(f\"{model} {quantization} Energy per Token for {category}\")\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Energy (J)')\n",
    "        plt.legend()\n",
    "\n",
    "        # Plot latencies\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot([sum(x)/len(x) for x in latencies], marker='o', color='green', label='latency')\n",
    "        plt.title(f\"{model} {quantization} Latency for {category}\")\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('simScore')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{model.replace('/','-').replace('.', '_')}_{quantization}_{category}\", dpi=300)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_energy_vs_latency(metricsquant, categories, model_name, quantization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MT_Bench with vLLM\n",
    "\n",
    "\n",
    "### Das funktioniert noch nicht mit vLLM!!!  out of memory error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import transformers\n",
    "import accelerate\n",
    "import vllm\n",
    "import bitsandbytes\n",
    "from vllm import LLM, SamplingParams\n",
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib\n",
    "from collections import Counter\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "#matplotlib.use('TkAgg')\n",
    "#from awq import AutoAWQForCausalLM\n",
    "#from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-3.1-8B\"  # \"tiiuae/falcon-7b\"\n",
    "bootstrapping = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def get_gpu_power(gpu_index=1): #measuring for gpu1\n",
    "    \"\"\"Fetches the current power consumption of the GPU using nvidia-smi.\"\"\"\n",
    "    result = subprocess.run(['nvidia-smi', '--query-gpu=power.draw', '--format=csv,noheader,nounits', '-i',str(gpu_index)], \n",
    "                            stdout=subprocess.PIPE, text=True)\n",
    "    power = float(result.stdout.strip())  # Power in watts\n",
    "    return power\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#llm = LLM(\"tiiuae/falcon-7b\",dtype = torch.float16 ,trust_remote_code=True)\n",
    "#llm = LLM(model_name,trust_remote_code=True)\n",
    "llm = LLM(model_name)\n",
    "# Prepare sampling parameters and prompt\n",
    "sampling_params = SamplingParams(temperature=0.5, max_tokens=200)\n",
    "#prompt = \"Generate a python code that accepts a list of numbers and returns the sum.\"\n",
    "\n",
    "\n",
    "\n",
    "def get_gpu_power(gpu_index=1): #measuring for gpu1\n",
    "    \"\"\"Fetches the current power consumption of the GPU using nvidia-smi.\"\"\"\n",
    "    result = subprocess.run(['nvidia-smi', '--query-gpu=power.draw', '--format=csv,noheader,nounits', '-i',str(gpu_index)], \n",
    "                            stdout=subprocess.PIPE, text=True)\n",
    "    power = float(result.stdout.strip())  # Power in watts\n",
    "    return power\n",
    "\n",
    "\n",
    "# Load dataset once and keep it ready for all experiments\n",
    "def load_dataset(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Filter dataset by category\n",
    "def filter_texts_by_category(df, category):\n",
    "    return df[df['category'] == category]['text'].values\n",
    "\n",
    "# Run the bootstrapping experiment for each text in a given category\n",
    "def run_experiment_for_texts(texts, bootstrapping):\n",
    "    latencies = []\n",
    "    energy_per_token = []\n",
    "    throughputs= []\n",
    "    generated_texts= []\n",
    "\n",
    "    for i, text in enumerate(texts):\n",
    "        #inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "        \n",
    "        text_latencies = []\n",
    "        text_energy_per_token = []\n",
    "        text_throughput = []\n",
    "        text_generated = []\n",
    "\n",
    "\n",
    "        for _ in range(bootstrapping):\n",
    "            power_start = get_gpu_power()\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "            #prompts = inputs['input_ids'].tolist()\n",
    "            # Generate output from the model\n",
    "            response = llm.generate(text, sampling_params)\n",
    "\n",
    "\n",
    "\n",
    "            end_time = time.time()\n",
    "            power_end = get_gpu_power()\n",
    "            # Measure latency\n",
    "            latency = end_time - start_time\n",
    "            text_latencies.append(latency)\n",
    "\n",
    "\n",
    "            # Calculate energy consumption (average power * time)\n",
    "            avg_power = (power_start + power_end) / 2\n",
    "            energy = avg_power * latency\n",
    "            output_tokens = len(response[0].outputs[0].token_ids)\n",
    "            energy_token = energy / output_tokens if output_tokens > 0 else 0\n",
    "            text_energy_per_token.append(energy_token)\n",
    "\n",
    "\n",
    "            #througput\n",
    "            throughput = output_tokens / latency\n",
    "            text_throughput.append(throughput)\n",
    "\n",
    "            # Decode the generated output text\n",
    "            #generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            generated_text = response[0].outputs[0].text\n",
    "            # Filter out the repetitive question from the generated text\n",
    "            filtered_generated_text = generated_text.replace(text, \"\").strip()\n",
    "            text_generated.append(filtered_generated_text)\n",
    "\n",
    "        latencies.append(text_latencies)\n",
    "        energy_per_token.append(text_energy_per_token)\n",
    "        throughputs.append(text_throughput)\n",
    "        generated_texts.append(text_generated)\n",
    "\n",
    "    return latencies, energy_per_token, throughputs, generated_texts\n",
    "\n",
    "# Store and collect metrics for each category\n",
    "def collect_metrics_for_categories(df, categories, bootstrapping):\n",
    "    category_metrics = {}\n",
    "\n",
    "    for category in categories:\n",
    "        print(f\"Processing category: {category}\")\n",
    "        texts = filter_texts_by_category(df, category)\n",
    "        latencies, energy_per_token, throughputs, generated_texts = run_experiment_for_texts(texts, bootstrapping)\n",
    "\n",
    "        # Store metrics for each category\n",
    "        category_metrics[category] = {\n",
    "            \"latencies\": latencies,\n",
    "            \"energy_per_token\": energy_per_token,\n",
    "            \"throughput\" : throughputs,\n",
    "            \"generated_texts\": generated_texts\n",
    "        }\n",
    "\n",
    "    return category_metrics\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"./question.jsonl\"\n",
    "\n",
    "df_mtconversation = load_dataset(file_path)\n",
    "\n",
    "# Define the categories to test\n",
    "categories = ['generic', 'knowledge', 'roleplay', 'common-sense', 'fermi', 'counterfactual', 'coding', 'math', 'writing']\n",
    "\n",
    "# Collect metrics for each category\n",
    "vllm_metrics = collect_metrics_for_categories(df_mtconversation, categories, bootstrapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save metrics to JSON file\n",
    "with open(f\"vLLM_{model_name.replace('/','-').replace('.', '_')}_metrics.json\", \"w\") as json_file:\n",
    "    json.dump(vllm_metrics, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MMLU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Science, Technology, Engineering, Mathematics = stem\n",
    "stem = [\"clinical_knowledge\",\n",
    "\"medical_genetics\", \n",
    "\"high_school_physics\",\n",
    "\"virology\",\n",
    "\"high_school_biology\",\n",
    "\"abstract_algebra\",\n",
    "\"professional_medicine\",\n",
    "\"nutrition\",\n",
    "\"machine_learning\",\n",
    "\"anatomy\",\n",
    "\"college_medicine\",\n",
    "\"college_chemistry\",\n",
    "\"elementary_mathematics\",\n",
    "\"human_aging\",\n",
    "\"college_mathematics\",\n",
    "\"high_school_statistics\",\n",
    "\"high_school_mathematics\",\n",
    "\"high_school_computer_science\",\n",
    "\"conceptual_physics\",\n",
    "\"high_school_chemistry\",\n",
    "\"college_physics\",\n",
    "\"electrical_engineering\",\n",
    "\"astronomy\",\n",
    "\"college_biology\",\n",
    "\"computer_security\"]\n",
    "\n",
    "humanities= [\"high_school_european_history\",\n",
    "\"high_school_us_history\",\n",
    "\"high_school_world_history\",\n",
    "\"philosophy\",\n",
    "\"global_facts\",\n",
    "\"security_studies\",\n",
    "\"prehistory\",\n",
    "\"high_school_government_and_politics\",\n",
    "\"logical_fallacies\",\n",
    "\"international_law\",\n",
    "\"jurisprudence\",\n",
    "\"world_religions\",\n",
    "\"us_foreign_policy\",\n",
    "\"moral_scenarios\",\n",
    "\"moral_disputes\"\n",
    "]\n",
    "\n",
    "sociology = [\"sociology\",\n",
    "\"professional_psychology\",\n",
    "\"high_school_psychology\",\n",
    "\"human_sexuality\"]\n",
    "\n",
    "others = [\"business_ethics\",\n",
    "\"high_school_microeconomics\",\n",
    "\"econometrics\",\n",
    "\"professional_accounting\",\n",
    "\"public_relations\",\n",
    "\"marketing\",\n",
    "\"professional_law\",\n",
    "\"management\",\n",
    "\"miscellaneous\",\n",
    "\"high_school_macroeconomics\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"tiiuae/falcon-7b\" #\"facebook/opt-125m\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data for category:  high_school_biology\n",
      "Loading Data for category:  abstract_algebra\n",
      "Loading Data for category:  professional_medicine\n",
      "Loading Data for category:  nutrition\n",
      "loading_ data finish\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "stem, humanities ..etc\n",
    "\"\"\"\n",
    "\n",
    "# Example list of categories\n",
    "categories = ['high_school_biology', 'abstract_algebra', 'professional_medicine', 'nutrition']\n",
    "\n",
    "# Call the function and get the dictionary of DataFrames\n",
    "category_dfs = convert_to_dataframe(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22440d1850be43d9b3f8c52e108a8f26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julius/energy_per_token/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import subprocess\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "#cataegory = \"sociology\"\n",
    "# Load the MMLU dataset\n",
    "#def load_mmlu_dataset():\n",
    "#    mmlu_dataset = load_dataset(\"lukaemon/mmlu\",category, split='validation',trust_remote_code=True)\n",
    "#    return mmlu_dataset\n",
    "\n",
    "\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def get_gpu_power(gpu_index=1): #measuring for gpu1\n",
    "    \"\"\"Fetches the current power consumption of the GPU using nvidia-smi.\"\"\"\n",
    "    result = subprocess.run(['nvidia-smi', '--query-gpu=power.draw', '--format=csv,noheader,nounits', '-i',str(gpu_index)], \n",
    "                            stdout=subprocess.PIPE, text=True)\n",
    "    power = float(result.stdout.strip())  # Power in watts\n",
    "    return power\n",
    "\n",
    "def convert_to_dataframe(categories):\n",
    "    category_dataframes = {}  # Dictionary to store DataFrames for each category\n",
    "    \n",
    "    for category in categories:\n",
    "        print(\"Loading Data for category: \", category)\n",
    "        \n",
    "        # Load the dataset for the given category\n",
    "        mmlu_dataset = load_dataset(\"lukaemon/mmlu\",category, split='validation',trust_remote_code=True)\n",
    "        #print(type(mmlu_dataset))\n",
    "        # Create a DataFrame for the current category\n",
    "        df_category = pd.DataFrame({\n",
    "            'input': mmlu_dataset['input'],  # The question or prompt\n",
    "            'A': mmlu_dataset['A'],          # Option A\n",
    "            'B': mmlu_dataset['B'],          # Option B\n",
    "            'C': mmlu_dataset['C'],          # Option C\n",
    "            'D': mmlu_dataset['D'],          # Option D\n",
    "            'target': mmlu_dataset['target'] # The correct answer (e.g., 'A', 'B', 'C', 'D')\n",
    "        })\n",
    "        \n",
    "        # Store the DataFrame in the dictionary, with the category as the key\n",
    "        category_dataframes[category] = df_category\n",
    "    print(\"loading_ data finish\")\n",
    "    return category_dataframes\n",
    "\n",
    "# Filter dataset by category\n",
    "#def filter_texts_by_category(df, category):\n",
    "#    return df[df['category'] == category]['text'].values\n",
    "\n",
    "# Filter dataset by subject category (e.g., 'high_school_biology', 'abstract_algebra')\n",
    "def filter_dict_by_category(df, category):\n",
    "    return df[category] \n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "def map_generated_text_to_option(generated_text):\n",
    "    valid_options = ['A', 'B', 'C', 'D']\n",
    "    generated_text = generated_text.strip().upper()\n",
    "    if generated_text in valid_options:\n",
    "        return generated_text\n",
    "    else:\n",
    "        # Attempt to extract the option from the text\n",
    "        for option in valid_options:\n",
    "            if option in generated_text:\n",
    "                return option\n",
    "        # If no valid option is found, return None\n",
    "        return None\n",
    "        \n",
    "\n",
    "# Run the bootstrapping experiment for multiple-choice questions in a given category\n",
    "def run_experiment_for_texts(datadictionary,categories, bootstrapping ):\n",
    "    category_latencies = []\n",
    "    category_energy_per_token = []\n",
    "    \n",
    "    \n",
    "    #print(categories)\n",
    "    category_accuracy= []\n",
    "\n",
    "\n",
    "\n",
    "    # task in one category\n",
    "    for category in categories:\n",
    "        data = datadictionary[category]#filter_dict_by_category(datadictionary, category)\n",
    "        question_text = data['input'].values\n",
    "        choices = [data['A'].values, data['B'].values, data['C'].values, data['D'].values]\n",
    "        correct_answer = data['target'].values\n",
    "        \n",
    "        task_accuracy= []\n",
    "        task_latencies = []\n",
    "        task_energy_per_token = []\n",
    "        #print(\" type question_text: \", question_text)\n",
    "        #print(\"len question_text: \", len(question_text))\n",
    "        print(\"processing category: \", category)\n",
    "\n",
    "\n",
    "        # Prompts of one tasks\n",
    "        for i, tasks in enumerate (question_text):\n",
    "\n",
    "\n",
    "\n",
    "            print(f\"Question {i+1}/{len(question_text)}\")\n",
    "            # Concatenate question with options for LLM input\n",
    "            full_input = f\"Question: {question_text[i]}\\nA) {choices[0][i]}\\nB) ,{choices[1][i]}\\nC) ,{choices[2][i]}\\nD) ,{choices[3][i]} Answer: (\"\n",
    "            print(\"full input: \", full_input)\n",
    "            inputs = tokenizer(full_input, return_tensors=\"pt\").to(\"cuda\")  # Prepare input tensors\n",
    "\n",
    "            text_latencies = []\n",
    "            text_energy_per_token = []\n",
    "            correct_predictions = 0  # To calculate accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # Prompt Bootstrapping \n",
    "            for _ in range(bootstrapping):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                #print(f\"bootstrapping {_ + 1}/{bootstrapping}\")\n",
    "                power_start = get_gpu_power()  # Assuming a function to get GPU power\n",
    "                start_time = time.time()\n",
    "\n",
    "                # Generate the model's response\n",
    "                output = model.generate(inputs['input_ids'], max_new_tokens=1, do_sample=False, num_beams=1, eos_token_id=tokenizer.encode('\\n')[0])  # Adjust tokens if necessary\n",
    "\n",
    "                end_time = time.time()\n",
    "                power_end = get_gpu_power()\n",
    "\n",
    "                # Measure latency\n",
    "                latency = end_time - start_time\n",
    "                text_latencies.append(latency)\n",
    "\n",
    "                # Calculate energy consumption\n",
    "                avg_power = (power_start + power_end) / 2\n",
    "                energy = avg_power * latency\n",
    "\n",
    "                # Token count from output (assuming a tensor output)\n",
    "                output_tokens = output.shape[1] - inputs['input_ids'].shape[1]\n",
    "                energy_token = energy / output_tokens if output_tokens > 0 else 0\n",
    "                text_energy_per_token.append(energy_token)\n",
    "\n",
    "                # Decode the model's generated answer (you might need to adjust based on model output format)\n",
    "                #generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "                generated_text = tokenizer.decode(output[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True).strip()\n",
    "\n",
    "                print(f\"Generated answer: '{generated_text}' | Correct answer: '{correct_answer[i]}'\")\n",
    "                \n",
    "                # Check if the model's generated answer matches the correct answer\n",
    "                mapped_answer = map_generated_text_to_option(generated_text)\n",
    "\n",
    "                print(f\"Mapped answer: '{mapped_answer}' | Correct answer: '{correct_answer[i]}'\")\n",
    "  \n",
    "                # Check if the mapped answer matches the correct answer\n",
    "                if mapped_answer == correct_answer[i]:\n",
    "                    correct_predictions += 1\n",
    "                else:\n",
    "                    print(f\"Invalid or incorrect answer generated: '{generated_text}' mapped to '{mapped_answer}'\")\n",
    "\n",
    "\n",
    "            task_latencies.append(np.mean(text_latencies))\n",
    "            task_energy_per_token.append(np.mean(text_energy_per_token))\n",
    "            accuracy = correct_predictions / bootstrapping \n",
    "            task_accuracy.append(accuracy)\n",
    "        category_latencies.append(task_latencies)\n",
    "        category_energy_per_token.append(task_energy_per_token)\n",
    "        category_accuracy.append(np.mean(task_accuracy))\n",
    "    #return task_latencies, task_energy_per_token, category_accuracy\n",
    "    return category_latencies, category_energy_per_token, category_accuracy\n",
    "\n",
    "\n",
    "# Store and collect metrics for each category\n",
    "# Collect metrics for each category\n",
    "def collect_metrics_for_categories(data_dict, categories, bootstrapping):\n",
    "    category_metrics = {}\n",
    "\n",
    "    \"\"\"    for category in categories:\n",
    "            print(f\"Processing category: {category}\")\n",
    "            texts = filter_texts_by_category(df, category)\n",
    "\n",
    "            if texts.empty:\n",
    "                print(f\"No texts found for category {category}\")\n",
    "                continue\n",
    "    \"\"\"\n",
    "    latencies, energy_per_token, accuracy = run_experiment_for_texts(data_dict, categories, bootstrapping)\n",
    "\n",
    "    \"\"\"\n",
    "    print(f\"Number of categories: {len(categories)}\")\n",
    "    print(categories)\n",
    "    print(f\"Length of latencies: {len(latencies)}\")\n",
    "    print(latencies)\n",
    "    print(f\"Length of energy_per_token: {len(energy_per_token)}\")\n",
    "    print(energy_per_token)\n",
    "    print(f\"Length of accuracy: {len(accuracy)}\")\n",
    "    print(accuracy)\"\"\"\n",
    "\n",
    "    # Ensure that the lengths match\n",
    "    if len(latencies) != len(accuracy):\n",
    "        print(\"Mismatch in the number of categories and collected metrics.\")\n",
    "        print(\"Please check if all categories have been processed correctly.\")\n",
    "        return category_metrics\n",
    "\n",
    "    for idx, category in enumerate(categories):\n",
    "        if idx < len(latencies):\n",
    "            category_metrics[category] = {\n",
    "                \"latencies\": latencies[idx],\n",
    "                \"energy_per_token\": energy_per_token[idx],\n",
    "                \"accuracy\": accuracy[idx]\n",
    "            }\n",
    "        else:\n",
    "            print(f\"No metrics collected for category '{category}'. Skipping...\")\n",
    "\n",
    "\n",
    "    return category_metrics\n",
    "\n",
    "# Plot the energy consumption per token comparison\n",
    "def plot_energy_vs_latency(metrics, categories):\n",
    "    for category in categories:\n",
    "        category_data = metrics[category]\n",
    "        energy_per_token = category_data[\"energy_per_token\"]\n",
    "        latencies = category_data[\"latencies\"]\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "\n",
    "        # Plot energy per token\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot([sum(x)/len(x) for x in energy_per_token], marker='o', color='blue', label='Energy per Token (J)')\n",
    "        plt.title(f\"Energy per Token for {category}\")\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Energy (J)')\n",
    "        plt.legend()\n",
    "\n",
    "        # Plot latencies\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot([sum(x)/len(x) for x in latencies], marker='o', color='green', label='Latency (s)')\n",
    "        plt.title(f\"Latency for {category}\")\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Latency (s)')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    #plot_energy_vs_latency(metrics, categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data for category:  sociology\n",
      "Loading Data for category:  professional_psychology\n",
      "Loading Data for category:  high_school_psychology\n",
      "Loading Data for category:  human_sexuality\n",
      "loading_ data finish\n",
      "data_dict geladen\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Processing data\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "categories = stem\n",
    "categories = sociology  #sociology is shorter / less prompts\n",
    "#mmlu_dataset = load_mmlu_dataset()\n",
    "data_dict = convert_to_dataframe(categories)\n",
    "print(\"data_dict geladen\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing category:  sociology\n",
      "Question 1/22\n",
      "full input:  Answers should be written as only one of the capital letters A, B, C, D. No parantheses should be used. Question: Scientific management involved:\n",
      "A) the subdivision of labour into small tasks\n",
      "B) ,the measurement and specification of work tasks\n",
      "C) ,motivation and rewards for productivity\n",
      "D) ,all of the above Answer: \n",
      "Generated answer: '(' | Correct answer: 'D'\n",
      "Mapped answer: 'None' | Correct answer: 'D'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: '(' | Correct answer: 'D'\n",
      "Mapped answer: 'None' | Correct answer: 'D'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Generated answer: '(' | Correct answer: 'D'\n",
      "Mapped answer: 'None' | Correct answer: 'D'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Question 2/22\n",
      "full input:  Answers should be written as only one of the capital letters A, B, C, D. No parantheses should be used. Question: In Patterson's study of Brixton, it was found that:\n",
      "A) black and white residents competed for economic resources\n",
      "B) ,African-Caribbean migrants were concentrated in the poorest and most expensive housing\n",
      "C) ,white working class communities resented the arrival of black families\n",
      "D) ,all of the above Answer: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: '(' | Correct answer: 'D'\n",
      "Mapped answer: 'None' | Correct answer: 'D'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Generated answer: '(' | Correct answer: 'D'\n",
      "Mapped answer: 'None' | Correct answer: 'D'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: '(' | Correct answer: 'D'\n",
      "Mapped answer: 'None' | Correct answer: 'D'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Question 3/22\n",
      "full input:  Answers should be written as only one of the capital letters A, B, C, D. No parantheses should be used. Question: The 1944 Education Act provided:\n",
      "A) state elementary education for all\n",
      "B) ,free secondary education for all\n",
      "C) ,public schooling for those who could afford it\n",
      "D) ,assisted places in public schools for those on low incomes Answer: \n",
      "Generated answer: '(' | Correct answer: 'B'\n",
      "Mapped answer: 'None' | Correct answer: 'B'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: '(' | Correct answer: 'B'\n",
      "Mapped answer: 'None' | Correct answer: 'B'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Generated answer: '(' | Correct answer: 'B'\n",
      "Mapped answer: 'None' | Correct answer: 'B'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Question 4/22\n",
      "full input:  Answers should be written as only one of the capital letters A, B, C, D. No parantheses should be used. Question: The functionalist theory of inequality suggests that:\n",
      "A) high rewards and incentives ensure that the most skilled individuals will take the most important social positions\n",
      "B) ,inequality is inevitable and we are born into poverty or wealth\n",
      "C) ,there are no social functions of inequality, so it should be eradicated\n",
      "D) ,the idea of a meritocracy is a dangerous ideology Answer: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: '(' | Correct answer: 'A'\n",
      "Mapped answer: 'None' | Correct answer: 'A'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Generated answer: '(' | Correct answer: 'A'\n",
      "Mapped answer: 'None' | Correct answer: 'A'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: '(' | Correct answer: 'A'\n",
      "Mapped answer: 'None' | Correct answer: 'A'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Question 5/22\n",
      "full input:  Answers should be written as only one of the capital letters A, B, C, D. No parantheses should be used. Question: It is difficult to ascertain the true extent of domestic violence because:\n",
      "A) there is a large 'dark figure' of unreported incidents\n",
      "B) ,the changing definitions of legal categories have made it harder to convict offenders\n",
      "C) ,researchers are not allowed access to official statistics\n",
      "D) ,there is no valid or reliable way of researching such a sensitive topic Answer: \n",
      "Generated answer: '(' | Correct answer: 'A'\n",
      "Mapped answer: 'None' | Correct answer: 'A'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: '(' | Correct answer: 'A'\n",
      "Mapped answer: 'None' | Correct answer: 'A'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Generated answer: '(' | Correct answer: 'A'\n",
      "Mapped answer: 'None' | Correct answer: 'A'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Question 6/22\n",
      "full input:  Answers should be written as only one of the capital letters A, B, C, D. No parantheses should be used. Question: Wirth (1938) said that social relationships in the urban way of life were 'segmental' because:\n",
      "A) they were confined to particular areas of the city\n",
      "B) ,people knew each other only through specific, situational roles, and not as whole, rounded individuals\n",
      "C) ,there were distinctive patterns of activity for each social class\n",
      "D) ,they were based on face to face interaction with close friends and family Answer: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: '(' | Correct answer: 'B'\n",
      "Mapped answer: 'None' | Correct answer: 'B'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Generated answer: '(' | Correct answer: 'B'\n",
      "Mapped answer: 'None' | Correct answer: 'B'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: '(' | Correct answer: 'B'\n",
      "Mapped answer: 'None' | Correct answer: 'B'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Question 7/22\n",
      "full input:  Answers should be written as only one of the capital letters A, B, C, D. No parantheses should be used. Question: Society cannot be studied in the same way as the natural world because:\n",
      "A) human behaviour is meaningful, and varies between individuals and cultures\n",
      "B) ,it is difficult for sociologists to gain access to a research laboratory\n",
      "C) ,sociologists are not rational or critical enough in their approach\n",
      "D) ,we cannot collect empirical data about social life Answer: \n",
      "Generated answer: '(' | Correct answer: 'A'\n",
      "Mapped answer: 'None' | Correct answer: 'A'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: '(' | Correct answer: 'A'\n",
      "Mapped answer: 'None' | Correct answer: 'A'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Generated answer: '(' | Correct answer: 'A'\n",
      "Mapped answer: 'None' | Correct answer: 'A'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Question 8/22\n",
      "full input:  Answers should be written as only one of the capital letters A, B, C, D. No parantheses should be used. Question: A social stratum is:\n",
      "A) a level in the social hierarchy, comprising people with shared life chances\n",
      "B) ,a methodological tool used to identify a person's social class\n",
      "C) ,the boundary between two levels of the social hierarchy\n",
      "D) ,a symbol of status, used to differentiate between social classes Answer: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: '(' | Correct answer: 'A'\n",
      "Mapped answer: 'None' | Correct answer: 'A'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Generated answer: '(' | Correct answer: 'A'\n",
      "Mapped answer: 'None' | Correct answer: 'A'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: '(' | Correct answer: 'A'\n",
      "Mapped answer: 'None' | Correct answer: 'A'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Question 9/22\n",
      "full input:  Answers should be written as only one of the capital letters A, B, C, D. No parantheses should be used. Question: Foucault's term 'biopolitics' refers to:\n",
      "A) forms of power over the body, such as physical training, as a means of disciplining the mind\n",
      "B) ,forms of knowledge such as demographic statistics, which allow us to map and measure populations\n",
      "C) ,public health measures, such as improved sanitation and freshwater schemes\n",
      "D) ,intervention by the state to regulate sexual behaviour Answer: \n",
      "Generated answer: '(' | Correct answer: 'B'\n",
      "Mapped answer: 'None' | Correct answer: 'B'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: '(' | Correct answer: 'B'\n",
      "Mapped answer: 'None' | Correct answer: 'B'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Generated answer: '(' | Correct answer: 'B'\n",
      "Mapped answer: 'None' | Correct answer: 'B'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Question 10/22\n",
      "full input:  Answers should be written as only one of the capital letters A, B, C, D. No parantheses should be used. Question: Weber said that all knowledge is 'value-relevant' because:\n",
      "A) sociologists like to put a value on different theories\n",
      "B) ,knowledge refers to people and their values\n",
      "C) ,theorists interpret the world in terms of their own values\n",
      "D) ,attempts to provide knowledge about the world are always valuable Answer: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: '(' | Correct answer: 'C'\n",
      "Mapped answer: 'None' | Correct answer: 'C'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Generated answer: '(' | Correct answer: 'C'\n",
      "Mapped answer: 'None' | Correct answer: 'C'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: '(' | Correct answer: 'C'\n",
      "Mapped answer: 'None' | Correct answer: 'C'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Question 11/22\n",
      "full input:  Answers should be written as only one of the capital letters A, B, C, D. No parantheses should be used. Question: Urbanization occurred in the nineteenth century because:\n",
      "A) commuters started moving out of villages and into cities\n",
      "B) ,towns and cities were becoming increasingly planned and managed\n",
      "C) ,industrial capitalism led to a shift of population from rural to urban areas\n",
      "D) ,transport systems were not provided, so it was easier to live in the city Answer: \n",
      "Generated answer: '(' | Correct answer: 'C'\n",
      "Mapped answer: 'None' | Correct answer: 'C'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: '(' | Correct answer: 'C'\n",
      "Mapped answer: 'None' | Correct answer: 'C'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Generated answer: '(' | Correct answer: 'C'\n",
      "Mapped answer: 'None' | Correct answer: 'C'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Question 12/22\n",
      "full input:  Answers should be written as only one of the capital letters A, B, C, D. No parantheses should be used. Question: The view of anti-psychiatrists like Scheff was that mental illness was:\n",
      "A) a form of deviance, occurring when people challenged taken for granted expectations about interaction\n",
      "B) ,a socially negotiated 'insanity role' into which anyone could drift\n",
      "C) ,shaped by the processes of interaction taking place within the family and community\n",
      "D) ,all of the above Answer: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: '(' | Correct answer: 'D'\n",
      "Mapped answer: 'None' | Correct answer: 'D'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Generated answer: '(' | Correct answer: 'D'\n",
      "Mapped answer: 'None' | Correct answer: 'D'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: '(' | Correct answer: 'D'\n",
      "Mapped answer: 'None' | Correct answer: 'D'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Question 13/22\n",
      "full input:  Answers should be written as only one of the capital letters A, B, C, D. No parantheses should be used. Question: The 'two-sex' model that Laqueur (1990) identified:\n",
      "A) contrasted homosexuality with heterosexuality\n",
      "B) ,distinguished between male and females as separate sexes\n",
      "C) ,represented women's genitalia as underdeveloped versions of men's\n",
      "D) ,argued for male superiority over women Answer: \n",
      "Generated answer: '(' | Correct answer: 'B'\n",
      "Mapped answer: 'None' | Correct answer: 'B'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: '(' | Correct answer: 'B'\n",
      "Mapped answer: 'None' | Correct answer: 'B'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Generated answer: '(' | Correct answer: 'B'\n",
      "Mapped answer: 'None' | Correct answer: 'B'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Question 14/22\n",
      "full input:  Answers should be written as only one of the capital letters A, B, C, D. No parantheses should be used. Question: Despite the introduction of a national curriculum in 1988, girls continued to face disadvantages in education because:\n",
      "A) subject choice remained gendered at A level and in higher education\n",
      "B) ,boys received more attention from teachers in the classroom\n",
      "C) ,the hidden curriculum ensures that subjects like science are taught in gendered styles\n",
      "D) ,all of the above Answer: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: '(' | Correct answer: 'D'\n",
      "Mapped answer: 'None' | Correct answer: 'D'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Generated answer: '(' | Correct answer: 'D'\n",
      "Mapped answer: 'None' | Correct answer: 'D'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: '(' | Correct answer: 'D'\n",
      "Mapped answer: 'None' | Correct answer: 'D'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Question 15/22\n",
      "full input:  Answers should be written as only one of the capital letters A, B, C, D. No parantheses should be used. Question: Which of these trends did the New Right not suggest as evidence of declining family values?\n",
      "A) the tendency for cohabitation before marriage\n",
      "B) ,the rising divorce rate\n",
      "C) ,the absence of fathers in many households\n",
      "D) ,the increasing number of single parent families Answer: \n",
      "Generated answer: '(' | Correct answer: 'A'\n",
      "Mapped answer: 'None' | Correct answer: 'A'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: '(' | Correct answer: 'A'\n",
      "Mapped answer: 'None' | Correct answer: 'A'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Generated answer: '(' | Correct answer: 'A'\n",
      "Mapped answer: 'None' | Correct answer: 'A'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Question 16/22\n",
      "full input:  Answers should be written as only one of the capital letters A, B, C, D. No parantheses should be used. Question: Sociology can be considered a social science because:\n",
      "A) its theories are logical, explicit and supported by empirical evidence\n",
      "B) ,sociologists collect data in a relatively objective and systematic way\n",
      "C) ,ideas and research findings are scrutinized by other sociologists\n",
      "D) ,all of the above Answer: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: '(' | Correct answer: 'D'\n",
      "Mapped answer: 'None' | Correct answer: 'D'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Generated answer: '(' | Correct answer: 'D'\n",
      "Mapped answer: 'None' | Correct answer: 'D'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: '(' | Correct answer: 'D'\n",
      "Mapped answer: 'None' | Correct answer: 'D'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Question 17/22\n",
      "full input:  Answers should be written as only one of the capital letters A, B, C, D. No parantheses should be used. Question: In Esping-Andersen's (1990) outline of three types of state welfare, the social democratic model involved:\n",
      "A) loyalty to the state and traditional values\n",
      "B) ,individualistic self-reliance but high unemployment\n",
      "C) ,universalistic benefits and public sector employment\n",
      "D) ,deregulation of wages and prices by the introduction of the free market Answer: \n",
      "Generated answer: '(' | Correct answer: 'C'\n",
      "Mapped answer: 'None' | Correct answer: 'C'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: '(' | Correct answer: 'C'\n",
      "Mapped answer: 'None' | Correct answer: 'C'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Generated answer: '(' | Correct answer: 'C'\n",
      "Mapped answer: 'None' | Correct answer: 'C'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Question 18/22\n",
      "full input:  Answers should be written as only one of the capital letters A, B, C, D. No parantheses should be used. Question: Parsons argued that the two main functions of the modern family were:\n",
      "A) secondary socialization and strict discipline\n",
      "B) ,emotional support and sexual gratification\n",
      "C) ,primary socialization and personality stabilization\n",
      "D) ,oppressing women and reproducing the labour force Answer: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: '(' | Correct answer: 'C'\n",
      "Mapped answer: 'None' | Correct answer: 'C'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Generated answer: '(' | Correct answer: 'C'\n",
      "Mapped answer: 'None' | Correct answer: 'C'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: '(' | Correct answer: 'C'\n",
      "Mapped answer: 'None' | Correct answer: 'C'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Question 19/22\n",
      "full input:  Answers should be written as only one of the capital letters A, B, C, D. No parantheses should be used. Question: Which of these changes did not occur during the 'sexual revolution' of the 1960s?\n",
      "A) a growing fear of HIV and AIDS, fuelled by the New Right\n",
      "B) ,divorce law reforms\n",
      "C) ,the availability of oral contraception\n",
      "D) ,the recognition of women's sexual pleasure Answer: \n",
      "Generated answer: '(' | Correct answer: 'A'\n",
      "Mapped answer: 'None' | Correct answer: 'A'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: '(' | Correct answer: 'A'\n",
      "Mapped answer: 'None' | Correct answer: 'A'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Generated answer: '(' | Correct answer: 'A'\n",
      "Mapped answer: 'None' | Correct answer: 'A'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Question 20/22\n",
      "full input:  Answers should be written as only one of the capital letters A, B, C, D. No parantheses should be used. Question: In Parsons' view, the function of the sick role was to:\n",
      "A) provide a set of guidelines for medical staff to treat patients in hospital\n",
      "B) ,create a sense of meaning and order out of the illness experience\n",
      "C) ,create a scapegoat for people to blame, in the context of family life\n",
      "D) ,provide normative expectations to minimise the disruptive effects of illness on social life Answer: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: '(' | Correct answer: 'D'\n",
      "Mapped answer: 'None' | Correct answer: 'D'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Generated answer: '(' | Correct answer: 'D'\n",
      "Mapped answer: 'None' | Correct answer: 'D'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: '(' | Correct answer: 'D'\n",
      "Mapped answer: 'None' | Correct answer: 'D'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Question 21/22\n",
      "full input:  Answers should be written as only one of the capital letters A, B, C, D. No parantheses should be used. Question: Disciplinary power was exercised in the 19th century factories by:\n",
      "A) the use of corporal punishment by employers\n",
      "B) ,excluding women from participating in waged labour\n",
      "C) ,punctuality, uninterrupted work and the threat of dismissal\n",
      "D) ,making routine tasks less monotonous Answer: \n",
      "Generated answer: '(' | Correct answer: 'C'\n",
      "Mapped answer: 'None' | Correct answer: 'C'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: '(' | Correct answer: 'C'\n",
      "Mapped answer: 'None' | Correct answer: 'C'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Generated answer: '(' | Correct answer: 'C'\n",
      "Mapped answer: 'None' | Correct answer: 'C'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Question 22/22\n",
      "full input:  Answers should be written as only one of the capital letters A, B, C, D. No parantheses should be used. Question: Which of the following statements most closely corresponds with differential association theory?\n",
      "A) If all of your friends jumped off a bridge, I suppose you would too.\n",
      "B) ,You should be proud to be a part of this organization.\n",
      "C) ,If the door is closed, try the window.\n",
      "D) ,Once a thief, always a thief. Answer: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: '(' | Correct answer: 'A'\n",
      "Mapped answer: 'None' | Correct answer: 'A'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Generated answer: '(' | Correct answer: 'A'\n",
      "Mapped answer: 'None' | Correct answer: 'A'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: '(' | Correct answer: 'A'\n",
      "Mapped answer: 'None' | Correct answer: 'A'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "processing category:  professional_psychology\n",
      "Question 1/69\n",
      "full input:  Answers should be written as only one of the capital letters A, B, C, D. No parantheses should be used. Question: Which of the following statements best exemplifies criterion-referenced (as opposed to norm-referenced) measurement\n",
      "A) Alice answered 63% of the items correctly\n",
      "B) ,Susan’s score was average for her class\n",
      "C) ,James ranked at the 86th percentile on the Scholastic Aptitude Test (SAT\n",
      "D) ,Joe received a z score of 1.6 Answer: \n",
      "Generated answer: '(' | Correct answer: 'A'\n",
      "Mapped answer: 'None' | Correct answer: 'A'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: '(' | Correct answer: 'A'\n",
      "Mapped answer: 'None' | Correct answer: 'A'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Generated answer: '(' | Correct answer: 'A'\n",
      "Mapped answer: 'None' | Correct answer: 'A'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Question 2/69\n",
      "full input:  Answers should be written as only one of the capital letters A, B, C, D. No parantheses should be used. Question: Justice primarily involves ___________.\n",
      "A) Recognizing fairness for all persons\n",
      "B) ,Promoting accuracy and truthfulness\n",
      "C) ,Benefitting others and “do no harm”\n",
      "D) ,Respecting cultural and individual differences Answer: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: '(' | Correct answer: 'A'\n",
      "Mapped answer: 'None' | Correct answer: 'A'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Generated answer: '(' | Correct answer: 'A'\n",
      "Mapped answer: 'None' | Correct answer: 'A'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: '(' | Correct answer: 'A'\n",
      "Mapped answer: 'None' | Correct answer: 'A'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Question 3/69\n",
      "full input:  Answers should be written as only one of the capital letters A, B, C, D. No parantheses should be used. Question: Miranda M., age 32, makes an appointment with you at the recommendation of her physician who has diagnosed her recurrent headaches as the result of stress. The woman tells you that a friend of hers was trained in self-hypnosis for tension headaches and says that she also wants training in self-hypnosis. You took a weekend workshop on hypnosis in graduate school but have not used it in your clinical practice. As an ethical psychologist, you should:\n",
      "A) admit your lack of expertise in the use of self-hypnosis and refer the woman to another therapist.\n",
      "B) ,advise the woman that the underlying cause of her headaches would probably not be dealt with by self-hypnosis and suggest that she begin individual psychotherapy.\n",
      "C) ,accept the woman for treatment and begin training her in self-hypnosis after reviewing your workshop notes.\n",
      "D) ,accept the woman for treatment and begin training her in self-hypnosis but obtain consultation if you encounter any problems during the course of treatment. Answer: \n",
      "Generated answer: '(' | Correct answer: 'A'\n",
      "Mapped answer: 'None' | Correct answer: 'A'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: '(' | Correct answer: 'A'\n",
      "Mapped answer: 'None' | Correct answer: 'A'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Generated answer: '(' | Correct answer: 'A'\n",
      "Mapped answer: 'None' | Correct answer: 'A'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Question 4/69\n",
      "full input:  Answers should be written as only one of the capital letters A, B, C, D. No parantheses should be used. Question: During a session, co-therapists leading a group that has been meeting for 6 months have ‘philosophical difference on how to deal with two members’ concerns. According to Yalom, the most helpful intervention would be for the\n",
      "A) co-therapists to disagree openly in the group session\n",
      "B) ,more experienced therapist to sake charge of the session\n",
      "C) ,members to select which therapist they wish to work with and to divide the group into two sections\n",
      "D) ,co-therapists to change the topic, and resolve their differences outside the group environment Answer: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: '(' | Correct answer: 'A'\n",
      "Mapped answer: 'None' | Correct answer: 'A'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Generated answer: '(' | Correct answer: 'A'\n",
      "Mapped answer: 'None' | Correct answer: 'A'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: '(' | Correct answer: 'A'\n",
      "Mapped answer: 'None' | Correct answer: 'A'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Question 5/69\n",
      "full input:  Answers should be written as only one of the capital letters A, B, C, D. No parantheses should be used. Question: A client's insurance company pays 75% of a client’s therapy fee, while the client is required to pay the remaining 25%. The client asks you to bill the insurance company at a higher hourly rate so that he will not have to pay his 25% co-payment.  If you do so, you will be acting:\n",
      "A) illegally and unethically.\n",
      "B) ,illegally but ethically.\n",
      "C) ,legally but unethically.\n",
      "D) ,legally and ethically. Answer: \n",
      "Generated answer: '(' | Correct answer: 'A'\n",
      "Mapped answer: 'None' | Correct answer: 'A'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: '(' | Correct answer: 'A'\n",
      "Mapped answer: 'None' | Correct answer: 'A'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Generated answer: '(' | Correct answer: 'A'\n",
      "Mapped answer: 'None' | Correct answer: 'A'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Question 6/69\n",
      "full input:  Answers should be written as only one of the capital letters A, B, C, D. No parantheses should be used. Question: According to classical test theory, the variance of the obtained rest scores is equal to the\n",
      "A) sum of the general variance and the specific variance\n",
      "B) ,sum of the true score variance and the error variance\n",
      "C) ,variance error of the distribution of scores\n",
      "D) ,variance error of the mean of the scores Answer: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: '(' | Correct answer: 'B'\n",
      "Mapped answer: 'None' | Correct answer: 'B'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Generated answer: '(' | Correct answer: 'B'\n",
      "Mapped answer: 'None' | Correct answer: 'B'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: '(' | Correct answer: 'B'\n",
      "Mapped answer: 'None' | Correct answer: 'B'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Question 7/69\n",
      "full input:  Answers should be written as only one of the capital letters A, B, C, D. No parantheses should be used. Question: An experimental psychologist with a Ph.D. wants to become a clinical psychologist. According to the APA’s General Guidelines for Providers of Psychological Services, he must:\n",
      "A) complete appropriate coursework.\n",
      "B) ,complete doctoral-level coursework and training in clinical psychology.\n",
      "C) ,obtain supervision from a licensed clinical psychologist.\n",
      "D) ,obtain a Ph.D. in clinical psychology from an accredited school. Answer: \n",
      "Generated answer: '(' | Correct answer: 'B'\n",
      "Mapped answer: 'None' | Correct answer: 'B'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: '(' | Correct answer: 'B'\n",
      "Mapped answer: 'None' | Correct answer: 'B'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Generated answer: '(' | Correct answer: 'B'\n",
      "Mapped answer: 'None' | Correct answer: 'B'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Question 8/69\n",
      "full input:  Answers should be written as only one of the capital letters A, B, C, D. No parantheses should be used. Question: When the kappa statistic for a measure is .90, this indicates that the measure:\n",
      "A) has adequate inter-rater reliability.\n",
      "B) ,has adequate internal consistency reliability.\n",
      "C) ,has low criterion-related validity.\n",
      "D) ,has low incremental validity. Answer: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: '(' | Correct answer: 'A'\n",
      "Mapped answer: 'None' | Correct answer: 'A'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Generated answer: '(' | Correct answer: 'A'\n",
      "Mapped answer: 'None' | Correct answer: 'A'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: '(' | Correct answer: 'A'\n",
      "Mapped answer: 'None' | Correct answer: 'A'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Question 9/69\n",
      "full input:  Answers should be written as only one of the capital letters A, B, C, D. No parantheses should be used. Question: When faced with an approach-avoidance conﬂict:\n",
      "A) the closer we get to the goal, the stronger our desire to approach it.\n",
      "B) ,the closer we get to the goal, the stronger our desire to avoid it.\n",
      "C) ,regardless of our proximity to the goal, the desire to approach the goal is equal to the desire to avoid it.\n",
      "D) ,the strength of our desires to approach and to avoid the goal are unaffected by our proximity to it. Answer: \n",
      "Generated answer: '(' | Correct answer: 'B'\n",
      "Mapped answer: 'None' | Correct answer: 'B'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: '(' | Correct answer: 'B'\n",
      "Mapped answer: 'None' | Correct answer: 'B'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Generated answer: '(' | Correct answer: 'B'\n",
      "Mapped answer: 'None' | Correct answer: 'B'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Question 10/69\n",
      "full input:  Answers should be written as only one of the capital letters A, B, C, D. No parantheses should be used. Question: Dr. La-Keysha Leonard, a licensed psychologist, has been seeing a prison inmate in group therapy for several months and is asked by the parole board to evaluate the inmate to assist with their decision regarding his parole. As an ethical psychologist, Dr. Leonard should:\n",
      "A) agree to evaluate the inmate.\n",
      "B) ,agree to evaluate the inmate only if she believes she can do so objectively and without bias.\n",
      "C) ,agree to do so only if she is allowed to explain the purpose of the evaluation and the limits on confidentiality to the inmate.\n",
      "D) ,refuse to evaluate the inmate for the purpose of parole. Answer: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: '(' | Correct answer: 'D'\n",
      "Mapped answer: 'None' | Correct answer: 'D'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Generated answer: '(' | Correct answer: 'D'\n",
      "Mapped answer: 'None' | Correct answer: 'D'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: '(' | Correct answer: 'D'\n",
      "Mapped answer: 'None' | Correct answer: 'D'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Question 11/69\n",
      "full input:  Answers should be written as only one of the capital letters A, B, C, D. No parantheses should be used. Question: Dr. Stern is a psychologist who assists with hiring and promotion decisions at a mental health clinic. She recommends that one of the psychologists not be considered for promotion because of an unresolved charge of sexual harassment against him. This is:\n",
      "A) consistent with the provisions of the Ethics Code.\n",
      "B) ,a violation of the provisions of the Ethics Code.\n",
      "C) ,ethical as long as the clinic reconsiders the psychologist if he is acquitted of the charge.\n",
      "D) ,not addressed in the Ethics Code. Answer: \n",
      "Generated answer: '(' | Correct answer: 'B'\n",
      "Mapped answer: 'None' | Correct answer: 'B'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: '(' | Correct answer: 'B'\n",
      "Mapped answer: 'None' | Correct answer: 'B'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Generated answer: '(' | Correct answer: 'B'\n",
      "Mapped answer: 'None' | Correct answer: 'B'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n",
      "Question 12/69\n",
      "full input:  Answers should be written as only one of the capital letters A, B, C, D. No parantheses should be used. Question: Research on the sexual misconduct of therapists has found that:\n",
      "A) therapists who have sex with their clients often had sexual relations in the past with their own therapist, a professor, or a supervisor.\n",
      "B) ,therapists who have sex with their clients are more likely than those who do not to have a history of non-sexual dual relationships with clients.\n",
      "C) ,therapists who have sex with their clients tend to be less experienced and younger than those who do not.\n",
      "D) ,there are no consistent differences between therapists who do and do not have sex with their clients in terms of other dual relationships or sexual relationships with their own therapist, professor, or supervisor. Answer: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:193 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: '(' | Correct answer: 'B'\n",
      "Mapped answer: 'None' | Correct answer: 'B'\n",
      "Invalid or incorrect answer generated: '(' mapped to 'None'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m bootstrapping \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m  \u001b[38;5;66;03m# Number of iterations for each prompt\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Collect metrics for each category\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m mmlu_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mcollect_metrics_for_categories\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbootstrapping\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[61], line 194\u001b[0m, in \u001b[0;36mcollect_metrics_for_categories\u001b[0;34m(data_dict, categories, bootstrapping)\u001b[0m\n\u001b[1;32m    184\u001b[0m category_metrics \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    186\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"    for category in categories:\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;124;03m        print(f\"Processing category: {category}\")\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;124;03m        texts = filter_texts_by_category(df, category)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03m            continue\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 194\u001b[0m latencies, energy_per_token, accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiment_for_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbootstrapping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;124;03mprint(f\"Number of categories: {len(categories)}\")\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;124;03mprint(categories)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03mprint(f\"Length of accuracy: {len(accuracy)}\")\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03mprint(accuracy)\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;66;03m# Ensure that the lengths match\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[61], line 134\u001b[0m, in \u001b[0;36mrun_experiment_for_texts\u001b[0;34m(datadictionary, categories, bootstrapping)\u001b[0m\n\u001b[1;32m    131\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# Generate the model's response\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Adjust tokens if necessary\u001b[39;00m\n\u001b[1;32m    136\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    137\u001b[0m power_end \u001b[38;5;241m=\u001b[39m get_gpu_power()\n",
      "File \u001b[0;32m~/energy_per_token/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/energy_per_token/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:2024\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2016\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2017\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2018\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2019\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2020\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2021\u001b[0m     )\n\u001b[1;32m   2023\u001b[0m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2024\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2025\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2027\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2028\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2030\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2031\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2032\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2033\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2035\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2036\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   2037\u001b[0m     prepared_logits_warper \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2038\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2039\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mdo_sample\n\u001b[1;32m   2040\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2041\u001b[0m     )\n",
      "File \u001b[0;32m~/energy_per_token/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:2971\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2968\u001b[0m unfinished_sequences \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(batch_size, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2969\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_initial_cache_position(input_ids, model_kwargs)\n\u001b[0;32m-> 2971\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_has_unfinished_sequences\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2972\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthis_peer_finished\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcur_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\n\u001b[1;32m   2973\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   2974\u001b[0m     \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[1;32m   2975\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2977\u001b[0m     \u001b[38;5;66;03m# prepare variable output controls (note: some models won't accept all output controls)\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Run Experiments\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "bootstrapping = 3  # Number of iterations for each prompt\n",
    "\n",
    "# Collect metrics for each category\n",
    "mmlu_metrics = collect_metrics_for_categories(data_dict, categories, bootstrapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save metrics to JSON file\n",
    "with open(f\"MMLU_{model_name.replace('/','-').replace('.', '_')}_bootstrapping={bootstrapping}_metrics.json\", \"w\") as json_file:\n",
    "    json.dump(mmlu_metrics, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
