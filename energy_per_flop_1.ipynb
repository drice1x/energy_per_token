{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import transformers\n",
    "import accelerate\n",
    "import vllm\n",
    "import bitsandbytes\n",
    "from vllm import LLM, SamplingParams\n",
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib\n",
    "from collections import Counter\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "#matplotlib.use('TkAgg')\n",
    "#from awq import AutoAWQForCausalLM\n",
    "#from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../projects/question.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      9\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../projects/question.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     11\u001b[0m     data \u001b[38;5;241m=\u001b[39m [json\u001b[38;5;241m.\u001b[39mloads(line) \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f]\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m#for line in f:\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m#    data = json.loads(line)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m#    print(data)\u001b[39;00m\n",
      "File \u001b[0;32m~/energy_per_token/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../projects/question.jsonl'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Multitask Benchmark datenset json\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "file_path = \"../projects/question.jsonl\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "    #for line in f:\n",
    "    #    data = json.loads(line)\n",
    "    #    print(data)\n",
    "df_mtconversation = pd.DataFrame(data)\n",
    "df_mtconversation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "file_path = \"../projects/question.jsonl\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "    #for line in f:\n",
    "    #    data = json.loads(line)\n",
    "    #    print(data)\n",
    "df_mtconversation = pd.DataFrame(data)\n",
    "df_mtconversation\n",
    "\n",
    "# Categories:\n",
    "print(df_mtconversation.category.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpu_power(gpu_index=1): #measuring for gpu1\n",
    "    \"\"\"Fetches the current power consumption of the GPU using nvidia-smi.\"\"\"\n",
    "    result = subprocess.run(['nvidia-smi', '--query-gpu=power.draw', '--format=csv,noheader,nounits', '-i',str(gpu_index)], \n",
    "                            stdout=subprocess.PIPE, text=True)\n",
    "    power = float(result.stdout.strip())  # Power in watts\n",
    "    return power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hier mit simplem MT_Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (NEW) Energy per flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing category: coding\n",
      "Processing category: math\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch\n",
    "import subprocess\n",
    "import pynvml\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Specify the GPU device you want to use\n",
    "device = \"cuda:0\"  # Change this to your preferred GPU\n",
    "\n",
    "# Load model and tokenizer on the specified device\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")\n",
    "\n",
    "# Initialize NVML for power measurement\n",
    "def initialize_nvml():\n",
    "    pynvml.nvmlInit()\n",
    "\n",
    "def shutdown_nvml():\n",
    "    pynvml.nvmlShutdown()\n",
    "\n",
    "def get_gpu_handle(gpu_index=0):\n",
    "    return pynvml.nvmlDeviceGetHandleByIndex(gpu_index)\n",
    "\n",
    "def measure_power_consumption(handle, duration_sec=1.0, interval_sec=0.1):\n",
    "    power_readings = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while (time.time() - start_time) < duration_sec:\n",
    "        power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0  # Convert from mW to W\n",
    "        power_readings.append(power)\n",
    "        time.sleep(interval_sec)\n",
    "    \n",
    "    return sum(power_readings) / len(power_readings) if power_readings else 0\n",
    "\n",
    "def measure_energy_during_inference(handle, inference_function, *args, **kwargs):\n",
    "    power_start = measure_power_consumption(handle, duration_sec=0.5)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    result = inference_function(*args, **kwargs)  \n",
    "    end_time = time.time()\n",
    "    \n",
    "    power_end = measure_power_consumption(handle, duration_sec=0.5)\n",
    "    \n",
    "    avg_power = (power_start + power_end) / 2\n",
    "    elapsed_time = end_time - start_time  \n",
    "    energy_consumed = avg_power * elapsed_time  \n",
    "    \n",
    "    return energy_consumed, elapsed_time, result\n",
    "\n",
    "def calculate_perplexity(model, input_text, tokenizer):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)  # Ensure input is on the same device\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "        perplexity = torch.exp(loss)\n",
    "    return perplexity.item()\n",
    "\n",
    "def run_experiment_for_texts(texts, bootstrapping, handle):\n",
    "    latencies = []\n",
    "    energy_per_token = []\n",
    "    throughputs = []\n",
    "    generated_texts = []\n",
    "    perplexities = []\n",
    "\n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\").to(device)  # Ensure input is on the same device\n",
    "        text_latencies = []\n",
    "        text_energy_per_token = []\n",
    "        text_throughput = []\n",
    "        text_generated = []\n",
    "        text_perplexities = []\n",
    "\n",
    "        for _ in range(bootstrapping):\n",
    "            energy_consumed, latency, output = measure_energy_during_inference(handle, model.generate, inputs['input_ids'], max_new_tokens=200)\n",
    "            text_latencies.append(latency)\n",
    "\n",
    "            output_tokens = output.size(-1)\n",
    "            energy_token = energy_consumed / output_tokens if output_tokens > 0 else 0\n",
    "            text_energy_per_token.append(energy_token)\n",
    "\n",
    "            throughput = output_tokens / latency\n",
    "            text_throughput.append(throughput)\n",
    "\n",
    "            perplexity = calculate_perplexity(model, text, tokenizer)\n",
    "            text_perplexities.append(perplexity)\n",
    "\n",
    "            generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            filtered_generated_text = generated_text.replace(text, \"\").strip()\n",
    "            text_generated.append(filtered_generated_text)\n",
    "\n",
    "        latencies.append(text_latencies)\n",
    "        energy_per_token.append(text_energy_per_token)\n",
    "        throughputs.append(text_throughput)\n",
    "        generated_texts.append(text_generated)\n",
    "        perplexities.append(text_perplexities)\n",
    "\n",
    "    return latencies, energy_per_token, throughputs, generated_texts, perplexities\n",
    "\n",
    "def collect_metrics_for_categories(df, categories, bootstrapping):\n",
    "    category_metrics = {}\n",
    "    handle = get_gpu_handle(gpu_index=0)\n",
    "\n",
    "    for category in categories:\n",
    "        print(f\"Processing category: {category}\")\n",
    "        texts = filter_texts_by_category(df, category)\n",
    "        latencies, energy_per_token, throughputs, generated_texts, perplexities = run_experiment_for_texts(texts, bootstrapping, handle)\n",
    "\n",
    "        category_metrics[category] = {\n",
    "            \"latencies\": latencies,\n",
    "            \"energy_per_token\": energy_per_token,\n",
    "            \"throughput\": throughputs,\n",
    "            \"generated_texts\": generated_texts,\n",
    "            \"perplexities\": perplexities\n",
    "        }\n",
    "\n",
    "    shutdown_nvml()  \n",
    "    return category_metrics\n",
    "\n",
    "def filter_texts_by_category(df, category):\n",
    "    return df[df['category'] == category]['text'].values\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Example Usage\n",
    "file_path = \"../projects/question.jsonl\"\n",
    "bootstrapping = 2  \n",
    "df_mtconversation = load_dataset(file_path)\n",
    "\n",
    "categories = [  'coding', 'math']\n",
    "\n",
    "initialize_nvml()\n",
    "\n",
    "metrics = collect_metrics_for_categories(df_mtconversation, categories, bootstrapping)\n",
    "\n",
    "# (Optionally, you can visualize the collected metrics here)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def plot_metrics(metrics, categories):\n",
    "    num_categories = len(categories)\n",
    "\n",
    "    # Prepare the data for plotting\n",
    "    energy_per_token = []\n",
    "    avg_latencies = []\n",
    "    avg_perplexities = []\n",
    "\n",
    "    for category in categories:\n",
    "        if category in metrics:\n",
    "            energy_per_token.append(np.mean(metrics[category][\"energy_per_token\"]))\n",
    "            avg_latencies.append(np.mean(metrics[category][\"latencies\"]))\n",
    "            avg_perplexities.append(np.mean(metrics[category][\"perplexities\"]))\n",
    "        else:\n",
    "            energy_per_token.append(0)\n",
    "            avg_latencies.append(0)\n",
    "            avg_perplexities.append(0)\n",
    "\n",
    "    x = np.arange(num_categories)  # the label locations\n",
    "    width = 0.25  # the width of the bars\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Plot energy consumption\n",
    "    bars1 = ax1.bar(x - width, energy_per_token, width, label='Energy per Token (Joules)', color='b')\n",
    "    ax1.set_ylabel('Energy per Token (Joules)', color='b')\n",
    "    ax1.tick_params(axis='y', labelcolor='b')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(categories)\n",
    "    \n",
    "    # Create a second y-axis for latencies\n",
    "    ax2 = ax1.twinx()\n",
    "    bars2 = ax2.bar(x, avg_latencies, width, label='Average Latency (s)', color='g')\n",
    "    ax2.set_ylabel('Average Latency (s)', color='g')\n",
    "    ax2.tick_params(axis='y', labelcolor='g')\n",
    "\n",
    "    # Create a third y-axis for perplexities\n",
    "    ax3 = ax1.twinx()\n",
    "    bars3 = ax3.bar(x + width, avg_perplexities, width, label='Average Perplexity', color='r')\n",
    "    ax3.spines['right'].set_position(('outward', 60))  # move the third y-axis to the right\n",
    "    ax3.set_ylabel('Average Perplexity', color='r')\n",
    "    ax3.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "    # Adding titles and legend\n",
    "    fig.suptitle('Metrics Comparison Across Task Categories', fontsize=16)\n",
    "    fig.legend(loc='upper right', bbox_to_anchor=(0.85, 0.85))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the plotting function\n",
    "plot_metrics(metrics, categories)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MT_Bench mit quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    }
   ],
   "source": [
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",  # Specify the quantization type\n",
    "    bnb_4bit_use_double_quant=True,  # Use double quantization if needed\n",
    "    bnb_4bit_compute_dtype=torch.float16  # Specify computation dtype\n",
    ")\n",
    "device = \"cuda:0\" \n",
    "\n",
    "# Configure 4-bit quantization\n",
    "#quant_config = { \"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\" }\n",
    "# Load the model and tokenizer with quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\",\n",
    "                                             #\"tiiuae/falcon-mamba-7b\",\n",
    "                                             quantization_config=quant_config,\n",
    "                                             )\n",
    "tokenizer = AutoTokenizer.from_pretrained (\"facebook/opt-125m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ebergy per Token next to perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing category: knowledge\n",
      "Processing category: common-sense\n",
      "Processing category: coding\n",
      "Processing category: math\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch\n",
    "import subprocess\n",
    "import pynvml\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Specify the GPU device you want to use\n",
    "device = \"cuda:0\"  # Change this to your preferred GPU\n",
    "\n",
    "# Load model and tokenizer on the specified device\n",
    "#model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\").to(device)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")\n",
    "\n",
    "# Initialize NVML for power measurement\n",
    "def initialize_nvml():\n",
    "    pynvml.nvmlInit()\n",
    "\n",
    "def shutdown_nvml():\n",
    "    pynvml.nvmlShutdown()\n",
    "\n",
    "def get_gpu_handle(gpu_index=0):\n",
    "    return pynvml.nvmlDeviceGetHandleByIndex(gpu_index)\n",
    "\n",
    "def measure_power_consumption(handle, duration_sec=1.0, interval_sec=0.1):\n",
    "    power_readings = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while (time.time() - start_time) < duration_sec:\n",
    "        power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0  # Convert from mW to W\n",
    "        power_readings.append(power)\n",
    "        time.sleep(interval_sec)\n",
    "    \n",
    "    return sum(power_readings) / len(power_readings) if power_readings else 0\n",
    "\n",
    "def measure_energy_during_inference(handle, inference_function, *args, **kwargs):\n",
    "    power_start = measure_power_consumption(handle, duration_sec=0.5)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    result = inference_function(*args, **kwargs)  \n",
    "    end_time = time.time()\n",
    "    \n",
    "    power_end = measure_power_consumption(handle, duration_sec=0.5)\n",
    "    \n",
    "    avg_power = (power_start + power_end) / 2\n",
    "    elapsed_time = end_time - start_time  \n",
    "    energy_consumed = avg_power * elapsed_time  \n",
    "    \n",
    "    return energy_consumed, elapsed_time, result\n",
    "\n",
    "def calculate_perplexity(model, input_text, tokenizer):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)  # Ensure input is on the same device\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "        perplexity = torch.exp(loss)\n",
    "    return perplexity.item()\n",
    "\n",
    "def run_experiment_for_texts(texts, bootstrapping, handle):\n",
    "    latencies = []\n",
    "    energy_per_token = []\n",
    "    throughputs = []\n",
    "    generated_texts = []\n",
    "    perplexities = []\n",
    "\n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\").to(device)  # Ensure input is on the same device\n",
    "        text_latencies = []\n",
    "        text_energy_per_token = []\n",
    "        text_throughput = []\n",
    "        text_generated = []\n",
    "        text_perplexities = []\n",
    "\n",
    "        for _ in range(bootstrapping):\n",
    "            energy_consumed, latency, output = measure_energy_during_inference(handle, model.generate, inputs['input_ids'], max_new_tokens=200)\n",
    "            text_latencies.append(latency)\n",
    "\n",
    "            output_tokens = output.size(-1)\n",
    "            energy_token = energy_consumed / output_tokens if output_tokens > 0 else 0\n",
    "            text_energy_per_token.append(energy_token)\n",
    "\n",
    "            throughput = output_tokens / latency\n",
    "            text_throughput.append(throughput)\n",
    "\n",
    "            perplexity = calculate_perplexity(model, text, tokenizer)\n",
    "            text_perplexities.append(perplexity)\n",
    "\n",
    "            generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            filtered_generated_text = generated_text.replace(text, \"\").strip()\n",
    "            text_generated.append(filtered_generated_text)\n",
    "\n",
    "        latencies.append(text_latencies)\n",
    "        energy_per_token.append(text_energy_per_token)\n",
    "        throughputs.append(text_throughput)\n",
    "        generated_texts.append(text_generated)\n",
    "        perplexities.append(text_perplexities)\n",
    "\n",
    "    return latencies, energy_per_token, throughputs, generated_texts, perplexities\n",
    "\n",
    "def collect_metrics_for_categories(df, categories, bootstrapping):\n",
    "    category_metrics = {}\n",
    "    handle = get_gpu_handle(gpu_index=0)\n",
    "\n",
    "    for category in categories:\n",
    "        print(f\"Processing category: {category}\")\n",
    "        texts = filter_texts_by_category(df, category)\n",
    "        latencies, energy_per_token, throughputs, generated_texts, perplexities = run_experiment_for_texts(texts, bootstrapping, handle)\n",
    "\n",
    "        category_metrics[category] = {\n",
    "            \"latencies\": latencies,\n",
    "            \"energy_per_token\": energy_per_token,\n",
    "            \"throughput\": throughputs,\n",
    "            \"generated_texts\": generated_texts,\n",
    "            \"perplexities\": perplexities\n",
    "        }\n",
    "\n",
    "    shutdown_nvml()  \n",
    "    return category_metrics\n",
    "\n",
    "def filter_texts_by_category(df, category):\n",
    "    return df[df['category'] == category]['text'].values\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Example Usage\n",
    "file_path = \"../projects/question.jsonl\"\n",
    "bootstrapping = 2  \n",
    "df_mtconversation = load_dataset(file_path)\n",
    "\n",
    "categories = [ 'knowledge', 'common-sense', 'coding', 'math']\n",
    "\n",
    "initialize_nvml()\n",
    "\n",
    "metrics = collect_metrics_for_categories(df_mtconversation, categories, bootstrapping)\n",
    "\n",
    "# (Optionally, you can visualize the collected metrics here)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def plot_metrics(metrics, categories):\n",
    "    num_categories = len(categories)\n",
    "\n",
    "    # Prepare the data for plotting\n",
    "    energy_per_floost = []\n",
    "    energy_per_token = []\n",
    "    avg_latencies = []\n",
    "    avg_perplexities = []\n",
    "\n",
    "    for category in categories:\n",
    "        if category in metrics:\n",
    "            energy_per_token.append(np.mean(metrics[category][\"energy_per_token\"]))\n",
    "            avg_latencies.append(np.mean(metrics[category][\"latencies\"]))\n",
    "            avg_perplexities.append(np.mean(metrics[category][\"perplexities\"]))\n",
    "        else:\n",
    "            energy_per_token.append(0)\n",
    "            avg_latencies.append(0)\n",
    "            avg_perplexities.append(0)\n",
    "\n",
    "    x = np.arange(num_categories)  # the label locations\n",
    "    width = 0.25  # the width of the bars\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Plot energy consumption\n",
    "    bars1 = ax1.bar(x - width, energy_per_token, width, label='Energy per Token (Joules)', color='b')\n",
    "    ax1.set_ylabel('Energy per Token (Joules)', color='b')\n",
    "    ax1.tick_params(axis='y', labelcolor='b')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(categories)\n",
    "    \n",
    "    # Create a second y-axis for latencies\n",
    "    ax2 = ax1.twinx()\n",
    "    bars2 = ax2.bar(x, avg_latencies, width, label='Average Latency (s)', color='g')\n",
    "    ax2.set_ylabel('Average Latency (s)', color='g')\n",
    "    ax2.tick_params(axis='y', labelcolor='g')\n",
    "\n",
    "    # Create a third y-axis for perplexities\n",
    "    ax3 = ax1.twinx()\n",
    "    bars3 = ax3.bar(x + width, avg_perplexities, width, label='Average Perplexity', color='r')\n",
    "    ax3.spines['right'].set_position(('outward', 60))  # move the third y-axis to the right\n",
    "    ax3.set_ylabel('Average Perplexity', color='r')\n",
    "    ax3.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "    # Adding titles and legend\n",
    "    fig.suptitle('Metrics Comparison Across Task Categories', fontsize=16)\n",
    "    fig.legend(loc='upper right', bbox_to_anchor=(0.85, 0.85))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the plotting function\n",
    "plot_metrics(metrics, categories)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# energy_per_flops - 20.10.24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hier: That one is working (takes a lot of time)\n",
    "\n",
    "### Reasons might be: the duration_sec in measure_power_consumption (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"facebook/opt-125m\"\n",
    "bootstrapping = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julius/energy_per_token/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing category: knowledge\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        12.18%     188.590ms        17.98%     278.336ms      19.329us     122.295ms        52.99%     122.295ms       8.493us         14400     38220.595  \n",
      "                                               aten::mm         0.19%       2.937ms         0.27%       4.162ms      20.811us      44.887ms        19.45%      44.887ms     224.434us           200     17374.003  \n",
      "                                              aten::bmm         4.22%      65.364ms         5.80%      89.835ms      18.716us       8.359ms         3.62%       8.359ms       1.741us          4800       949.248  \n",
      "                                              aten::add         4.18%      64.634ms         6.80%     105.272ms      13.156us       9.761ms         4.23%       9.761ms       1.220us          8002         8.029  \n",
      "                                              aten::mul         1.70%      26.358ms         2.80%      43.322ms      14.426us       3.245ms         1.41%       3.245ms       1.081us          3003         2.099  \n",
      "                                            aten::empty         3.49%      54.038ms         3.49%      54.038ms       2.866us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.26%       4.028ms         4.81%      74.532ms      17.695us       0.000us         0.00%       1.503ms       0.357us          4212            --  \n",
      "                                         aten::_to_copy         0.59%       9.180ms         4.55%      70.504ms      25.135us       0.000us         0.00%       1.503ms       0.536us          2805            --  \n",
      "                                    aten::empty_strided         0.85%      13.217ms         0.85%      13.217ms       4.397us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.03%      15.967ms         3.81%      59.010ms      16.105us       2.420ms         1.05%       2.420ms       0.660us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.548s\n",
      "Self CUDA time total: 230.811ms\n",
      "\n",
      "output: tensor([[    2,  2264,    32,   103,   801,  8819,     9,   634,    10,   881,\n",
      "            12,  3698,  4136,  7304,  4411,    10, 31460,  7304,    15,   258,\n",
      "             5,  1737,     8,  1050,   474,   116, 50118, 50118,   133,  4136,\n",
      "          7304,    16,    10, 31460,  7304,     6,    53,    24,    16,    45,\n",
      "         31460,     4,    20,  4136,  7304,    16,    10, 31460,  7304,     6,\n",
      "            53,    24,    16,    45, 31460,     4,    20,  4136,  7304,    16,\n",
      "            10, 31460,  7304,     6,    53,    24,    16,    45, 31460,     4,\n",
      "            20,  4136,  7304,    16,    10, 31460,  7304,     6,    53,    24,\n",
      "            16,    45, 31460,     4,    20,  4136,  7304,    16,    10, 31460,\n",
      "          7304,     6,    53,    24,    16,    45, 31460,     4,    20,  4136,\n",
      "          7304,    16,    10, 31460,  7304,     6,    53,    24,    16,    45,\n",
      "         31460,     4,    20,  4136,  7304,    16,    10, 31460,  7304,     6,\n",
      "            53,    24,    16,    45, 31460,     4,    20,  4136,  7304,    16,\n",
      "            10, 31460,  7304,     6,    53,    24,    16,    45, 31460,     4,\n",
      "            20,  4136,  7304,    16,    10, 31460,  7304,     6,    53,    24,\n",
      "            16,    45, 31460,     4,    20,  4136,  7304,    16,    10, 31460,\n",
      "          7304,     6,    53,    24,    16,    45, 31460,     4,    20,  4136,\n",
      "          7304,    16,    10, 31460,  7304,     6,    53,    24,    16,    45,\n",
      "         31460,     4,    20,  4136,  7304,    16,    10, 31460,  7304,     6,\n",
      "            53,    24,    16,    45, 31460,     4,    20,  4136,  7304,    16,\n",
      "            10, 31460,  7304,     6,    53,    24,    16,    45, 31460,     4,\n",
      "            20,  4136,  7304,    16,    10, 31460,  7304,     6,    53,    24,\n",
      "            16,    45, 31460,     4,    20,  4136]], device='cuda:0')\n",
      "text_energy_per_token: [5.852261699971793]\n",
      "output_tokens: 226\n",
      "flop: 56553974230\n",
      "energy_consumed:  1322.6111441936253\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.06%     187.740ms        21.87%     255.629ms      17.752us     122.146ms        52.96%     122.146ms       8.482us         14400     38220.595  \n",
      "                                               aten::mm         0.25%       2.887ms         0.37%       4.296ms      21.481us      44.898ms        19.47%      44.898ms     224.491us           200     17374.003  \n",
      "                                              aten::bmm         5.02%      58.700ms         6.80%      79.435ms      16.549us       8.351ms         3.62%       8.351ms       1.740us          4800       949.248  \n",
      "                                              aten::add         5.50%      64.280ms         8.35%      97.576ms      12.194us       9.769ms         4.24%       9.769ms       1.221us          8002         8.029  \n",
      "                                              aten::mul         2.24%      26.183ms         3.34%      39.001ms      12.987us       3.247ms         1.41%       3.247ms       1.081us          3003         2.099  \n",
      "                                            aten::empty         5.09%      59.524ms         5.09%      59.524ms       3.156us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       4.122ms         5.95%      69.570ms      16.517us       0.000us         0.00%       1.505ms       0.357us          4212            --  \n",
      "                                         aten::_to_copy         0.77%       8.984ms         5.60%      65.448ms      23.333us       0.000us         0.00%       1.505ms       0.536us          2805            --  \n",
      "                                    aten::empty_strided         1.36%      15.949ms         1.36%      15.949ms       5.306us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.38%      16.131ms         4.54%      53.101ms      14.493us       2.422ms         1.05%       2.422ms       0.661us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.169s\n",
      "Self CUDA time total: 230.651ms\n",
      "\n",
      "output: tensor([[    2,  2264,    32,   103,   801,  8819,     9,   634,    10,   881,\n",
      "            12,  3698,  4136,  7304,  4411,    10, 31460,  7304,    15,   258,\n",
      "             5,  1737,     8,  1050,   474,   116, 50118, 50118,   133,  4136,\n",
      "          7304,    16,    10, 31460,  7304,     6,    53,    24,    16,    45,\n",
      "         31460,     4,    20,  4136,  7304,    16,    10, 31460,  7304,     6,\n",
      "            53,    24,    16,    45, 31460,     4,    20,  4136,  7304,    16,\n",
      "            10, 31460,  7304,     6,    53,    24,    16,    45, 31460,     4,\n",
      "            20,  4136,  7304,    16,    10, 31460,  7304,     6,    53,    24,\n",
      "            16,    45, 31460,     4,    20,  4136,  7304,    16,    10, 31460,\n",
      "          7304,     6,    53,    24,    16,    45, 31460,     4,    20,  4136,\n",
      "          7304,    16,    10, 31460,  7304,     6,    53,    24,    16,    45,\n",
      "         31460,     4,    20,  4136,  7304,    16,    10, 31460,  7304,     6,\n",
      "            53,    24,    16,    45, 31460,     4,    20,  4136,  7304,    16,\n",
      "            10, 31460,  7304,     6,    53,    24,    16,    45, 31460,     4,\n",
      "            20,  4136,  7304,    16,    10, 31460,  7304,     6,    53,    24,\n",
      "            16,    45, 31460,     4,    20,  4136,  7304,    16,    10, 31460,\n",
      "          7304,     6,    53,    24,    16,    45, 31460,     4,    20,  4136,\n",
      "          7304,    16,    10, 31460,  7304,     6,    53,    24,    16,    45,\n",
      "         31460,     4,    20,  4136,  7304,    16,    10, 31460,  7304,     6,\n",
      "            53,    24,    16,    45, 31460,     4,    20,  4136,  7304,    16,\n",
      "            10, 31460,  7304,     6,    53,    24,    16,    45, 31460,     4,\n",
      "            20,  4136,  7304,    16,    10, 31460,  7304,     6,    53,    24,\n",
      "            16,    45, 31460,     4,    20,  4136]], device='cuda:0')\n",
      "text_energy_per_token: [5.852261699971793, 8.884083323174032]\n",
      "output_tokens: 226\n",
      "flop: 56553974230\n",
      "energy_consumed:  2007.8028310373313\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.52%     178.723ms        21.24%     244.638ms      16.989us     122.129ms        53.02%     122.129ms       8.481us         14400     36521.902  \n",
      "                                               aten::mm         0.37%       4.224ms         0.48%       5.524ms      27.619us      44.924ms        19.50%      44.924ms     224.618us           200     16601.825  \n",
      "                                              aten::bmm         5.26%      60.565ms         7.37%      84.910ms      17.690us       8.140ms         3.53%       8.140ms       1.696us          4800       860.406  \n",
      "                                              aten::add         5.66%      65.230ms         8.66%      99.801ms      12.472us       9.754ms         4.23%       9.754ms       1.219us          8002         7.490  \n",
      "                                              aten::mul         2.26%      26.039ms         3.35%      38.618ms      12.860us       3.241ms         1.41%       3.241ms       1.079us          3003         2.005  \n",
      "                                            aten::empty         4.77%      54.988ms         4.77%      54.988ms       2.916us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.36%       4.110ms         5.97%      68.738ms      16.320us       0.000us         0.00%       1.497ms       0.355us          4212            --  \n",
      "                                         aten::_to_copy         0.78%       8.934ms         5.61%      64.628ms      23.040us       0.000us         0.00%       1.497ms       0.534us          2805            --  \n",
      "                                    aten::empty_strided         1.18%      13.592ms         1.18%      13.592ms       4.522us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.40%      16.106ms         4.62%      53.223ms      14.526us       2.412ms         1.05%       2.412ms       0.658us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.152s\n",
      "Self CUDA time total: 230.336ms\n",
      "\n",
      "output: tensor([[    2,  2264,  2433,    74,    47,  1701,    77, 15293,    41, 10510,\n",
      "             8,  6500,   285,  4264,   467,   116, 50118, 50118,   133,   412,\n",
      "             9,   764,  2659,    34,    10,   251,   750,     9,  1976,  4555,\n",
      "             6,  6500,     6,     8,  1522,   285,  4264,     4,   166,    32,\n",
      "          2021,     7,  1976,     5,   275,   678,  4264,  1735,    13,    70,\n",
      "            84,  1196,     4, 50118, 50118,  2264,  2433,    74,    47,  1701,\n",
      "            77, 15293,    41, 10510,     8,  6500,   285,  4264,   467,   116,\n",
      "         50118, 50118,   133,   412,     9,   764,  2659,    34,    10,   251,\n",
      "           750,     9,  1976,  4555,     6,  6500,     6,     8,  1522,   285,\n",
      "          4264,     4,   166,    32,  2021,     7,  1976,     5,   275,   678,\n",
      "          4264,  1735,    13,    70,    84,  1196,     4, 50118, 50118,  2264,\n",
      "          2433,    74,    47,  1701,    77, 15293,    41, 10510,     8,  6500,\n",
      "           285,  4264,   467,   116, 50118, 50118,   133,   412,     9,   764,\n",
      "          2659,    34,    10,   251,   750,     9,  1976,  4555,     6,  6500,\n",
      "             6,     8,  1522,   285,  4264,     4,   166,    32,  2021,     7,\n",
      "          1976,     5,   275,   678,  4264,  1735,    13,    70,    84,  1196,\n",
      "             4, 50118, 50118,  2264,  2433,    74,    47,  1701,    77, 15293,\n",
      "            41, 10510,     8,  6500,   285,  4264,   467,   116, 50118, 50118,\n",
      "           133,   412,     9,   764,  2659,    34,    10,   251,   750,     9,\n",
      "          1976,  4555,     6,  6500,     6,     8,  1522,   285,  4264,     4,\n",
      "           166,    32,  2021,     7,  1976,     5,   275,   678,  4264,  1735,\n",
      "            13,    70,    84,  1196,     4, 50118]], device='cuda:0')\n",
      "text_energy_per_token: [9.376887687194348]\n",
      "output_tokens: 216\n",
      "flop: 53993627700\n",
      "energy_consumed:  2025.4077404339791\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.65%     172.314ms        21.46%     236.327ms      16.412us     122.048ms        53.01%     122.048ms       8.476us         14400     36521.902  \n",
      "                                               aten::mm         0.38%       4.137ms         0.49%       5.352ms      26.758us      44.894ms        19.50%      44.894ms     224.472us           200     16601.825  \n",
      "                                              aten::bmm         5.26%      57.948ms         7.10%      78.217ms      16.295us       8.146ms         3.54%       8.146ms       1.697us          4800       860.406  \n",
      "                                              aten::add         5.70%      62.828ms         8.69%      95.709ms      11.961us       9.749ms         4.23%       9.749ms       1.218us          8002         7.490  \n",
      "                                              aten::mul         2.44%      26.857ms         3.57%      39.326ms      13.096us       3.238ms         1.41%       3.238ms       1.078us          3003         2.005  \n",
      "                                            aten::empty         4.83%      53.201ms         4.83%      53.201ms       2.821us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.880ms         6.15%      67.685ms      16.070us       0.000us         0.00%       1.500ms       0.356us          4212            --  \n",
      "                                         aten::_to_copy         0.75%       8.303ms         5.79%      63.805ms      22.747us       0.000us         0.00%       1.500ms       0.535us          2805            --  \n",
      "                                    aten::empty_strided         1.16%      12.786ms         1.16%      12.786ms       4.253us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.45%      15.938ms         4.87%      53.644ms      14.641us       2.416ms         1.05%       2.416ms       0.659us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.101s\n",
      "Self CUDA time total: 230.227ms\n",
      "\n",
      "output: tensor([[    2,  2264,  2433,    74,    47,  1701,    77, 15293,    41, 10510,\n",
      "             8,  6500,   285,  4264,   467,   116, 50118, 50118,   133,   412,\n",
      "             9,   764,  2659,    34,    10,   251,   750,     9,  1976,  4555,\n",
      "             6,  6500,     6,     8,  1522,   285,  4264,     4,   166,    32,\n",
      "          2021,     7,  1976,     5,   275,   678,  4264,  1735,    13,    70,\n",
      "            84,  1196,     4, 50118, 50118,  2264,  2433,    74,    47,  1701,\n",
      "            77, 15293,    41, 10510,     8,  6500,   285,  4264,   467,   116,\n",
      "         50118, 50118,   133,   412,     9,   764,  2659,    34,    10,   251,\n",
      "           750,     9,  1976,  4555,     6,  6500,     6,     8,  1522,   285,\n",
      "          4264,     4,   166,    32,  2021,     7,  1976,     5,   275,   678,\n",
      "          4264,  1735,    13,    70,    84,  1196,     4, 50118, 50118,  2264,\n",
      "          2433,    74,    47,  1701,    77, 15293,    41, 10510,     8,  6500,\n",
      "           285,  4264,   467,   116, 50118, 50118,   133,   412,     9,   764,\n",
      "          2659,    34,    10,   251,   750,     9,  1976,  4555,     6,  6500,\n",
      "             6,     8,  1522,   285,  4264,     4,   166,    32,  2021,     7,\n",
      "          1976,     5,   275,   678,  4264,  1735,    13,    70,    84,  1196,\n",
      "             4, 50118, 50118,  2264,  2433,    74,    47,  1701,    77, 15293,\n",
      "            41, 10510,     8,  6500,   285,  4264,   467,   116, 50118, 50118,\n",
      "           133,   412,     9,   764,  2659,    34,    10,   251,   750,     9,\n",
      "          1976,  4555,     6,  6500,     6,     8,  1522,   285,  4264,     4,\n",
      "           166,    32,  2021,     7,  1976,     5,   275,   678,  4264,  1735,\n",
      "            13,    70,    84,  1196,     4, 50118]], device='cuda:0')\n",
      "text_energy_per_token: [9.376887687194348, 9.412101735086353]\n",
      "output_tokens: 216\n",
      "flop: 53993627700\n",
      "energy_consumed:  2033.013974778652\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.05%     177.248ms        21.80%     240.763ms      16.720us     121.950ms        53.00%     121.950ms       8.469us         14400     36352.033  \n",
      "                                               aten::mm         0.26%       2.850ms         0.37%       4.083ms      20.417us      44.916ms        19.52%      44.916ms     224.579us           200     16524.607  \n",
      "                                              aten::bmm         5.13%      56.621ms         6.97%      77.041ms      16.050us       8.129ms         3.53%       8.129ms       1.694us          4800       851.927  \n",
      "                                              aten::add         5.75%      63.562ms         8.73%      96.461ms      12.055us       9.754ms         4.24%       9.754ms       1.219us          8002         7.437  \n",
      "                                              aten::mul         2.42%      26.784ms         3.56%      39.338ms      13.100us       3.240ms         1.41%       3.240ms       1.079us          3003         1.996  \n",
      "                                            aten::empty         5.16%      57.000ms         5.16%      57.000ms       3.023us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.859ms         6.33%      69.895ms      16.594us       0.000us         0.00%       1.506ms       0.358us          4212            --  \n",
      "                                         aten::_to_copy         0.74%       8.191ms         5.98%      66.036ms      23.542us       0.000us         0.00%       1.506ms       0.537us          2805            --  \n",
      "                                    aten::empty_strided         1.42%      15.739ms         1.42%      15.739ms       5.236us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.44%      15.927ms         4.92%      54.376ms      14.841us       2.420ms         1.05%       2.420ms       0.660us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.105s\n",
      "Self CUDA time total: 230.092ms\n",
      "\n",
      "output: tensor([[    2,  6179,    64,  3233, 16085,  2358,     8,  5775,  1986,     7,\n",
      "          5217,   776,  3872, 27366,   116, 50118, 50118,   133,  1948,    16,\n",
      "          2007,    35,     5,   168,    64,   304,  2358,     8,  5775,  1986,\n",
      "             7,  5217,   776,  3872, 27366,     4, 50118, 50118,   133,   168,\n",
      "            64,   304,  2358,     8,  5775,  1986,     7,  5217,   776,  3872,\n",
      "         27366,     4, 50118, 50118,   133,   168,    64,   304,  2358,     8,\n",
      "          5775,  1986,     7,  5217,   776,  3872, 27366,     4, 50118, 50118,\n",
      "           133,   168,    64,   304,  2358,     8,  5775,  1986,     7,  5217,\n",
      "           776,  3872, 27366,     4, 50118, 50118,   133,   168,    64,   304,\n",
      "          2358,     8,  5775,  1986,     7,  5217,   776,  3872, 27366,     4,\n",
      "         50118, 50118,   133,   168,    64,   304,  2358,     8,  5775,  1986,\n",
      "             7,  5217,   776,  3872, 27366,     4, 50118, 50118,   133,   168,\n",
      "            64,   304,  2358,     8,  5775,  1986,     7,  5217,   776,  3872,\n",
      "         27366,     4, 50118, 50118,   133,   168,    64,   304,  2358,     8,\n",
      "          5775,  1986,     7,  5217,   776,  3872, 27366,     4, 50118, 50118,\n",
      "           133,   168,    64,   304,  2358,     8,  5775,  1986,     7,  5217,\n",
      "           776,  3872, 27366,     4, 50118, 50118,   133,   168,    64,   304,\n",
      "          2358,     8,  5775,  1986,     7,  5217,   776,  3872, 27366,     4,\n",
      "         50118, 50118,   133,   168,    64,   304,  2358,     8,  5775,  1986,\n",
      "             7,  5217,   776,  3872, 27366,     4, 50118, 50118,   133,   168,\n",
      "            64,   304,  2358,     8,  5775,  1986,     7,  5217,   776,  3872,\n",
      "         27366,     4, 50118, 50118,   133]], device='cuda:0')\n",
      "text_energy_per_token: [9.501395126946028]\n",
      "output_tokens: 215\n",
      "flop: 53738000135\n",
      "energy_consumed:  2042.7999522933958\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.78%     171.828ms        21.59%     235.058ms      16.323us     122.186ms        53.04%     122.186ms       8.485us         14400     36352.033  \n",
      "                                               aten::mm         0.27%       2.906ms         0.38%       4.088ms      20.442us      44.930ms        19.50%      44.930ms     224.650us           200     16524.607  \n",
      "                                              aten::bmm         5.32%      57.945ms         7.18%      78.106ms      16.272us       8.133ms         3.53%       8.133ms       1.694us          4800       851.927  \n",
      "                                              aten::add         5.83%      63.417ms         8.83%      96.109ms      12.011us       9.751ms         4.23%       9.751ms       1.219us          8002         7.437  \n",
      "                                              aten::mul         2.45%      26.704ms         3.60%      39.220ms      13.060us       3.240ms         1.41%       3.240ms       1.079us          3003         1.996  \n",
      "                                            aten::empty         4.73%      51.533ms         4.73%      51.533ms       2.733us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.34%       3.744ms         6.37%      69.293ms      16.451us       0.000us         0.00%       1.494ms       0.355us          4212            --  \n",
      "                                         aten::_to_copy         0.75%       8.202ms         6.02%      65.549ms      23.369us       0.000us         0.00%       1.494ms       0.533us          2805            --  \n",
      "                                    aten::empty_strided         1.28%      13.932ms         1.28%      13.932ms       4.635us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.46%      15.930ms         5.00%      54.398ms      14.847us       2.409ms         1.05%       2.409ms       0.658us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.089s\n",
      "Self CUDA time total: 230.369ms\n",
      "\n",
      "output: tensor([[    2,  6179,    64,  3233, 16085,  2358,     8,  5775,  1986,     7,\n",
      "          5217,   776,  3872, 27366,   116, 50118, 50118,   133,  1948,    16,\n",
      "          2007,    35,     5,   168,    64,   304,  2358,     8,  5775,  1986,\n",
      "             7,  5217,   776,  3872, 27366,     4, 50118, 50118,   133,   168,\n",
      "            64,   304,  2358,     8,  5775,  1986,     7,  5217,   776,  3872,\n",
      "         27366,     4, 50118, 50118,   133,   168,    64,   304,  2358,     8,\n",
      "          5775,  1986,     7,  5217,   776,  3872, 27366,     4, 50118, 50118,\n",
      "           133,   168,    64,   304,  2358,     8,  5775,  1986,     7,  5217,\n",
      "           776,  3872, 27366,     4, 50118, 50118,   133,   168,    64,   304,\n",
      "          2358,     8,  5775,  1986,     7,  5217,   776,  3872, 27366,     4,\n",
      "         50118, 50118,   133,   168,    64,   304,  2358,     8,  5775,  1986,\n",
      "             7,  5217,   776,  3872, 27366,     4, 50118, 50118,   133,   168,\n",
      "            64,   304,  2358,     8,  5775,  1986,     7,  5217,   776,  3872,\n",
      "         27366,     4, 50118, 50118,   133,   168,    64,   304,  2358,     8,\n",
      "          5775,  1986,     7,  5217,   776,  3872, 27366,     4, 50118, 50118,\n",
      "           133,   168,    64,   304,  2358,     8,  5775,  1986,     7,  5217,\n",
      "           776,  3872, 27366,     4, 50118, 50118,   133,   168,    64,   304,\n",
      "          2358,     8,  5775,  1986,     7,  5217,   776,  3872, 27366,     4,\n",
      "         50118, 50118,   133,   168,    64,   304,  2358,     8,  5775,  1986,\n",
      "             7,  5217,   776,  3872, 27366,     4, 50118, 50118,   133,   168,\n",
      "            64,   304,  2358,     8,  5775,  1986,     7,  5217,   776,  3872,\n",
      "         27366,     4, 50118, 50118,   133]], device='cuda:0')\n",
      "text_energy_per_token: [9.501395126946028, 9.144692846501927]\n",
      "output_tokens: 215\n",
      "flop: 53738000135\n",
      "energy_consumed:  1966.1089619979143\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.76%     172.292ms        21.56%     235.682ms      16.367us     122.212ms        53.01%     122.212ms       8.487us         14400     37031.510  \n",
      "                                               aten::mm         0.38%       4.198ms         0.50%       5.429ms      27.144us      44.910ms        19.48%      44.910ms     224.548us           200     16833.479  \n",
      "                                              aten::bmm         5.41%      59.119ms         7.25%      79.276ms      16.516us       8.290ms         3.60%       8.290ms       1.727us          4800       886.284  \n",
      "                                              aten::add         6.03%      65.980ms         9.04%      98.802ms      12.347us       9.758ms         4.23%       9.758ms       1.219us          8002         7.648  \n",
      "                                              aten::mul         2.32%      25.370ms         3.46%      37.846ms      12.603us       3.241ms         1.41%       3.241ms       1.079us          3003         2.033  \n",
      "                                            aten::empty         4.81%      52.627ms         4.81%      52.627ms       2.791us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.773ms         6.22%      67.956ms      16.134us       0.000us         0.00%       1.500ms       0.356us          4212            --  \n",
      "                                         aten::_to_copy         0.74%       8.116ms         5.87%      64.183ms      22.882us       0.000us         0.00%       1.500ms       0.535us          2805            --  \n",
      "                                    aten::empty_strided         1.30%      14.230ms         1.30%      14.230ms       4.734us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.45%      15.905ms         4.82%      52.740ms      14.394us       2.415ms         1.05%       2.415ms       0.659us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.093s\n",
      "Self CUDA time total: 230.558ms\n",
      "\n",
      "output: tensor([[    2,  6179,   109,  2777,     8,  4106,  7926,  3327,     5,   169,\n",
      "            82,  8469,     8,  1026,  4158,    11, 30286, 17537,   116, 50118,\n",
      "         50118,   133,   892,     6,  1027,    11,     5,  8812,  3574, 32255,\n",
      "             6,   303,    14,  2777,     8,  4106,  7926,  3327,     5,   169,\n",
      "            82,  8469,     8,  1026,  4158,    11, 30286, 17537,     4, 50118,\n",
      "         50118,    17,    48, 46969,     8,  4106,  7926,  3327,     5,   169,\n",
      "            82,  8469,     8,  1026,  4158,    11, 30286, 17537,     6,    17,\n",
      "            46,   161,   892,  1029,    12, 11515,   925,     4,   871,   305,\n",
      "             4,   289,  3343,  2596,     6, 15221,     6,    10,  3097,     9,\n",
      "         16797,    23,     5,   589,     9,   886,     6,   764,  2659,     4,\n",
      "            44,    48, 46969,     8,  4106,  7926,  3327,     5,   169,    82,\n",
      "          8469,     8,  1026,  4158,    11, 30286, 17537,     4,    17,    46,\n",
      "         50118, 50118,   133,   892,     6,  1027,    11,     5,  8812,  3574,\n",
      "         32255,     6,   303,    14,  2777,     8,  4106,  7926,  3327,     5,\n",
      "           169,    82,  8469,     8,  1026,  4158,    11, 30286, 17537,     4,\n",
      "         50118, 50118,    17,    48, 46969,     8,  4106,  7926,  3327,     5,\n",
      "           169,    82,  8469,     8,  1026,  4158,    11, 30286, 17537,     6,\n",
      "            17,    46,   161,   892,  1029,    12, 11515,   925,     4,   871,\n",
      "           305,     4,   289,  3343,  2596,     6, 15221,     6,    10,  3097,\n",
      "             9, 16797,    23,     5,   589,     9,   886,     6,   764,  2659,\n",
      "             4,    44,    48, 46969,     8,  4106,  7926,  3327,     5,   169,\n",
      "            82,  8469,     8,  1026,  4158,    11, 30286, 17537,     4]],\n",
      "       device='cuda:0')\n",
      "text_energy_per_token: [9.60340019070547]\n",
      "output_tokens: 219\n",
      "flop: 54760954491\n",
      "energy_consumed:  2103.144641764498\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.15%     181.415ms        22.01%     247.259ms      17.171us     122.209ms        53.01%     122.209ms       8.487us         14400     37031.510  \n",
      "                                               aten::mm         0.25%       2.826ms         0.36%       4.071ms      20.353us      44.896ms        19.47%      44.896ms     224.478us           200     16833.479  \n",
      "                                              aten::bmm         5.10%      57.332ms         6.92%      77.712ms      16.190us       8.290ms         3.60%       8.290ms       1.727us          4800       886.284  \n",
      "                                              aten::add         5.61%      63.004ms         8.55%      96.027ms      12.000us       9.761ms         4.23%       9.761ms       1.220us          8002         7.648  \n",
      "                                              aten::mul         2.29%      25.697ms         3.42%      38.452ms      12.805us       3.234ms         1.40%       3.234ms       1.077us          3003         2.033  \n",
      "                                            aten::empty         4.94%      55.471ms         4.94%      55.471ms       2.941us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.938ms         6.28%      70.595ms      16.760us       0.000us         0.00%       1.498ms       0.356us          4212            --  \n",
      "                                         aten::_to_copy         0.77%       8.614ms         5.93%      66.657ms      23.764us       0.000us         0.00%       1.498ms       0.534us          2805            --  \n",
      "                                    aten::empty_strided         1.29%      14.510ms         1.29%      14.510ms       4.827us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.43%      16.054ms         4.87%      54.677ms      14.923us       2.415ms         1.05%       2.415ms       0.659us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.124s\n",
      "Self CUDA time total: 230.548ms\n",
      "\n",
      "output: tensor([[    2,  6179,   109,  2777,     8,  4106,  7926,  3327,     5,   169,\n",
      "            82,  8469,     8,  1026,  4158,    11, 30286, 17537,   116, 50118,\n",
      "         50118,   133,   892,     6,  1027,    11,     5,  8812,  3574, 32255,\n",
      "             6,   303,    14,  2777,     8,  4106,  7926,  3327,     5,   169,\n",
      "            82,  8469,     8,  1026,  4158,    11, 30286, 17537,     4, 50118,\n",
      "         50118,    17,    48, 46969,     8,  4106,  7926,  3327,     5,   169,\n",
      "            82,  8469,     8,  1026,  4158,    11, 30286, 17537,     6,    17,\n",
      "            46,   161,   892,  1029,    12, 11515,   925,     4,   871,   305,\n",
      "             4,   289,  3343,  2596,     6, 15221,     6,    10,  3097,     9,\n",
      "         16797,    23,     5,   589,     9,   886,     6,   764,  2659,     4,\n",
      "            44,    48, 46969,     8,  4106,  7926,  3327,     5,   169,    82,\n",
      "          8469,     8,  1026,  4158,    11, 30286, 17537,     4,    17,    46,\n",
      "         50118, 50118,   133,   892,     6,  1027,    11,     5,  8812,  3574,\n",
      "         32255,     6,   303,    14,  2777,     8,  4106,  7926,  3327,     5,\n",
      "           169,    82,  8469,     8,  1026,  4158,    11, 30286, 17537,     4,\n",
      "         50118, 50118,    17,    48, 46969,     8,  4106,  7926,  3327,     5,\n",
      "           169,    82,  8469,     8,  1026,  4158,    11, 30286, 17537,     6,\n",
      "            17,    46,   161,   892,  1029,    12, 11515,   925,     4,   871,\n",
      "           305,     4,   289,  3343,  2596,     6, 15221,     6,    10,  3097,\n",
      "             9, 16797,    23,     5,   589,     9,   886,     6,   764,  2659,\n",
      "             4,    44,    48, 46969,     8,  4106,  7926,  3327,     5,   169,\n",
      "            82,  8469,     8,  1026,  4158,    11, 30286, 17537,     4]],\n",
      "       device='cuda:0')\n",
      "text_energy_per_token: [9.60340019070547, 9.799414799980594]\n",
      "output_tokens: 219\n",
      "flop: 54760954491\n",
      "energy_consumed:  2146.07184119575\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.46%     176.369ms        21.17%     241.551ms      16.774us     122.167ms        52.98%     122.167ms       8.484us         14400     37371.249  \n",
      "                                               aten::mm         0.37%       4.214ms         0.48%       5.472ms      27.359us      44.923ms        19.48%      44.923ms     224.616us           200     16987.914  \n",
      "                                              aten::bmm         5.31%      60.553ms         7.11%      81.161ms      16.908us       8.307ms         3.60%       8.307ms       1.731us          4800       903.905  \n",
      "                                              aten::add         5.93%      67.646ms         8.84%     100.829ms      12.600us       9.765ms         4.23%       9.765ms       1.220us          8002         7.756  \n",
      "                                              aten::mul         2.27%      25.889ms         3.37%      38.490ms      12.817us       3.243ms         1.41%       3.243ms       1.080us          3003         2.052  \n",
      "                                            aten::empty         4.68%      53.356ms         4.68%      53.356ms       2.829us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.36%       4.068ms         6.22%      70.934ms      16.841us       0.000us         0.00%       1.506ms       0.358us          4212            --  \n",
      "                                         aten::_to_copy         0.77%       8.784ms         5.86%      66.866ms      23.838us       0.000us         0.00%       1.506ms       0.537us          2805            --  \n",
      "                                    aten::empty_strided         1.27%      14.438ms         1.27%      14.438ms       4.803us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.41%      16.045ms         4.80%      54.814ms      14.960us       2.421ms         1.05%       2.421ms       0.661us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.141s\n",
      "Self CUDA time total: 230.578ms\n",
      "\n",
      "output: tensor([[    2, 47066, 21700,    10,  5665,   147,  7350,  2316,   115,    28,\n",
      "           341,     7,  1477,     5,  1318,     8,  5838,     9,  3717,  2996,\n",
      "             4, 50118, 50118,  2264,    74,    47,   101,     7,   192,    11,\n",
      "             5,   499,   116, 50118, 50118,  2264,    74,    47,   101,     7,\n",
      "           192,    11,     5,   499,   116, 50118, 50118,  2264,    74,    47,\n",
      "           101,     7,   192,    11,     5,   499,   116, 50118, 50118,  2264,\n",
      "            74,    47,   101,     7,   192,    11,     5,   499,   116, 50118,\n",
      "         50118,  2264,    74,    47,   101,     7,   192,    11,     5,   499,\n",
      "           116, 50118, 50118,  2264,    74,    47,   101,     7,   192,    11,\n",
      "             5,   499,   116, 50118, 50118,  2264,    74,    47,   101,     7,\n",
      "           192,    11,     5,   499,   116, 50118, 50118,  2264,    74,    47,\n",
      "           101,     7,   192,    11,     5,   499,   116, 50118, 50118,  2264,\n",
      "            74,    47,   101,     7,   192,    11,     5,   499,   116, 50118,\n",
      "         50118,  2264,    74,    47,   101,     7,   192,    11,     5,   499,\n",
      "           116, 50118, 50118,  2264,    74,    47,   101,     7,   192,    11,\n",
      "             5,   499,   116, 50118, 50118,  2264,    74,    47,   101,     7,\n",
      "           192,    11,     5,   499,   116, 50118, 50118,  2264,    74,    47,\n",
      "           101,     7,   192,    11,     5,   499,   116, 50118, 50118,  2264,\n",
      "            74,    47,   101,     7,   192,    11,     5,   499,   116, 50118,\n",
      "         50118,  2264,    74,    47,   101,     7,   192,    11,     5,   499,\n",
      "           116, 50118, 50118,  2264,    74,    47,   101,     7,   192,    11,\n",
      "             5,   499,   116, 50118, 50118,  2264,    74,    47,   101,     7,\n",
      "           192]], device='cuda:0')\n",
      "text_energy_per_token: [9.032529919601888]\n",
      "output_tokens: 221\n",
      "flop: 55272875765\n",
      "energy_consumed:  1996.1891122320174\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.12%     180.986ms        21.88%     245.624ms      17.057us     122.112ms        52.98%     122.112ms       8.480us         14400     37371.249  \n",
      "                                               aten::mm         0.25%       2.834ms         0.37%       4.098ms      20.488us      44.903ms        19.48%      44.903ms     224.516us           200     16987.914  \n",
      "                                              aten::bmm         5.11%      57.314ms         6.92%      77.684ms      16.184us       8.305ms         3.60%       8.305ms       1.730us          4800       903.905  \n",
      "                                              aten::add         5.62%      63.088ms         8.56%      96.083ms      12.007us       9.757ms         4.23%       9.757ms       1.219us          8002         7.756  \n",
      "                                              aten::mul         2.29%      25.747ms         3.41%      38.279ms      12.747us       3.243ms         1.41%       3.243ms       1.080us          3003         2.052  \n",
      "                                            aten::empty         5.06%      56.747ms         5.06%      56.747ms       3.009us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.36%       3.998ms         6.18%      69.360ms      16.467us       0.000us         0.00%       1.504ms       0.357us          4212            --  \n",
      "                                         aten::_to_copy         0.77%       8.684ms         5.82%      65.361ms      23.302us       0.000us         0.00%       1.504ms       0.536us          2805            --  \n",
      "                                    aten::empty_strided         1.41%      15.837ms         1.41%      15.837ms       5.268us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.43%      16.026ms         4.73%      53.095ms      14.491us       2.420ms         1.05%       2.420ms       0.661us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.122s\n",
      "Self CUDA time total: 230.490ms\n",
      "\n",
      "output: tensor([[    2, 47066, 21700,    10,  5665,   147,  7350,  2316,   115,    28,\n",
      "           341,     7,  1477,     5,  1318,     8,  5838,     9,  3717,  2996,\n",
      "             4, 50118, 50118,  2264,    74,    47,   101,     7,   192,    11,\n",
      "             5,   499,   116, 50118, 50118,  2264,    74,    47,   101,     7,\n",
      "           192,    11,     5,   499,   116, 50118, 50118,  2264,    74,    47,\n",
      "           101,     7,   192,    11,     5,   499,   116, 50118, 50118,  2264,\n",
      "            74,    47,   101,     7,   192,    11,     5,   499,   116, 50118,\n",
      "         50118,  2264,    74,    47,   101,     7,   192,    11,     5,   499,\n",
      "           116, 50118, 50118,  2264,    74,    47,   101,     7,   192,    11,\n",
      "             5,   499,   116, 50118, 50118,  2264,    74,    47,   101,     7,\n",
      "           192,    11,     5,   499,   116, 50118, 50118,  2264,    74,    47,\n",
      "           101,     7,   192,    11,     5,   499,   116, 50118, 50118,  2264,\n",
      "            74,    47,   101,     7,   192,    11,     5,   499,   116, 50118,\n",
      "         50118,  2264,    74,    47,   101,     7,   192,    11,     5,   499,\n",
      "           116, 50118, 50118,  2264,    74,    47,   101,     7,   192,    11,\n",
      "             5,   499,   116, 50118, 50118,  2264,    74,    47,   101,     7,\n",
      "           192,    11,     5,   499,   116, 50118, 50118,  2264,    74,    47,\n",
      "           101,     7,   192,    11,     5,   499,   116, 50118, 50118,  2264,\n",
      "            74,    47,   101,     7,   192,    11,     5,   499,   116, 50118,\n",
      "         50118,  2264,    74,    47,   101,     7,   192,    11,     5,   499,\n",
      "           116, 50118, 50118,  2264,    74,    47,   101,     7,   192,    11,\n",
      "             5,   499,   116, 50118, 50118,  2264,    74,    47,   101,     7,\n",
      "           192]], device='cuda:0')\n",
      "text_energy_per_token: [9.032529919601888, 9.160851598062127]\n",
      "output_tokens: 221\n",
      "flop: 55272875765\n",
      "energy_consumed:  2024.5482031717302\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.92%     184.426ms        21.72%     251.588ms      17.471us     122.308ms        53.00%     122.308ms       8.494us         14400     38220.595  \n",
      "                                               aten::mm         0.25%       2.862ms         0.36%       4.138ms      20.689us      44.865ms        19.44%      44.865ms     224.323us           200     17374.003  \n",
      "                                              aten::bmm         5.01%      57.979ms         6.79%      78.618ms      16.379us       8.351ms         3.62%       8.351ms       1.740us          4800       949.248  \n",
      "                                              aten::add         5.51%      63.857ms         8.38%      97.079ms      12.132us       9.752ms         4.23%       9.752ms       1.219us          8002         8.029  \n",
      "                                              aten::mul         2.24%      25.972ms         3.34%      38.663ms      12.875us       3.241ms         1.40%       3.241ms       1.079us          3003         2.099  \n",
      "                                            aten::empty         5.10%      59.041ms         5.10%      59.041ms       3.131us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       4.084ms         6.19%      71.720ms      17.028us       0.000us         0.00%       1.506ms       0.357us          4212            --  \n",
      "                                         aten::_to_copy         0.77%       8.952ms         5.84%      67.636ms      24.113us       0.000us         0.00%       1.506ms       0.537us          2805            --  \n",
      "                                    aten::empty_strided         1.40%      16.190ms         1.40%      16.190ms       5.386us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.39%      16.109ms         4.74%      54.947ms      14.996us       2.419ms         1.05%       2.419ms       0.660us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.158s\n",
      "Self CUDA time total: 230.785ms\n",
      "\n",
      "output: tensor([[    2, 43043,  1851,     5,   609,     9, 10596,  5390,   634,  4307,\n",
      "          1729,  4454,    12, 36061,   466,   806,     6,     8,  2268,    63,\n",
      "           801,  2975,     8, 13557,  8819,     4, 50118, 50118,   133,  4307,\n",
      "          1729,  4454,    12, 36061,   466,   806,    16,    10,    92,   806,\n",
      "            14,    16,   145,  2226,    30,     5,   589,     9,   886,     6,\n",
      "         10817,     6,     8,    16,   145,   341,     7,  1045,    10,    92,\n",
      "         10596,  5390,   467,     4,    20,   806,    16,   145,   341,     7,\n",
      "          1045,    10,    92, 10596,  5390,   467,    14,    16,   145,   341,\n",
      "             7,  1045,    10,    92, 10596,  5390,   467,    14,    16,   145,\n",
      "           341,     7,  1045,    10,    92, 10596,  5390,   467,    14,    16,\n",
      "           145,   341,     7,  1045,    10,    92, 10596,  5390,   467,    14,\n",
      "            16,   145,   341,     7,  1045,    10,    92, 10596,  5390,   467,\n",
      "            14,    16,   145,   341,     7,  1045,    10,    92, 10596,  5390,\n",
      "           467,    14,    16,   145,   341,     7,  1045,    10,    92, 10596,\n",
      "          5390,   467,    14,    16,   145,   341,     7,  1045,    10,    92,\n",
      "         10596,  5390,   467,    14,    16,   145,   341,     7,  1045,    10,\n",
      "            92, 10596,  5390,   467,    14,    16,   145,   341,     7,  1045,\n",
      "            10,    92, 10596,  5390,   467,    14,    16,   145,   341,     7,\n",
      "          1045,    10,    92, 10596,  5390,   467,    14,    16,   145,   341,\n",
      "             7,  1045,    10,    92, 10596,  5390,   467,    14,    16,   145,\n",
      "           341,     7,  1045,    10,    92, 10596,  5390,   467,    14,    16,\n",
      "           145,   341,     7,  1045,    10,    92, 10596,  5390,   467,    14,\n",
      "            16,   145,   341,     7,  1045,    10]], device='cuda:0')\n",
      "text_energy_per_token: [9.88010567788065]\n",
      "output_tokens: 226\n",
      "flop: 56553974230\n",
      "energy_consumed:  2232.903883201027\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.05%     174.934ms        21.86%     238.281ms      16.547us     122.194ms        52.98%     122.194ms       8.486us         14400     38220.595  \n",
      "                                               aten::mm         0.26%       2.818ms         0.37%       4.065ms      20.323us      44.848ms        19.44%      44.848ms     224.242us           200     17374.003  \n",
      "                                              aten::bmm         5.24%      57.114ms         7.08%      77.192ms      16.082us       8.349ms         3.62%       8.349ms       1.739us          4800       949.248  \n",
      "                                              aten::add         5.69%      62.016ms         8.70%      94.786ms      11.845us       9.762ms         4.23%       9.762ms       1.220us          8002         8.029  \n",
      "                                              aten::mul         2.33%      25.394ms         3.49%      38.021ms      12.661us       3.241ms         1.41%       3.241ms       1.079us          3003         2.099  \n",
      "                                            aten::empty         4.98%      54.316ms         4.98%      54.316ms       2.880us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.779ms         6.35%      69.225ms      16.435us       0.000us         0.00%       1.510ms       0.358us          4212            --  \n",
      "                                         aten::_to_copy         0.75%       8.148ms         6.00%      65.446ms      23.332us       0.000us         0.00%       1.510ms       0.538us          2805            --  \n",
      "                                    aten::empty_strided         1.40%      15.265ms         1.40%      15.265ms       5.078us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.46%      15.937ms         4.98%      54.285ms      14.816us       2.426ms         1.05%       2.426ms       0.662us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.090s\n",
      "Self CUDA time total: 230.661ms\n",
      "\n",
      "output: tensor([[    2, 43043,  1851,     5,   609,     9, 10596,  5390,   634,  4307,\n",
      "          1729,  4454,    12, 36061,   466,   806,     6,     8,  2268,    63,\n",
      "           801,  2975,     8, 13557,  8819,     4, 50118, 50118,   133,  4307,\n",
      "          1729,  4454,    12, 36061,   466,   806,    16,    10,    92,   806,\n",
      "            14,    16,   145,  2226,    30,     5,   589,     9,   886,     6,\n",
      "         10817,     6,     8,    16,   145,   341,     7,  1045,    10,    92,\n",
      "         10596,  5390,   467,     4,    20,   806,    16,   145,   341,     7,\n",
      "          1045,    10,    92, 10596,  5390,   467,    14,    16,   145,   341,\n",
      "             7,  1045,    10,    92, 10596,  5390,   467,    14,    16,   145,\n",
      "           341,     7,  1045,    10,    92, 10596,  5390,   467,    14,    16,\n",
      "           145,   341,     7,  1045,    10,    92, 10596,  5390,   467,    14,\n",
      "            16,   145,   341,     7,  1045,    10,    92, 10596,  5390,   467,\n",
      "            14,    16,   145,   341,     7,  1045,    10,    92, 10596,  5390,\n",
      "           467,    14,    16,   145,   341,     7,  1045,    10,    92, 10596,\n",
      "          5390,   467,    14,    16,   145,   341,     7,  1045,    10,    92,\n",
      "         10596,  5390,   467,    14,    16,   145,   341,     7,  1045,    10,\n",
      "            92, 10596,  5390,   467,    14,    16,   145,   341,     7,  1045,\n",
      "            10,    92, 10596,  5390,   467,    14,    16,   145,   341,     7,\n",
      "          1045,    10,    92, 10596,  5390,   467,    14,    16,   145,   341,\n",
      "             7,  1045,    10,    92, 10596,  5390,   467,    14,    16,   145,\n",
      "           341,     7,  1045,    10,    92, 10596,  5390,   467,    14,    16,\n",
      "           145,   341,     7,  1045,    10,    92, 10596,  5390,   467,    14,\n",
      "            16,   145,   341,     7,  1045,    10]], device='cuda:0')\n",
      "text_energy_per_token: [9.88010567788065, 8.78710965975871]\n",
      "output_tokens: 226\n",
      "flop: 56553974230\n",
      "energy_consumed:  1985.8867831054688\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.76%     172.471ms        21.55%     235.806ms      16.375us     122.254ms        53.01%     122.254ms       8.490us         14400     37201.379  \n",
      "                                               aten::mm         0.38%       4.176ms         0.49%       5.409ms      27.046us      44.900ms        19.47%      44.900ms     224.501us           200     16910.696  \n",
      "                                              aten::bmm         5.40%      59.102ms         7.25%      79.374ms      16.536us       8.305ms         3.60%       8.305ms       1.730us          4800       895.058  \n",
      "                                              aten::add         6.04%      66.090ms         9.04%      98.938ms      12.364us       9.771ms         4.24%       9.771ms       1.221us          8002         7.702  \n",
      "                                              aten::mul         2.33%      25.448ms         3.47%      37.924ms      12.629us       3.246ms         1.41%       3.246ms       1.081us          3003         2.043  \n",
      "                                            aten::empty         4.82%      52.797ms         4.82%      52.797ms       2.800us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.785ms         6.17%      67.524ms      16.031us       0.000us         0.00%       1.487ms       0.353us          4212            --  \n",
      "                                         aten::_to_copy         0.74%       8.090ms         5.82%      63.739ms      22.723us       0.000us         0.00%       1.487ms       0.530us          2805            --  \n",
      "                                    aten::empty_strided         1.29%      14.070ms         1.29%      14.070ms       4.681us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.46%      15.970ms         4.80%      52.538ms      14.339us       2.401ms         1.04%       2.401ms       0.655us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.094s\n",
      "Self CUDA time total: 230.623ms\n",
      "\n",
      "output: tensor([[    2,  6179,   109, 32509,   173,     7,  1744,  2172,     8,  1822,\n",
      "            31, 19166,  6357,     6,     8,    99,    16, 19400, 17381,   116,\n",
      "         50118,   133,   315,   532,    34,    57,    11,     5, 11201,     9,\n",
      "            10,   720, 23387, 14414,    13,     5,   375,   367,   107,     4,\n",
      "            20, 23387, 14414,    34,    57,    10,   720, 23387, 14414,    13,\n",
      "             5,   375,   367,   107,     6,     8,     5,   315,   532,    34,\n",
      "            57,    11,     5, 11201,     9,    10,   720, 23387, 14414,    13,\n",
      "             5,   375,   367,   107,     4,    20, 23387, 14414,    34,    57,\n",
      "            10,   720, 23387, 14414,    13,     5,   375,   367,   107,     6,\n",
      "             8,     5,   315,   532,    34,    57,    11,     5, 11201,     9,\n",
      "            10,   720, 23387, 14414,    13,     5,   375,   367,   107,     4,\n",
      "         50118,   133,   315,   532,    34,    57,    11,     5, 11201,     9,\n",
      "            10,   720, 23387, 14414,    13,     5,   375,   367,   107,     4,\n",
      "            20, 23387, 14414,    34,    57,    10,   720, 23387, 14414,    13,\n",
      "             5,   375,   367,   107,     6,     8,     5,   315,   532,    34,\n",
      "            57,    11,     5, 11201,     9,    10,   720, 23387, 14414,    13,\n",
      "             5,   375,   367,   107,     4, 50118,   133,   315,   532,    34,\n",
      "            57,    11,     5, 11201,     9,    10,   720, 23387, 14414,    13,\n",
      "             5,   375,   367,   107,     4,    20, 23387, 14414,    34,    57,\n",
      "            10,   720, 23387, 14414,    13,     5,   375,   367,   107,     6,\n",
      "             8,     5,   315,   532,    34,    57,    11,     5, 11201,     9,\n",
      "            10,   720, 23387, 14414,    13,     5,   375,   367,   107,     4]],\n",
      "       device='cuda:0')\n",
      "text_energy_per_token: [10.080514990572606]\n",
      "output_tokens: 220\n",
      "flop: 55016878120\n",
      "energy_consumed:  2217.713297925973\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.04%     181.299ms        21.87%     247.136ms      17.162us     122.115ms        53.00%     122.115ms       8.480us         14400     37201.379  \n",
      "                                               aten::mm         0.25%       2.838ms         0.36%       4.072ms      20.361us      44.913ms        19.49%      44.913ms     224.567us           200     16910.696  \n",
      "                                              aten::bmm         5.07%      57.323ms         6.88%      77.739ms      16.196us       8.305ms         3.60%       8.305ms       1.730us          4800       895.058  \n",
      "                                              aten::add         5.57%      63.008ms         8.49%      95.993ms      11.996us       9.760ms         4.24%       9.760ms       1.220us          8002         7.702  \n",
      "                                              aten::mul         2.28%      25.739ms         3.40%      38.419ms      12.794us       3.243ms         1.41%       3.243ms       1.080us          3003         2.043  \n",
      "                                            aten::empty         5.12%      57.894ms         5.12%      57.894ms       3.070us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       4.011ms         6.20%      70.032ms      16.627us       0.000us         0.00%       1.485ms       0.353us          4212            --  \n",
      "                                         aten::_to_copy         0.77%       8.647ms         5.84%      66.021ms      23.537us       0.000us         0.00%       1.485ms       0.529us          2805            --  \n",
      "                                    aten::empty_strided         1.37%      15.516ms         1.37%      15.516ms       5.162us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.42%      16.034ms         4.79%      54.179ms      14.787us       2.399ms         1.04%       2.399ms       0.655us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.130s\n",
      "Self CUDA time total: 230.427ms\n",
      "\n",
      "output: tensor([[    2,  6179,   109, 32509,   173,     7,  1744,  2172,     8,  1822,\n",
      "            31, 19166,  6357,     6,     8,    99,    16, 19400, 17381,   116,\n",
      "         50118,   133,   315,   532,    34,    57,    11,     5, 11201,     9,\n",
      "            10,   720, 23387, 14414,    13,     5,   375,   367,   107,     4,\n",
      "            20, 23387, 14414,    34,    57,    10,   720, 23387, 14414,    13,\n",
      "             5,   375,   367,   107,     6,     8,     5,   315,   532,    34,\n",
      "            57,    11,     5, 11201,     9,    10,   720, 23387, 14414,    13,\n",
      "             5,   375,   367,   107,     4,    20, 23387, 14414,    34,    57,\n",
      "            10,   720, 23387, 14414,    13,     5,   375,   367,   107,     6,\n",
      "             8,     5,   315,   532,    34,    57,    11,     5, 11201,     9,\n",
      "            10,   720, 23387, 14414,    13,     5,   375,   367,   107,     4,\n",
      "         50118,   133,   315,   532,    34,    57,    11,     5, 11201,     9,\n",
      "            10,   720, 23387, 14414,    13,     5,   375,   367,   107,     4,\n",
      "            20, 23387, 14414,    34,    57,    10,   720, 23387, 14414,    13,\n",
      "             5,   375,   367,   107,     6,     8,     5,   315,   532,    34,\n",
      "            57,    11,     5, 11201,     9,    10,   720, 23387, 14414,    13,\n",
      "             5,   375,   367,   107,     4, 50118,   133,   315,   532,    34,\n",
      "            57,    11,     5, 11201,     9,    10,   720, 23387, 14414,    13,\n",
      "             5,   375,   367,   107,     4,    20, 23387, 14414,    34,    57,\n",
      "            10,   720, 23387, 14414,    13,     5,   375,   367,   107,     6,\n",
      "             8,     5,   315,   532,    34,    57,    11,     5, 11201,     9,\n",
      "            10,   720, 23387, 14414,    13,     5,   375,   367,   107,     4]],\n",
      "       device='cuda:0')\n",
      "text_energy_per_token: [10.080514990572606, 9.824999293370679]\n",
      "output_tokens: 220\n",
      "flop: 55016878120\n",
      "energy_consumed:  2161.4998445415495\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.64%     177.643ms        21.35%     242.510ms      16.841us     122.236ms        52.98%     122.236ms       8.489us         14400     38390.465  \n",
      "                                               aten::mm         0.37%       4.202ms         0.48%       5.444ms      27.222us      44.901ms        19.46%      44.901ms     224.504us           200     17451.221  \n",
      "                                              aten::bmm         5.22%      59.320ms         7.04%      79.978ms      16.662us       8.356ms         3.62%       8.356ms       1.741us          4800       958.538  \n",
      "                                              aten::add         5.94%      67.427ms         8.85%     100.552ms      12.566us       9.758ms         4.23%       9.758ms       1.220us          8002         8.084  \n",
      "                                              aten::mul         2.27%      25.782ms         3.37%      38.326ms      12.763us       3.245ms         1.41%       3.245ms       1.080us          3003         2.109  \n",
      "                                            aten::empty         4.79%      54.399ms         4.79%      54.399ms       2.885us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.998ms         6.19%      70.300ms      16.690us       0.000us         0.00%       1.486ms       0.353us          4212            --  \n",
      "                                         aten::_to_copy         0.77%       8.785ms         5.84%      66.301ms      23.637us       0.000us         0.00%       1.486ms       0.530us          2805            --  \n",
      "                                    aten::empty_strided         1.26%      14.308ms         1.26%      14.308ms       4.760us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.41%      16.034ms         4.78%      54.302ms      14.820us       2.401ms         1.04%       2.401ms       0.655us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.136s\n",
      "Self CUDA time total: 230.706ms\n",
      "\n",
      "output: tensor([[    2,  6179,   109,   592,   433,  4818,  2712,     5,   169,    82,\n",
      "         14623,     8,   458,   340,     6,     8,    99,    32,     5,   801,\n",
      "          8819,    13,     5,  2504,     9, 23038,   116, 50118, 50118,   133,\n",
      "           592,   433,  4818,    14,  2712,     5,   169,    82, 14623,     8,\n",
      "           458,   340,     6,     8,    99,    32,     5,   801,  8819,    13,\n",
      "             5,  2504,     9, 23038,   116, 50118, 50118,   133,   592,   433,\n",
      "          4818,    14,  2712,     5,   169,    82, 14623,     8,   458,   340,\n",
      "             6,     8,    99,    32,     5,   801,  8819,    13,     5,  2504,\n",
      "             9, 23038,   116, 50118, 50118,   133,   592,   433,  4818,    14,\n",
      "          2712,     5,   169,    82, 14623,     8,   458,   340,     6,     8,\n",
      "            99,    32,     5,   801,  8819,    13,     5,  2504,     9, 23038,\n",
      "           116, 50118, 50118,   133,   592,   433,  4818,    14,  2712,     5,\n",
      "           169,    82, 14623,     8,   458,   340,     6,     8,    99,    32,\n",
      "             5,   801,  8819,    13,     5,  2504,     9, 23038,   116, 50118,\n",
      "         50118,   133,   592,   433,  4818,    14,  2712,     5,   169,    82,\n",
      "         14623,     8,   458,   340,     6,     8,    99,    32,     5,   801,\n",
      "          8819,    13,     5,  2504,     9, 23038,   116, 50118, 50118,   133,\n",
      "           592,   433,  4818,    14,  2712,     5,   169,    82, 14623,     8,\n",
      "           458,   340,     6,     8,    99,    32,     5,   801,  8819,    13,\n",
      "             5,  2504,     9, 23038,   116, 50118, 50118,   133,   592,   433,\n",
      "          4818,    14,  2712,     5,   169,    82, 14623,     8,   458,   340,\n",
      "             6,     8,    99,    32,     5,   801,  8819,    13,     5,  2504,\n",
      "             9, 23038,   116, 50118, 50118,   133,   592]], device='cuda:0')\n",
      "text_energy_per_token: [9.52275183293788]\n",
      "output_tokens: 227\n",
      "flop: 56810415971\n",
      "energy_consumed:  2161.664666076899\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.08%     181.570ms        21.93%     247.701ms      17.201us     122.323ms        52.99%     122.323ms       8.495us         14400     38390.465  \n",
      "                                               aten::mm         0.25%       2.840ms         0.36%       4.069ms      20.347us      44.884ms        19.44%      44.884ms     224.420us           200     17451.221  \n",
      "                                              aten::bmm         5.07%      57.266ms         6.88%      77.684ms      16.184us       8.358ms         3.62%       8.358ms       1.741us          4800       958.538  \n",
      "                                              aten::add         5.60%      63.195ms         8.52%      96.262ms      12.030us       9.760ms         4.23%       9.760ms       1.220us          8002         8.084  \n",
      "                                              aten::mul         2.28%      25.736ms         3.39%      38.335ms      12.766us       3.248ms         1.41%       3.248ms       1.082us          3003         2.109  \n",
      "                                            aten::empty         5.13%      57.963ms         5.13%      57.963ms       3.074us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.36%       4.041ms         6.08%      68.669ms      16.303us       0.000us         0.00%       1.503ms       0.357us          4212            --  \n",
      "                                         aten::_to_copy         0.76%       8.632ms         5.72%      64.628ms      23.040us       0.000us         0.00%       1.503ms       0.536us          2805            --  \n",
      "                                    aten::empty_strided         1.35%      15.294ms         1.35%      15.294ms       5.088us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.42%      16.014ms         4.67%      52.729ms      14.391us       2.418ms         1.05%       2.418ms       0.660us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.129s\n",
      "Self CUDA time total: 230.845ms\n",
      "\n",
      "output: tensor([[    2,  6179,   109,   592,   433,  4818,  2712,     5,   169,    82,\n",
      "         14623,     8,   458,   340,     6,     8,    99,    32,     5,   801,\n",
      "          8819,    13,     5,  2504,     9, 23038,   116, 50118, 50118,   133,\n",
      "           592,   433,  4818,    14,  2712,     5,   169,    82, 14623,     8,\n",
      "           458,   340,     6,     8,    99,    32,     5,   801,  8819,    13,\n",
      "             5,  2504,     9, 23038,   116, 50118, 50118,   133,   592,   433,\n",
      "          4818,    14,  2712,     5,   169,    82, 14623,     8,   458,   340,\n",
      "             6,     8,    99,    32,     5,   801,  8819,    13,     5,  2504,\n",
      "             9, 23038,   116, 50118, 50118,   133,   592,   433,  4818,    14,\n",
      "          2712,     5,   169,    82, 14623,     8,   458,   340,     6,     8,\n",
      "            99,    32,     5,   801,  8819,    13,     5,  2504,     9, 23038,\n",
      "           116, 50118, 50118,   133,   592,   433,  4818,    14,  2712,     5,\n",
      "           169,    82, 14623,     8,   458,   340,     6,     8,    99,    32,\n",
      "             5,   801,  8819,    13,     5,  2504,     9, 23038,   116, 50118,\n",
      "         50118,   133,   592,   433,  4818,    14,  2712,     5,   169,    82,\n",
      "         14623,     8,   458,   340,     6,     8,    99,    32,     5,   801,\n",
      "          8819,    13,     5,  2504,     9, 23038,   116, 50118, 50118,   133,\n",
      "           592,   433,  4818,    14,  2712,     5,   169,    82, 14623,     8,\n",
      "           458,   340,     6,     8,    99,    32,     5,   801,  8819,    13,\n",
      "             5,  2504,     9, 23038,   116, 50118, 50118,   133,   592,   433,\n",
      "          4818,    14,  2712,     5,   169,    82, 14623,     8,   458,   340,\n",
      "             6,     8,    99,    32,     5,   801,  8819,    13,     5,  2504,\n",
      "             9, 23038,   116, 50118, 50118,   133,   592]], device='cuda:0')\n",
      "text_energy_per_token: [9.52275183293788, 9.407626163915811]\n",
      "output_tokens: 227\n",
      "flop: 56810415971\n",
      "energy_consumed:  2135.5311392088893\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.48%     178.942ms        21.16%     244.631ms      16.988us     122.237ms        52.97%     122.237ms       8.489us         14400     38560.334  \n",
      "                                               aten::mm         0.37%       4.234ms         0.48%       5.501ms      27.504us      44.911ms        19.46%      44.911ms     224.554us           200     17528.439  \n",
      "                                              aten::bmm         5.14%      59.464ms         6.94%      80.257ms      16.720us       8.370ms         3.63%       8.370ms       1.744us          4800       967.901  \n",
      "                                              aten::add         5.90%      68.153ms         8.77%     101.419ms      12.674us       9.772ms         4.23%       9.772ms       1.221us          8002         8.140  \n",
      "                                              aten::mul         2.25%      25.998ms         3.35%      38.673ms      12.878us       3.248ms         1.41%       3.248ms       1.082us          3003         2.118  \n",
      "                                            aten::empty         4.74%      54.847ms         4.74%      54.847ms       2.908us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       4.026ms         6.15%      71.052ms      16.869us       0.000us         0.00%       1.497ms       0.355us          4212            --  \n",
      "                                         aten::_to_copy         0.77%       8.955ms         5.80%      67.026ms      23.895us       0.000us         0.00%       1.497ms       0.534us          2805            --  \n",
      "                                    aten::empty_strided         1.27%      14.627ms         1.27%      14.627ms       4.866us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.39%      16.080ms         4.72%      54.594ms      14.900us       2.412ms         1.05%       2.412ms       0.658us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.156s\n",
      "Self CUDA time total: 230.785ms\n",
      "\n",
      "output: tensor([[    2,  6179,   109,  4106,     6,   592,     6,     8,   776,  2433,\n",
      "          2712,    82,    18,   689,  5717,     6,     8,   141,    64,    42,\n",
      "          2655,    28,   341,     7,  3720, 12732, 22669,   116, 50118, 50118,\n",
      "           133,   892,    21,  2964,    30,     5,   589,     9,   886,     6,\n",
      "         10817,     6,     8,     5,   589,     9,   886,     6, 10817,     6,\n",
      "             8,    21,  1027,    11,     5,  8812, 20056,     4, 50118, 50118,\n",
      "           133,   892,    21,  2964,    30,     5,   589,     9,   886,     6,\n",
      "         10817,     6,     8,     5,   589,     9,   886,     6, 10817,     6,\n",
      "             8,    21,  1027,    11,     5,  8812, 20056,     4, 50118, 50118,\n",
      "           133,   892,    21,  2964,    30,     5,   589,     9,   886,     6,\n",
      "         10817,     6,     8,     5,   589,     9,   886,     6, 10817,     6,\n",
      "             8,    21,  1027,    11,     5,  8812, 20056,     4, 50118, 50118,\n",
      "           133,   892,    21,  2964,    30,     5,   589,     9,   886,     6,\n",
      "         10817,     6,     8,     5,   589,     9,   886,     6, 10817,     6,\n",
      "             8,    21,  1027,    11,     5,  8812, 20056,     4, 50118, 50118,\n",
      "           133,   892,    21,  2964,    30,     5,   589,     9,   886,     6,\n",
      "         10817,     6,     8,     5,   589,     9,   886,     6, 10817,     6,\n",
      "             8,    21,  1027,    11,     5,  8812, 20056,     4, 50118, 50118,\n",
      "           133,   892,    21,  2964,    30,     5,   589,     9,   886,     6,\n",
      "         10817,     6,     8,     5,   589,     9,   886,     6, 10817,     6,\n",
      "             8,    21,  1027,    11,     5,  8812, 20056,     4, 50118, 50118,\n",
      "           133,   892,    21,  2964,    30,     5,   589,     9,   886,     6,\n",
      "         10817,     6,     8,     5,   589,     9,   886,     6]],\n",
      "       device='cuda:0')\n",
      "text_energy_per_token: [8.65712862955729]\n",
      "output_tokens: 228\n",
      "flop: 57066931728\n",
      "energy_consumed:  1973.8253275390623\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.25%     176.271ms        22.08%     239.579ms      16.637us     122.177ms        52.97%     122.177ms       8.484us         14400     38560.334  \n",
      "                                               aten::mm         0.26%       2.800ms         0.37%       4.048ms      20.242us      44.912ms        19.47%      44.912ms     224.560us           200     17528.439  \n",
      "                                              aten::bmm         5.25%      56.910ms         7.09%      76.950ms      16.031us       8.366ms         3.63%       8.366ms       1.743us          4800       967.901  \n",
      "                                              aten::add         5.70%      61.821ms         8.72%      94.625ms      11.825us       9.763ms         4.23%       9.763ms       1.220us          8002         8.140  \n",
      "                                              aten::mul         2.34%      25.373ms         3.50%      37.950ms      12.638us       3.243ms         1.41%       3.243ms       1.080us          3003         2.118  \n",
      "                                            aten::empty         4.96%      53.774ms         4.96%      53.774ms       2.852us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.753ms         6.16%      66.820ms      15.864us       0.000us         0.00%       1.487ms       0.353us          4212            --  \n",
      "                                         aten::_to_copy         0.75%       8.103ms         5.81%      63.067ms      22.484us       0.000us         0.00%       1.487ms       0.530us          2805            --  \n",
      "                                    aten::empty_strided         1.23%      13.303ms         1.23%      13.303ms       4.425us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.47%      15.910ms         4.97%      53.965ms      14.728us       2.402ms         1.04%       2.402ms       0.656us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.085s\n",
      "Self CUDA time total: 230.652ms\n",
      "\n",
      "output: tensor([[    2,  6179,   109,  4106,     6,   592,     6,     8,   776,  2433,\n",
      "          2712,    82,    18,   689,  5717,     6,     8,   141,    64,    42,\n",
      "          2655,    28,   341,     7,  3720, 12732, 22669,   116, 50118, 50118,\n",
      "           133,   892,    21,  2964,    30,     5,   589,     9,   886,     6,\n",
      "         10817,     6,     8,     5,   589,     9,   886,     6, 10817,     6,\n",
      "             8,    21,  1027,    11,     5,  8812, 20056,     4, 50118, 50118,\n",
      "           133,   892,    21,  2964,    30,     5,   589,     9,   886,     6,\n",
      "         10817,     6,     8,     5,   589,     9,   886,     6, 10817,     6,\n",
      "             8,    21,  1027,    11,     5,  8812, 20056,     4, 50118, 50118,\n",
      "           133,   892,    21,  2964,    30,     5,   589,     9,   886,     6,\n",
      "         10817,     6,     8,     5,   589,     9,   886,     6, 10817,     6,\n",
      "             8,    21,  1027,    11,     5,  8812, 20056,     4, 50118, 50118,\n",
      "           133,   892,    21,  2964,    30,     5,   589,     9,   886,     6,\n",
      "         10817,     6,     8,     5,   589,     9,   886,     6, 10817,     6,\n",
      "             8,    21,  1027,    11,     5,  8812, 20056,     4, 50118, 50118,\n",
      "           133,   892,    21,  2964,    30,     5,   589,     9,   886,     6,\n",
      "         10817,     6,     8,     5,   589,     9,   886,     6, 10817,     6,\n",
      "             8,    21,  1027,    11,     5,  8812, 20056,     4, 50118, 50118,\n",
      "           133,   892,    21,  2964,    30,     5,   589,     9,   886,     6,\n",
      "         10817,     6,     8,     5,   589,     9,   886,     6, 10817,     6,\n",
      "             8,    21,  1027,    11,     5,  8812, 20056,     4, 50118, 50118,\n",
      "           133,   892,    21,  2964,    30,     5,   589,     9,   886,     6,\n",
      "         10817,     6,     8,     5,   589,     9,   886,     6]],\n",
      "       device='cuda:0')\n",
      "text_energy_per_token: [8.65712862955729, 9.141871207248222]\n",
      "output_tokens: 228\n",
      "flop: 57066931728\n",
      "energy_consumed:  2084.3466352525948\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.16%     176.992ms        22.08%     241.831ms      16.794us     122.223ms        53.00%     122.223ms       8.488us         14400     37201.379  \n",
      "                                               aten::mm         0.26%       2.817ms         0.37%       4.062ms      20.309us      44.929ms        19.48%      44.929ms     224.643us           200     16910.696  \n",
      "                                              aten::bmm         5.15%      56.432ms         6.98%      76.482ms      15.934us       8.310ms         3.60%       8.310ms       1.731us          4800       895.058  \n",
      "                                              aten::add         5.66%      62.055ms         8.66%      94.877ms      11.857us       9.760ms         4.23%       9.760ms       1.220us          8002         7.702  \n",
      "                                              aten::mul         2.32%      25.397ms         3.45%      37.765ms      12.576us       3.246ms         1.41%       3.246ms       1.081us          3003         2.043  \n",
      "                                            aten::empty         5.19%      56.843ms         5.19%      56.843ms       3.014us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.805ms         6.20%      67.874ms      16.114us       0.000us         0.00%       1.491ms       0.354us          4212            --  \n",
      "                                         aten::_to_copy         0.74%       8.140ms         5.85%      64.069ms      22.841us       0.000us         0.00%       1.491ms       0.532us          2805            --  \n",
      "                                    aten::empty_strided         1.44%      15.760ms         1.44%      15.760ms       5.243us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.46%      15.944ms         4.79%      52.473ms      14.321us       2.406ms         1.04%       2.406ms       0.657us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.095s\n",
      "Self CUDA time total: 230.614ms\n",
      "\n",
      "output: tensor([[    2, 43043,  1851,     5,   609,     9,  1632,  4230,     8,   141,\n",
      "            24, 17992,     7,     5, 10795,     8, 14082,     9,  4707,     4,\n",
      "         50118, 50118, 43043,  1851,     5,   609,     9,  1632,  4230,     8,\n",
      "           141,    24, 17992,     7,     5, 10795,     8, 14082,     9,  4707,\n",
      "             4, 50118, 50118, 43043,  1851,     5,   609,     9,  1632,  4230,\n",
      "             8,   141,    24, 17992,     7,     5, 10795,     8, 14082,     9,\n",
      "          4707,     4, 50118, 50118, 43043,  1851,     5,   609,     9,  1632,\n",
      "          4230,     8,   141,    24, 17992,     7,     5, 10795,     8, 14082,\n",
      "             9,  4707,     4, 50118, 50118, 43043,  1851,     5,   609,     9,\n",
      "          1632,  4230,     8,   141,    24, 17992,     7,     5, 10795,     8,\n",
      "         14082,     9,  4707,     4, 50118, 50118, 43043,  1851,     5,   609,\n",
      "             9,  1632,  4230,     8,   141,    24, 17992,     7,     5, 10795,\n",
      "             8, 14082,     9,  4707,     4, 50118, 50118, 43043,  1851,     5,\n",
      "           609,     9,  1632,  4230,     8,   141,    24, 17992,     7,     5,\n",
      "         10795,     8, 14082,     9,  4707,     4, 50118, 50118, 43043,  1851,\n",
      "             5,   609,     9,  1632,  4230,     8,   141,    24, 17992,     7,\n",
      "             5, 10795,     8, 14082,     9,  4707,     4, 50118, 50118, 43043,\n",
      "          1851,     5,   609,     9,  1632,  4230,     8,   141,    24, 17992,\n",
      "             7,     5, 10795,     8, 14082,     9,  4707,     4, 50118, 50118,\n",
      "         43043,  1851,     5,   609,     9,  1632,  4230,     8,   141,    24,\n",
      "         17992,     7,     5, 10795,     8, 14082,     9,  4707,     4, 50118,\n",
      "         50118, 43043,  1851,     5,   609,     9,  1632,  4230,     8,   141]],\n",
      "       device='cuda:0')\n",
      "text_energy_per_token: [9.701405492167906]\n",
      "output_tokens: 220\n",
      "flop: 55016878120\n",
      "energy_consumed:  2134.3092082769394\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.51%     186.463ms        22.32%     252.136ms      17.509us     122.075ms        52.99%     122.075ms       8.477us         14400     37201.379  \n",
      "                                               aten::mm         0.26%       2.916ms         0.37%       4.170ms      20.848us      44.894ms        19.49%      44.894ms     224.469us           200     16910.696  \n",
      "                                              aten::bmm         5.27%      59.510ms         7.07%      79.814ms      16.628us       8.310ms         3.61%       8.310ms       1.731us          4800       895.058  \n",
      "                                              aten::add         5.54%      62.576ms         8.46%      95.522ms      11.937us       9.758ms         4.24%       9.758ms       1.219us          8002         7.702  \n",
      "                                              aten::mul         2.27%      25.610ms         3.38%      38.133ms      12.698us       3.244ms         1.41%       3.244ms       1.080us          3003         2.043  \n",
      "                                            aten::empty         5.11%      57.721ms         5.11%      57.721ms       3.061us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.952ms         6.17%      69.677ms      16.542us       0.000us         0.00%       1.491ms       0.354us          4212            --  \n",
      "                                         aten::_to_copy         0.75%       8.447ms         5.82%      65.725ms      23.431us       0.000us         0.00%       1.491ms       0.531us          2805            --  \n",
      "                                    aten::empty_strided         1.26%      14.243ms         1.26%      14.243ms       4.738us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.41%      15.927ms         4.80%      54.178ms      14.787us       2.405ms         1.04%       2.405ms       0.657us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.130s\n",
      "Self CUDA time total: 230.382ms\n",
      "\n",
      "output: tensor([[    2, 43043,  1851,     5,   609,     9,  1632,  4230,     8,   141,\n",
      "            24, 17992,     7,     5, 10795,     8, 14082,     9,  4707,     4,\n",
      "         50118, 50118, 43043,  1851,     5,   609,     9,  1632,  4230,     8,\n",
      "           141,    24, 17992,     7,     5, 10795,     8, 14082,     9,  4707,\n",
      "             4, 50118, 50118, 43043,  1851,     5,   609,     9,  1632,  4230,\n",
      "             8,   141,    24, 17992,     7,     5, 10795,     8, 14082,     9,\n",
      "          4707,     4, 50118, 50118, 43043,  1851,     5,   609,     9,  1632,\n",
      "          4230,     8,   141,    24, 17992,     7,     5, 10795,     8, 14082,\n",
      "             9,  4707,     4, 50118, 50118, 43043,  1851,     5,   609,     9,\n",
      "          1632,  4230,     8,   141,    24, 17992,     7,     5, 10795,     8,\n",
      "         14082,     9,  4707,     4, 50118, 50118, 43043,  1851,     5,   609,\n",
      "             9,  1632,  4230,     8,   141,    24, 17992,     7,     5, 10795,\n",
      "             8, 14082,     9,  4707,     4, 50118, 50118, 43043,  1851,     5,\n",
      "           609,     9,  1632,  4230,     8,   141,    24, 17992,     7,     5,\n",
      "         10795,     8, 14082,     9,  4707,     4, 50118, 50118, 43043,  1851,\n",
      "             5,   609,     9,  1632,  4230,     8,   141,    24, 17992,     7,\n",
      "             5, 10795,     8, 14082,     9,  4707,     4, 50118, 50118, 43043,\n",
      "          1851,     5,   609,     9,  1632,  4230,     8,   141,    24, 17992,\n",
      "             7,     5, 10795,     8, 14082,     9,  4707,     4, 50118, 50118,\n",
      "         43043,  1851,     5,   609,     9,  1632,  4230,     8,   141,    24,\n",
      "         17992,     7,     5, 10795,     8, 14082,     9,  4707,     4, 50118,\n",
      "         50118, 43043,  1851,     5,   609,     9,  1632,  4230,     8,   141]],\n",
      "       device='cuda:0')\n",
      "text_energy_per_token: [9.701405492167906, 9.540521937132857]\n",
      "output_tokens: 220\n",
      "flop: 55016878120\n",
      "energy_consumed:  2098.9148261692285\n",
      "Processing category: common-sense\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.55%     176.487ms        21.26%     241.286ms      16.756us     122.199ms        52.97%     122.199ms       8.486us         14400     38050.726  \n",
      "                                               aten::mm         0.37%       4.205ms         0.48%       5.462ms      27.311us      44.907ms        19.47%      44.907ms     224.536us           200     17296.785  \n",
      "                                              aten::bmm         5.23%      59.386ms         7.04%      79.936ms      16.653us       8.350ms         3.62%       8.350ms       1.740us          4800       940.032  \n",
      "                                              aten::add         5.93%      67.295ms         8.84%     100.372ms      12.543us       9.766ms         4.23%       9.766ms       1.220us          8002         7.973  \n",
      "                                              aten::mul         2.27%      25.792ms         3.38%      38.416ms      12.792us       3.250ms         1.41%       3.250ms       1.082us          3003         2.090  \n",
      "                                            aten::empty         4.92%      55.811ms         4.92%      55.811ms       2.960us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.955ms         6.22%      70.577ms      16.756us       0.000us         0.00%       1.505ms       0.357us          4212            --  \n",
      "                                         aten::_to_copy         0.77%       8.757ms         5.87%      66.622ms      23.751us       0.000us         0.00%       1.505ms       0.537us          2805            --  \n",
      "                                    aten::empty_strided         1.27%      14.399ms         1.27%      14.399ms       4.790us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.41%      16.011ms         4.81%      54.560ms      14.891us       2.421ms         1.05%       2.421ms       0.661us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.135s\n",
      "Self CUDA time total: 230.702ms\n",
      "\n",
      "output: tensor([[    2,  6179,    64,    47,  3094,   114,    10,  2391,    16,  1406,\n",
      "           566,  8803,    50,  4412, 21538,  6349,     6,     8,   596,   429,\n",
      "            42,   335,    28,  5616,   116, 50118, 50118,   133,  1049,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349,     4, 50118, 50118,   133,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349,     4, 50118, 50118,   133,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349,     4, 50118, 50118,   133,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349,     4, 50118, 50118,   133,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349,     4, 50118, 50118,   133,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349,     4, 50118, 50118,   133,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349,     4, 50118, 50118,   133,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349,     4, 50118, 50118,   133,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349,     4, 50118, 50118,   133,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349]], device='cuda:0')\n",
      "text_energy_per_token: [9.272540264322917]\n",
      "output_tokens: 225\n",
      "flop: 56297606505\n",
      "energy_consumed:  2086.321559472656\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.01%     179.650ms        21.88%     245.516ms      17.050us     122.269ms        52.96%     122.269ms       8.491us         14400     38050.726  \n",
      "                                               aten::mm         0.25%       2.831ms         0.36%       4.078ms      20.392us      44.937ms        19.47%      44.937ms     224.687us           200     17296.785  \n",
      "                                              aten::bmm         5.08%      57.057ms         6.90%      77.410ms      16.127us       8.355ms         3.62%       8.355ms       1.741us          4800       940.032  \n",
      "                                              aten::add         5.63%      63.173ms         8.57%      96.130ms      12.013us       9.779ms         4.24%       9.779ms       1.222us          8002         7.973  \n",
      "                                              aten::mul         2.30%      25.771ms         3.41%      38.289ms      12.750us       3.250ms         1.41%       3.250ms       1.082us          3003         2.090  \n",
      "                                            aten::empty         5.14%      57.632ms         5.14%      57.632ms       3.056us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.944ms         6.10%      68.449ms      16.251us       0.000us         0.00%       1.506ms       0.358us          4212            --  \n",
      "                                         aten::_to_copy         0.76%       8.574ms         5.75%      64.505ms      22.996us       0.000us         0.00%       1.506ms       0.537us          2805            --  \n",
      "                                    aten::empty_strided         1.38%      15.469ms         1.38%      15.469ms       5.146us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.43%      16.037ms         4.70%      52.752ms      14.397us       2.423ms         1.05%       2.423ms       0.661us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.122s\n",
      "Self CUDA time total: 230.856ms\n",
      "\n",
      "output: tensor([[    2,  6179,    64,    47,  3094,   114,    10,  2391,    16,  1406,\n",
      "           566,  8803,    50,  4412, 21538,  6349,     6,     8,   596,   429,\n",
      "            42,   335,    28,  5616,   116, 50118, 50118,   133,  1049,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349,     4, 50118, 50118,   133,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349,     4, 50118, 50118,   133,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349,     4, 50118, 50118,   133,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349,     4, 50118, 50118,   133,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349,     4, 50118, 50118,   133,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349,     4, 50118, 50118,   133,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349,     4, 50118, 50118,   133,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349,     4, 50118, 50118,   133,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349,     4, 50118, 50118,   133,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349]], device='cuda:0')\n",
      "text_energy_per_token: [9.272540264322917, 9.288968528618284]\n",
      "output_tokens: 225\n",
      "flop: 56297606505\n",
      "energy_consumed:  2090.0179189391138\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total KFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        14.78%      17.197ms        20.60%      23.959ms      18.487us      11.389ms        53.80%      11.389ms       8.788us          1296   7304380.416  \n",
      "                                               aten::mm         0.23%     270.733us         0.34%     391.044us      21.725us       4.000ms        18.90%       4.000ms     222.226us            18   3320365.056  \n",
      "                                              aten::bmm         4.85%       5.637ms         6.55%       7.621ms      17.642us     815.013us         3.85%     815.013us       1.887us           432     46854.144  \n",
      "                                              aten::add         5.29%       6.148ms         7.99%       9.299ms      12.880us     879.429us         4.15%     879.429us       1.218us           722      1008.755  \n",
      "                                              aten::mul         2.21%       2.567ms         3.26%       3.795ms      13.900us     297.089us         1.40%     297.089us       1.088us           273       396.998  \n",
      "                                            aten::empty         5.88%       6.841ms         5.88%       6.841ms       3.909us       0.000us         0.00%       0.000us       0.000us          1750            --  \n",
      "                                               aten::to         0.31%     357.321us         9.00%      10.473ms      26.853us       0.000us         0.00%     138.368us       0.355us           390            --  \n",
      "                                         aten::_to_copy         0.78%     901.735us         8.70%      10.115ms      39.359us       0.000us         0.00%     138.368us       0.538us           257            --  \n",
      "                                    aten::empty_strided         2.93%       3.405ms         2.93%       3.405ms      12.336us       0.000us         0.00%       0.000us       0.000us           276            --  \n",
      "                                            aten::copy_         1.76%       2.048ms         6.66%       7.751ms      19.976us     298.594us         1.41%     298.594us       0.770us           388            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 116.321ms\n",
      "Self CUDA time total: 21.170ms\n",
      "\n",
      "output: tensor([[    2,  2264,    32,   103, 12405, 14885,    14,  3608,   951,    16,\n",
      "         23748,     7,  1346,    10,  5674,    50,  1607,    77,    51,    32,\n",
      "           888, 10985,    50, 21969, 10312,   116, 50118,   100,   206,    24,\n",
      "            18,   142,    51,   214,    45,   269,   686,    99,    51,   214,\n",
      "          1686,    59,     4,     2]], device='cuda:0')\n",
      "text_energy_per_token: [2.5865397154873064]\n",
      "output_tokens: 44\n",
      "flop: 10673005369\n",
      "energy_consumed:  113.80774748144148\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total KFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.48%      17.564ms        21.45%      24.330ms      18.773us      11.392ms        53.81%      11.392ms       8.790us          1296   7304380.416  \n",
      "                                               aten::mm         0.25%     279.361us         0.35%     400.391us      22.244us       3.992ms        18.86%       3.992ms     221.794us            18   3320365.056  \n",
      "                                              aten::bmm         5.08%       5.758ms         6.82%       7.736ms      17.907us     815.079us         3.85%     815.079us       1.887us           432     46854.144  \n",
      "                                              aten::add         5.55%       6.293ms         8.40%       9.526ms      13.193us     879.117us         4.15%     879.117us       1.218us           722      1008.755  \n",
      "                                              aten::mul         2.30%       2.610ms         3.40%       3.858ms      14.132us     297.283us         1.40%     297.283us       1.089us           273       396.998  \n",
      "                                            aten::empty         6.08%       6.892ms         6.08%       6.892ms       3.938us       0.000us         0.00%       0.000us       0.000us          1750            --  \n",
      "                                               aten::to         0.33%     369.171us         6.43%       7.299ms      18.715us       0.000us         0.00%     139.746us       0.358us           390            --  \n",
      "                                         aten::_to_copy         0.75%     847.145us         6.11%       6.930ms      26.963us       0.000us         0.00%     139.746us       0.544us           257            --  \n",
      "                                    aten::empty_strided         1.37%       1.559ms         1.37%       1.559ms       5.647us       0.000us         0.00%       0.000us       0.000us           276            --  \n",
      "                                            aten::copy_         1.86%       2.111ms         5.72%       6.490ms      16.727us     301.315us         1.42%     301.315us       0.777us           388            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 113.436ms\n",
      "Self CUDA time total: 21.171ms\n",
      "\n",
      "output: tensor([[    2,  2264,    32,   103, 12405, 14885,    14,  3608,   951,    16,\n",
      "         23748,     7,  1346,    10,  5674,    50,  1607,    77,    51,    32,\n",
      "           888, 10985,    50, 21969, 10312,   116, 50118,   100,   206,    24,\n",
      "            18,   142,    51,   214,    45,   269,   686,    99,    51,   214,\n",
      "          1686,    59,     4,     2]], device='cuda:0')\n",
      "text_energy_per_token: [2.5865397154873064, 13.580881542876636]\n",
      "output_tokens: 44\n",
      "flop: 10673005369\n",
      "energy_consumed:  597.558787886572\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.11%     175.888ms        22.03%     240.596ms      16.708us     122.119ms        52.96%     122.119ms       8.480us         14400     38050.726  \n",
      "                                               aten::mm         0.26%       2.795ms         0.37%       4.033ms      20.166us      44.918ms        19.48%      44.918ms     224.588us           200     17296.785  \n",
      "                                              aten::bmm         5.16%      56.401ms         7.00%      76.441ms      15.925us       8.364ms         3.63%       8.364ms       1.743us          4800       940.032  \n",
      "                                              aten::add         5.68%      61.988ms         8.68%      94.810ms      11.848us       9.761ms         4.23%       9.761ms       1.220us          8002         7.973  \n",
      "                                              aten::mul         2.33%      25.423ms         3.48%      37.967ms      12.643us       3.247ms         1.41%       3.247ms       1.081us          3003         2.090  \n",
      "                                            aten::empty         5.10%      55.744ms         5.10%      55.744ms       2.956us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.768ms         6.25%      68.211ms      16.195us       0.000us         0.00%       1.497ms       0.356us          4212            --  \n",
      "                                         aten::_to_copy         0.74%       8.128ms         5.90%      64.444ms      22.975us       0.000us         0.00%       1.497ms       0.534us          2805            --  \n",
      "                                    aten::empty_strided         1.23%      13.481ms         1.23%      13.481ms       4.485us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.46%      15.915ms         5.04%      55.085ms      15.034us       2.412ms         1.05%       2.412ms       0.658us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.092s\n",
      "Self CUDA time total: 230.595ms\n",
      "\n",
      "output: tensor([[    2,  7608,   429,   951,  2807,     7,   304,    10,  2225,  5456,\n",
      "            50,  1394,    13,  9969,  1386,     9, 13304,    15,    10, 11921,\n",
      "          2187,    50,  4368,  1553,   116, 50118, 10105,    24,    18,  3013,\n",
      "             7,   304,    10,  4368,  1553,     4, 50118,   100,   437,    45,\n",
      "           584,    24,    18,  3013,     6,    38,   437,   584,    24,    18,\n",
      "          3013,     7,   304,    10,  2225,  5456,     4, 50118,   100,   437,\n",
      "           584,    24,    18,  3013,     7,   304,    10,  2225,  5456,     4,\n",
      "         50118,   100,   437,   584,    24,    18,  3013,     7,   304,    10,\n",
      "          2225,  5456,     4, 50118,   100,   437,   584,    24,    18,  3013,\n",
      "             7,   304,    10,  2225,  5456,     4, 50118,   100,   437,   584,\n",
      "            24,    18,  3013,     7,   304,    10,  2225,  5456,     4, 50118,\n",
      "           100,   437,   584,    24,    18,  3013,     7,   304,    10,  2225,\n",
      "          5456,     4, 50118,   100,   437,   584,    24,    18,  3013,     7,\n",
      "           304,    10,  2225,  5456,     4, 50118,   100,   437,   584,    24,\n",
      "            18,  3013,     7,   304,    10,  2225,  5456,     4, 50118,   100,\n",
      "           437,   584,    24,    18,  3013,     7,   304,    10,  2225,  5456,\n",
      "             4, 50118,   100,   437,   584,    24,    18,  3013,     7,   304,\n",
      "            10,  2225,  5456,     4, 50118,   100,   437,   584,    24,    18,\n",
      "          3013,     7,   304,    10,  2225,  5456,     4, 50118,   100,   437,\n",
      "           584,    24,    18,  3013,     7,   304,    10,  2225,  5456,     4,\n",
      "         50118,   100,   437,   584,    24,    18,  3013,     7,   304,    10,\n",
      "          2225,  5456,     4, 50118,   100,   437,   584,    24,    18,  3013,\n",
      "             7,   304,    10,  2225,  5456]], device='cuda:0')\n",
      "text_energy_per_token: [7.581789022197724]\n",
      "output_tokens: 225\n",
      "flop: 56297606505\n",
      "energy_consumed:  1705.9025299944878\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.76%     174.208ms        21.57%     238.356ms      16.553us     122.215ms        52.99%     122.215ms       8.487us         14400     38050.726  \n",
      "                                               aten::mm         0.25%       2.803ms         0.37%       4.057ms      20.284us      44.903ms        19.47%      44.903ms     224.516us           200     17296.785  \n",
      "                                              aten::bmm         5.18%      57.221ms         7.01%      77.513ms      16.149us       8.335ms         3.61%       8.335ms       1.737us          4800       940.032  \n",
      "                                              aten::add         5.68%      62.740ms         8.66%      95.710ms      11.961us       9.764ms         4.23%       9.764ms       1.220us          8002         7.973  \n",
      "                                              aten::mul         2.32%      25.637ms         3.46%      38.235ms      12.732us       3.244ms         1.41%       3.244ms       1.080us          3003         2.090  \n",
      "                                            aten::empty         4.79%      52.915ms         4.79%      52.915ms       2.806us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.901ms         6.34%      70.036ms      16.628us       0.000us         0.00%       1.493ms       0.355us          4212            --  \n",
      "                                         aten::_to_copy         0.77%       8.508ms         5.98%      66.135ms      23.578us       0.000us         0.00%       1.493ms       0.532us          2805            --  \n",
      "                                    aten::empty_strided         1.29%      14.244ms         1.29%      14.244ms       4.739us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.44%      15.969ms         4.93%      54.517ms      14.879us       2.409ms         1.04%       2.409ms       0.657us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.105s\n",
      "Self CUDA time total: 230.657ms\n",
      "\n",
      "output: tensor([[    2,  7608,   429,   951,  2807,     7,   304,    10,  2225,  5456,\n",
      "            50,  1394,    13,  9969,  1386,     9, 13304,    15,    10, 11921,\n",
      "          2187,    50,  4368,  1553,   116, 50118, 10105,    24,    18,  3013,\n",
      "             7,   304,    10,  4368,  1553,     4, 50118,   100,   437,    45,\n",
      "           584,    24,    18,  3013,     6,    38,   437,   584,    24,    18,\n",
      "          3013,     7,   304,    10,  2225,  5456,     4, 50118,   100,   437,\n",
      "           584,    24,    18,  3013,     7,   304,    10,  2225,  5456,     4,\n",
      "         50118,   100,   437,   584,    24,    18,  3013,     7,   304,    10,\n",
      "          2225,  5456,     4, 50118,   100,   437,   584,    24,    18,  3013,\n",
      "             7,   304,    10,  2225,  5456,     4, 50118,   100,   437,   584,\n",
      "            24,    18,  3013,     7,   304,    10,  2225,  5456,     4, 50118,\n",
      "           100,   437,   584,    24,    18,  3013,     7,   304,    10,  2225,\n",
      "          5456,     4, 50118,   100,   437,   584,    24,    18,  3013,     7,\n",
      "           304,    10,  2225,  5456,     4, 50118,   100,   437,   584,    24,\n",
      "            18,  3013,     7,   304,    10,  2225,  5456,     4, 50118,   100,\n",
      "           437,   584,    24,    18,  3013,     7,   304,    10,  2225,  5456,\n",
      "             4, 50118,   100,   437,   584,    24,    18,  3013,     7,   304,\n",
      "            10,  2225,  5456,     4, 50118,   100,   437,   584,    24,    18,\n",
      "          3013,     7,   304,    10,  2225,  5456,     4, 50118,   100,   437,\n",
      "           584,    24,    18,  3013,     7,   304,    10,  2225,  5456,     4,\n",
      "         50118,   100,   437,   584,    24,    18,  3013,     7,   304,    10,\n",
      "          2225,  5456,     4, 50118,   100,   437,   584,    24,    18,  3013,\n",
      "             7,   304,    10,  2225,  5456]], device='cuda:0')\n",
      "text_energy_per_token: [7.581789022197724, 9.34931968732622]\n",
      "output_tokens: 225\n",
      "flop: 56297606505\n",
      "energy_consumed:  2103.5969296483995\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.93%     173.994ms        21.84%     238.497ms      16.562us     122.097ms        52.97%     122.097ms       8.479us         14400     37031.510  \n",
      "                                               aten::mm         0.26%       2.789ms         0.37%       4.035ms      20.176us      44.925ms        19.49%      44.925ms     224.627us           200     16833.479  \n",
      "                                              aten::bmm         5.16%      56.334ms         6.99%      76.343ms      15.905us       8.298ms         3.60%       8.298ms       1.729us          4800       886.284  \n",
      "                                              aten::add         5.67%      61.895ms         8.67%      94.707ms      11.835us       9.776ms         4.24%       9.776ms       1.222us          8002         7.648  \n",
      "                                              aten::mul         2.32%      25.389ms         3.46%      37.809ms      12.590us       3.247ms         1.41%       3.247ms       1.081us          3003         2.033  \n",
      "                                            aten::empty         5.10%      55.723ms         5.10%      55.723ms       2.955us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.34%       3.754ms         6.18%      67.527ms      16.032us       0.000us         0.00%       1.499ms       0.356us          4212            --  \n",
      "                                         aten::_to_copy         0.75%       8.215ms         5.84%      63.773ms      22.736us       0.000us         0.00%       1.499ms       0.535us          2805            --  \n",
      "                                    aten::empty_strided         1.40%      15.344ms         1.40%      15.344ms       5.104us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.46%      15.919ms         4.81%      52.495ms      14.327us       2.415ms         1.05%       2.415ms       0.659us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.092s\n",
      "Self CUDA time total: 230.482ms\n",
      "\n",
      "output: tensor([[    2,  6179,    64,    47,  3094,   114,    10,   621,    16, 14528,\n",
      "          2509,    11,    10,  1607,    50,  1622,   145, 24908,   116, 50118,\n",
      "           100,   206,    24,    18,    55,     9,    10,    22,   100,   437,\n",
      "            45,  2509,    11,    10,  1607,   113,   631,     4,  1437,    38,\n",
      "           437,    45,   686,   114,    24,    18,    95,   162,    50,   114,\n",
      "            24,    18,    95,     5,   169,    38,   437,   341,     7,    24,\n",
      "             4,  1437,    38,   437,    45,   686,   114,    24,    18,    95,\n",
      "           162,    50,   114,    24,    18,    95,     5,   169,    38,   437,\n",
      "           341,     7,    24,     4,  1437,    38,   437,    45,   686,   114,\n",
      "            24,    18,    95,   162,    50,   114,    24,    18,    95,     5,\n",
      "           169,    38,   437,   341,     7,    24,     4,  1437,    38,   437,\n",
      "            45,   686,   114,    24,    18,    95,   162,    50,   114,    24,\n",
      "            18,    95,     5,   169,    38,   437,   341,     7,    24,     4,\n",
      "          1437,    38,   437,    45,   686,   114,    24,    18,    95,   162,\n",
      "            50,   114,    24,    18,    95,     5,   169,    38,   437,   341,\n",
      "             7,    24,     4,  1437,    38,   437,    45,   686,   114,    24,\n",
      "            18,    95,   162,    50,   114,    24,    18,    95,     5,   169,\n",
      "            38,   437,   341,     7,    24,     4,  1437,    38,   437,    45,\n",
      "           686,   114,    24,    18,    95,   162,    50,   114,    24,    18,\n",
      "            95,     5,   169,    38,   437,   341,     7,    24,     4, 50118,\n",
      "           100,   206,    24,    18,    55,     9,    10,    22,   100,   437,\n",
      "            45,  2509,    11,    10,  1607,   113,   631,     4,  1437]],\n",
      "       device='cuda:0')\n",
      "text_energy_per_token: [8.854990637261574]\n",
      "output_tokens: 219\n",
      "flop: 54760954491\n",
      "energy_consumed:  1939.2429495602846\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.11%     181.188ms        21.87%     245.923ms      17.078us     122.139ms        52.99%     122.139ms       8.482us         14400     37031.510  \n",
      "                                               aten::mm         0.25%       2.820ms         0.36%       4.058ms      20.289us      44.903ms        19.48%      44.903ms     224.513us           200     16833.479  \n",
      "                                              aten::bmm         5.09%      57.272ms         6.91%      77.676ms      16.183us       8.300ms         3.60%       8.300ms       1.729us          4800       886.284  \n",
      "                                              aten::add         5.61%      63.038ms         8.54%      96.021ms      12.000us       9.769ms         4.24%       9.769ms       1.221us          8002         7.648  \n",
      "                                              aten::mul         2.30%      25.849ms         3.59%      40.407ms      13.456us       3.246ms         1.41%       3.246ms       1.081us          3003         2.033  \n",
      "                                            aten::empty         4.95%      55.672ms         4.95%      55.672ms       2.952us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.912ms         6.11%      68.682ms      16.306us       0.000us         0.00%       1.488ms       0.353us          4212            --  \n",
      "                                         aten::_to_copy         0.77%       8.708ms         5.76%      64.769ms      23.091us       0.000us         0.00%       1.488ms       0.531us          2805            --  \n",
      "                                    aten::empty_strided         1.25%      14.067ms         1.25%      14.067ms       4.680us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.43%      16.027ms         4.84%      54.440ms      14.858us       2.403ms         1.04%       2.403ms       0.656us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.124s\n",
      "Self CUDA time total: 230.487ms\n",
      "\n",
      "output: tensor([[    2,  6179,    64,    47,  3094,   114,    10,   621,    16, 14528,\n",
      "          2509,    11,    10,  1607,    50,  1622,   145, 24908,   116, 50118,\n",
      "           100,   206,    24,    18,    55,     9,    10,    22,   100,   437,\n",
      "            45,  2509,    11,    10,  1607,   113,   631,     4,  1437,    38,\n",
      "           437,    45,   686,   114,    24,    18,    95,   162,    50,   114,\n",
      "            24,    18,    95,     5,   169,    38,   437,   341,     7,    24,\n",
      "             4,  1437,    38,   437,    45,   686,   114,    24,    18,    95,\n",
      "           162,    50,   114,    24,    18,    95,     5,   169,    38,   437,\n",
      "           341,     7,    24,     4,  1437,    38,   437,    45,   686,   114,\n",
      "            24,    18,    95,   162,    50,   114,    24,    18,    95,     5,\n",
      "           169,    38,   437,   341,     7,    24,     4,  1437,    38,   437,\n",
      "            45,   686,   114,    24,    18,    95,   162,    50,   114,    24,\n",
      "            18,    95,     5,   169,    38,   437,   341,     7,    24,     4,\n",
      "          1437,    38,   437,    45,   686,   114,    24,    18,    95,   162,\n",
      "            50,   114,    24,    18,    95,     5,   169,    38,   437,   341,\n",
      "             7,    24,     4,  1437,    38,   437,    45,   686,   114,    24,\n",
      "            18,    95,   162,    50,   114,    24,    18,    95,     5,   169,\n",
      "            38,   437,   341,     7,    24,     4,  1437,    38,   437,    45,\n",
      "           686,   114,    24,    18,    95,   162,    50,   114,    24,    18,\n",
      "            95,     5,   169,    38,   437,   341,     7,    24,     4, 50118,\n",
      "           100,   206,    24,    18,    55,     9,    10,    22,   100,   437,\n",
      "            45,  2509,    11,    10,  1607,   113,   631,     4,  1437]],\n",
      "       device='cuda:0')\n",
      "text_energy_per_token: [8.854990637261574, 9.675778473297212]\n",
      "output_tokens: 219\n",
      "flop: 54760954491\n",
      "energy_consumed:  2118.9954856520894\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.63%     175.896ms        21.56%     242.667ms      16.852us     122.232ms        52.95%     122.232ms       8.488us         14400     38730.203  \n",
      "                                               aten::mm         0.37%       4.179ms         0.48%       5.431ms      27.155us      44.874ms        19.44%      44.874ms     224.368us           200     17605.657  \n",
      "                                              aten::bmm         5.35%      60.244ms         7.18%      80.813ms      16.836us       8.370ms         3.63%       8.370ms       1.744us          4800       977.338  \n",
      "                                              aten::add         5.85%      65.849ms         8.78%      98.824ms      12.350us       9.792ms         4.24%       9.792ms       1.224us          8002         8.196  \n",
      "                                              aten::mul         2.28%      25.704ms         3.40%      38.250ms      12.737us       3.245ms         1.41%       3.245ms       1.080us          3003         2.127  \n",
      "                                            aten::empty         4.78%      53.742ms         4.78%      53.742ms       2.850us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.974ms         6.21%      69.830ms      16.579us       0.000us         0.00%       1.497ms       0.355us          4212            --  \n",
      "                                         aten::_to_copy         0.75%       8.488ms         5.85%      65.856ms      23.478us       0.000us         0.00%       1.497ms       0.534us          2805            --  \n",
      "                                    aten::empty_strided         1.26%      14.216ms         1.26%      14.216ms       4.729us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.42%      15.997ms         4.82%      54.266ms      14.811us       2.414ms         1.05%       2.414ms       0.659us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.125s\n",
      "Self CUDA time total: 230.834ms\n",
      "\n",
      "output: tensor([[    2,  7608,   429,   951,  6573,     7,  2792,    23,    10,   650,\n",
      "             6,  8094,    12,  4447,   265,  1386,     9,    10,   739,  3206,\n",
      "          1400,     6,   190,   114,     5,   850,    32,   723,   116, 50118,\n",
      "         10105,    51,   214,    45,   164,     7,    28,   441,     7,  4960,\n",
      "             7,   907,     5,   276,  2682,    23,    10,   400,  1400,     4,\n",
      "         50118,   100,   437,    45,   584,    14,    51,   214,    45,   164,\n",
      "             7,    28,   441,     7,  4960,     7,   907,     5,   276,  2682,\n",
      "            23,    10,   400,  1400,     6,    53,    51,   214,    45,   164,\n",
      "             7,    28,   441,     7,  4960,     7,   907,     5,   276,  2682,\n",
      "            23,    10,   400,  1400,     4,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437]],\n",
      "       device='cuda:0')\n",
      "text_energy_per_token: [9.136712157982407]\n",
      "output_tokens: 229\n",
      "flop: 57323521501\n",
      "energy_consumed:  2092.3070841779713\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.21%     179.829ms        22.07%     244.868ms      17.005us     122.420ms        53.01%     122.420ms       8.501us         14400     38730.203  \n",
      "                                               aten::mm         0.25%       2.816ms         0.36%       4.047ms      20.233us      44.851ms        19.42%      44.851ms     224.253us           200     17605.657  \n",
      "                                              aten::bmm         5.16%      57.260ms         6.99%      77.493ms      16.144us       8.366ms         3.62%       8.366ms       1.743us          4800       977.338  \n",
      "                                              aten::add         5.65%      62.674ms         8.61%      95.551ms      11.941us       9.775ms         4.23%       9.775ms       1.222us          8002         8.196  \n",
      "                                              aten::mul         2.31%      25.628ms         3.44%      38.167ms      12.709us       3.246ms         1.41%       3.246ms       1.081us          3003         2.127  \n",
      "                                            aten::empty         5.06%      56.144ms         5.06%      56.144ms       2.977us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.840ms         6.12%      67.845ms      16.107us       0.000us         0.00%       1.495ms       0.355us          4212            --  \n",
      "                                         aten::_to_copy         0.75%       8.342ms         5.77%      64.005ms      22.818us       0.000us         0.00%       1.495ms       0.533us          2805            --  \n",
      "                                    aten::empty_strided         1.39%      15.442ms         1.39%      15.442ms       5.137us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.44%      15.975ms         4.74%      52.556ms      14.344us       2.412ms         1.04%       2.412ms       0.658us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.109s\n",
      "Self CUDA time total: 230.935ms\n",
      "\n",
      "output: tensor([[    2,  7608,   429,   951,  6573,     7,  2792,    23,    10,   650,\n",
      "             6,  8094,    12,  4447,   265,  1386,     9,    10,   739,  3206,\n",
      "          1400,     6,   190,   114,     5,   850,    32,   723,   116, 50118,\n",
      "         10105,    51,   214,    45,   164,     7,    28,   441,     7,  4960,\n",
      "             7,   907,     5,   276,  2682,    23,    10,   400,  1400,     4,\n",
      "         50118,   100,   437,    45,   584,    14,    51,   214,    45,   164,\n",
      "             7,    28,   441,     7,  4960,     7,   907,     5,   276,  2682,\n",
      "            23,    10,   400,  1400,     6,    53,    51,   214,    45,   164,\n",
      "             7,    28,   441,     7,  4960,     7,   907,     5,   276,  2682,\n",
      "            23,    10,   400,  1400,     4,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437]],\n",
      "       device='cuda:0')\n",
      "text_energy_per_token: [9.136712157982407, 9.163871481016736]\n",
      "output_tokens: 229\n",
      "flop: 57323521501\n",
      "energy_consumed:  2098.5265691528325\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.28%     175.796ms        21.12%     243.013ms      16.876us     122.578ms        53.01%     122.578ms       8.512us         14400     39579.550  \n",
      "                                               aten::mm         0.37%       4.227ms         0.47%       5.436ms      27.181us      44.923ms        19.43%      44.923ms     224.616us           200     17991.746  \n",
      "                                              aten::bmm         5.13%      59.045ms         6.93%      79.715ms      16.607us       8.429ms         3.64%       8.429ms       1.756us          4800      1025.630  \n",
      "                                              aten::add         5.76%      66.313ms         8.65%      99.519ms      12.437us       9.768ms         4.22%       9.768ms       1.221us          8002         8.481  \n",
      "                                              aten::mul         2.37%      27.300ms         3.47%      39.861ms      13.274us       3.245ms         1.40%       3.245ms       1.081us          3003         2.174  \n",
      "                                            aten::empty         4.66%      53.653ms         4.66%      53.653ms       2.845us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       4.036ms         6.28%      72.246ms      17.152us       0.000us         0.00%       1.503ms       0.357us          4212            --  \n",
      "                                         aten::_to_copy         0.77%       8.895ms         5.93%      68.210ms      24.317us       0.000us         0.00%       1.503ms       0.536us          2805            --  \n",
      "                                    aten::empty_strided         1.38%      15.843ms         1.38%      15.843ms       5.271us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.39%      16.016ms         4.74%      54.549ms      14.888us       2.419ms         1.05%       2.419ms       0.660us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.150s\n",
      "Self CUDA time total: 231.256ms\n",
      "\n",
      "output: tensor([[    2,  6179,    64,    47,  7118,     5, 10796,     9,    10,  1300,\n",
      "             9,   335,     6,   215,    25,    10,   340,  1566,    50,  5059,\n",
      "           618,     6,   396, 13304,  9382,    15,     5,  5070,     9,     5,\n",
      "          2730,    50, 10710,   116, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4]], device='cuda:0')\n",
      "text_energy_per_token: [9.039409934942336]\n",
      "output_tokens: 234\n",
      "flop: 58607580606\n",
      "energy_consumed:  2115.2219247765065\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.58%     171.516ms        21.48%     236.499ms      16.424us     122.571ms        53.01%     122.571ms       8.512us         14400     39579.550  \n",
      "                                               aten::mm         0.25%       2.793ms         0.37%       4.118ms      20.590us      44.901ms        19.42%      44.901ms     224.507us           200     17991.746  \n",
      "                                              aten::bmm         5.19%      57.206ms         7.16%      78.883ms      16.434us       8.424ms         3.64%       8.424ms       1.755us          4800      1025.630  \n",
      "                                              aten::add         5.66%      62.308ms         8.65%      95.224ms      11.900us       9.771ms         4.23%       9.771ms       1.221us          8002         8.481  \n",
      "                                              aten::mul         2.44%      26.831ms         3.57%      39.289ms      13.083us       3.245ms         1.40%       3.245ms       1.081us          3003         2.174  \n",
      "                                            aten::empty         4.73%      52.132ms         4.73%      52.132ms       2.764us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.873ms         6.31%      69.518ms      16.505us       0.000us         0.00%       1.507ms       0.358us          4212            --  \n",
      "                                         aten::_to_copy         0.76%       8.343ms         5.96%      65.645ms      23.403us       0.000us         0.00%       1.507ms       0.537us          2805            --  \n",
      "                                    aten::empty_strided         1.28%      14.086ms         1.28%      14.086ms       4.686us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.45%      15.958ms         4.92%      54.222ms      14.799us       2.423ms         1.05%       2.423ms       0.661us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.101s\n",
      "Self CUDA time total: 231.232ms\n",
      "\n",
      "output: tensor([[    2,  6179,    64,    47,  7118,     5, 10796,     9,    10,  1300,\n",
      "             9,   335,     6,   215,    25,    10,   340,  1566,    50,  5059,\n",
      "           618,     6,   396, 13304,  9382,    15,     5,  5070,     9,     5,\n",
      "          2730,    50, 10710,   116, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4]], device='cuda:0')\n",
      "text_energy_per_token: [9.039409934942336, 8.378001347091462]\n",
      "output_tokens: 234\n",
      "flop: 58607580606\n",
      "energy_consumed:  1960.4523152194024\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.73%     174.076ms        21.48%     237.758ms      16.511us     122.341ms        52.97%     122.341ms       8.496us         14400     39069.942  \n",
      "                                               aten::mm         0.38%       4.192ms         0.49%       5.424ms      27.120us      44.914ms        19.45%      44.914ms     224.570us           200     17760.092  \n",
      "                                              aten::bmm         5.36%      59.354ms         7.19%      79.604ms      16.584us       8.392ms         3.63%       8.392ms       1.748us          4800       996.434  \n",
      "                                              aten::add         6.02%      66.659ms         8.99%      99.489ms      12.433us       9.767ms         4.23%       9.767ms       1.221us          8002         8.309  \n",
      "                                              aten::mul         2.31%      25.518ms         3.43%      38.015ms      12.659us       3.245ms         1.41%       3.245ms       1.081us          3003         2.146  \n",
      "                                            aten::empty         4.82%      53.362ms         4.82%      53.362ms       2.830us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.825ms         6.14%      68.001ms      16.145us       0.000us         0.00%       1.502ms       0.356us          4212            --  \n",
      "                                         aten::_to_copy         0.75%       8.301ms         5.80%      64.177ms      22.879us       0.000us         0.00%       1.502ms       0.535us          2805            --  \n",
      "                                    aten::empty_strided         1.30%      14.359ms         1.30%      14.359ms       4.777us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.44%      15.896ms         4.75%      52.539ms      14.339us       2.416ms         1.05%       2.416ms       0.660us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.107s\n",
      "Self CUDA time total: 230.952ms\n",
      "\n",
      "output: tensor([[    2,  7608,   109,   103,    82,  2254,     5, 15583,     9,   145,\n",
      "          8265,     6,   215,    25,    30,  2494,  8444,  4133,    50,   164,\n",
      "            15, 15950,  1029, 22125,     6,   150,   643,  1877,   209,  3734,\n",
      "           116, 50118,   100,   218,    75,   216,     6,    38,   348,   393,\n",
      "            57,  8265,     9,   932,     4,    38,   348,   393,    57,  8265,\n",
      "             9,   932,     4,    38,   348,   393,    57,  8265,     9,   932,\n",
      "             4,    38,   348,   393,    57,  8265,     9,   932,     4,    38,\n",
      "           348,   393,    57,  8265,     9,   932,     4,    38,   348,   393,\n",
      "            57,  8265,     9,   932,     4,    38,   348,   393,    57,  8265,\n",
      "             9,   932,     4,    38,   348,   393,    57,  8265,     9,   932,\n",
      "             4,    38,   348,   393,    57,  8265,     9,   932,     4,    38,\n",
      "           348,   393,    57,  8265,     9,   932,     4,    38,   348,   393,\n",
      "            57,  8265,     9,   932,     4,    38,   348,   393,    57,  8265,\n",
      "             9,   932,     4,    38,   348,   393,    57,  8265,     9,   932,\n",
      "             4,    38,   348,   393,    57,  8265,     9,   932,     4,    38,\n",
      "           348,   393,    57,  8265,     9,   932,     4,    38,   348,   393,\n",
      "            57,  8265,     9,   932,     4,    38,   348,   393,    57,  8265,\n",
      "             9,   932,     4,    38,   348,   393,    57,  8265,     9,   932,\n",
      "             4,    38,   348,   393,    57,  8265,     9,   932,     4,    38,\n",
      "           348,   393,    57,  8265,     9,   932,     4,    38,   348,   393,\n",
      "            57,  8265,     9,   932,     4,    38,   348,   393,    57,  8265,\n",
      "             9,   932,     4,    38,   348,   393,    57,  8265,     9,   932,\n",
      "             4,    38,   348,   393,    57,  8265,     9,   932,     4,    38,\n",
      "           348]], device='cuda:0')\n",
      "text_energy_per_token: [9.234469127914915]\n",
      "output_tokens: 231\n",
      "flop: 57836923095\n",
      "energy_consumed:  2133.1623685483455\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.07%     181.096ms        21.90%     246.868ms      17.144us     122.407ms        52.97%     122.407ms       8.501us         14400     39069.942  \n",
      "                                               aten::mm         0.25%       2.803ms         0.36%       4.046ms      20.228us      44.938ms        19.45%      44.938ms     224.691us           200     17760.092  \n",
      "                                              aten::bmm         5.07%      57.169ms         6.88%      77.499ms      16.146us       8.387ms         3.63%       8.387ms       1.747us          4800       996.434  \n",
      "                                              aten::add         5.59%      63.040ms         8.53%      96.154ms      12.016us       9.775ms         4.23%       9.775ms       1.222us          8002         8.309  \n",
      "                                              aten::mul         2.29%      25.775ms         3.41%      38.432ms      12.798us       3.248ms         1.41%       3.248ms       1.081us          3003         2.146  \n",
      "                                            aten::empty         5.12%      57.757ms         5.12%      57.757ms       3.063us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.994ms         6.22%      70.163ms      16.658us       0.000us         0.00%       1.502ms       0.357us          4212            --  \n",
      "                                         aten::_to_copy         0.77%       8.664ms         5.87%      66.169ms      23.590us       0.000us         0.00%       1.502ms       0.536us          2805            --  \n",
      "                                    aten::empty_strided         1.39%      15.634ms         1.39%      15.634ms       5.201us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.41%      15.893ms         4.81%      54.166ms      14.783us       2.419ms         1.05%       2.419ms       0.660us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.127s\n",
      "Self CUDA time total: 231.084ms\n",
      "\n",
      "output: tensor([[    2,  7608,   109,   103,    82,  2254,     5, 15583,     9,   145,\n",
      "          8265,     6,   215,    25,    30,  2494,  8444,  4133,    50,   164,\n",
      "            15, 15950,  1029, 22125,     6,   150,   643,  1877,   209,  3734,\n",
      "           116, 50118,   100,   218,    75,   216,     6,    38,   348,   393,\n",
      "            57,  8265,     9,   932,     4,    38,   348,   393,    57,  8265,\n",
      "             9,   932,     4,    38,   348,   393,    57,  8265,     9,   932,\n",
      "             4,    38,   348,   393,    57,  8265,     9,   932,     4,    38,\n",
      "           348,   393,    57,  8265,     9,   932,     4,    38,   348,   393,\n",
      "            57,  8265,     9,   932,     4,    38,   348,   393,    57,  8265,\n",
      "             9,   932,     4,    38,   348,   393,    57,  8265,     9,   932,\n",
      "             4,    38,   348,   393,    57,  8265,     9,   932,     4,    38,\n",
      "           348,   393,    57,  8265,     9,   932,     4,    38,   348,   393,\n",
      "            57,  8265,     9,   932,     4,    38,   348,   393,    57,  8265,\n",
      "             9,   932,     4,    38,   348,   393,    57,  8265,     9,   932,\n",
      "             4,    38,   348,   393,    57,  8265,     9,   932,     4,    38,\n",
      "           348,   393,    57,  8265,     9,   932,     4,    38,   348,   393,\n",
      "            57,  8265,     9,   932,     4,    38,   348,   393,    57,  8265,\n",
      "             9,   932,     4,    38,   348,   393,    57,  8265,     9,   932,\n",
      "             4,    38,   348,   393,    57,  8265,     9,   932,     4,    38,\n",
      "           348,   393,    57,  8265,     9,   932,     4,    38,   348,   393,\n",
      "            57,  8265,     9,   932,     4,    38,   348,   393,    57,  8265,\n",
      "             9,   932,     4,    38,   348,   393,    57,  8265,     9,   932,\n",
      "             4,    38,   348,   393,    57,  8265,     9,   932,     4,    38,\n",
      "           348]], device='cuda:0')\n",
      "text_energy_per_token: [9.234469127914915, 9.05496914145544]\n",
      "output_tokens: 231\n",
      "flop: 57836923095\n",
      "energy_consumed:  2091.6978716762064\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.32%     177.536ms        22.23%     241.727ms      16.787us     122.230ms        53.00%     122.230ms       8.488us         14400     37371.249  \n",
      "                                               aten::mm         0.26%       2.791ms         0.37%       4.020ms      20.098us      44.920ms        19.48%      44.920ms     224.601us           200     16987.914  \n",
      "                                              aten::bmm         5.16%      56.095ms         6.98%      75.956ms      15.824us       8.321ms         3.61%       8.321ms       1.734us          4800       903.905  \n",
      "                                              aten::add         5.67%      61.642ms         8.67%      94.319ms      11.787us       9.761ms         4.23%       9.761ms       1.220us          8002         7.756  \n",
      "                                              aten::mul         2.33%      25.387ms         3.49%      37.914ms      12.625us       3.247ms         1.41%       3.247ms       1.081us          3003         2.052  \n",
      "                                            aten::empty         5.11%      55.532ms         5.11%      55.532ms       2.945us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.34%       3.740ms         6.33%      68.820ms      16.339us       0.000us         0.00%       1.498ms       0.356us          4212            --  \n",
      "                                         aten::_to_copy         0.74%       8.010ms         5.98%      65.080ms      23.202us       0.000us         0.00%       1.498ms       0.534us          2805            --  \n",
      "                                    aten::empty_strided         1.41%      15.349ms         1.41%      15.349ms       5.106us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.45%      15.774ms         4.96%      53.911ms      14.714us       2.413ms         1.05%       2.413ms       0.658us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.088s\n",
      "Self CUDA time total: 230.638ms\n",
      "\n",
      "output: tensor([[    2,  6179,    64, 21981,     5,  3650,     9,    97,    82,    11,\n",
      "            10,   592,  1068,   694, 14885,    59,  4106, 14513,     8,  2113,\n",
      "           116, 50118, 50118,  1121,    42,  2225,     6,    52,  4830,     5,\n",
      "           774,     9,  4106, 14513,     8,  2113,    11,     5,  3650,     9,\n",
      "            97,    82,    11,    10,   592,  1068,     4,   166,   465,    14,\n",
      "          4106, 14513,     8,  2113,    32,    45,   129,   505,    13,     5,\n",
      "          3650,     9,    97,    82,     6,    53,    67,    13,     5,  3650,\n",
      "             9,    97,    82,    11,    10,   592,  1068,     4,   166,    67,\n",
      "           465,    14,  4106, 14513,     8,  2113,    32,    45,   129,   505,\n",
      "            13,     5,  3650,     9,    97,    82,     6,    53,    67,    13,\n",
      "             5,  3650,     9,    97,    82,    11,    10,   592,  1068,     4,\n",
      "         50118, 50118,   170,   465,    14,  4106, 14513,     8,  2113,    32,\n",
      "            45,   129,   505,    13,     5,  3650,     9,    97,    82,     6,\n",
      "            53,    67,    13,     5,  3650,     9,    97,    82,    11,    10,\n",
      "           592,  1068,     4,   166,    67,   465,    14,  4106, 14513,     8,\n",
      "          2113,    32,    45,   129,   505,    13,     5,  3650,     9,    97,\n",
      "            82,     6,    53,    67,    13,     5,  3650,     9,    97,    82,\n",
      "            11,    10,   592,  1068,     4, 50118, 50118,   170,   465,    14,\n",
      "          4106, 14513,     8,  2113,    32,    45,   129,   505,    13,     5,\n",
      "          3650,     9,    97,    82,     6,    53,    67,    13,     5,  3650,\n",
      "             9,    97,    82,    11,    10,   592,  1068,     4,   166,    67,\n",
      "           465,    14,  4106, 14513,     8,  2113,    32,    45,   129,   505,\n",
      "            13]], device='cuda:0')\n",
      "text_energy_per_token: [9.193226051432191]\n",
      "output_tokens: 221\n",
      "flop: 55272875765\n",
      "energy_consumed:  2031.7029573665143\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.14%     180.980ms        21.96%     246.227ms      17.099us     122.301ms        53.01%     122.301ms       8.493us         14400     37371.249  \n",
      "                                               aten::mm         0.25%       2.815ms         0.36%       4.080ms      20.401us      44.946ms        19.48%      44.946ms     224.729us           200     16987.914  \n",
      "                                              aten::bmm         5.09%      57.054ms         7.11%      79.759ms      16.617us       8.303ms         3.60%       8.303ms       1.730us          4800       903.905  \n",
      "                                              aten::add         5.62%      63.014ms         8.54%      95.788ms      11.971us       9.761ms         4.23%       9.761ms       1.220us          8002         7.756  \n",
      "                                              aten::mul         2.29%      25.707ms         3.40%      38.154ms      12.705us       3.250ms         1.41%       3.250ms       1.082us          3003         2.052  \n",
      "                                            aten::empty         5.04%      56.541ms         5.04%      56.541ms       2.998us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.968ms         6.11%      68.521ms      16.268us       0.000us         0.00%       1.498ms       0.356us          4212            --  \n",
      "                                         aten::_to_copy         0.77%       8.589ms         5.76%      64.554ms      23.014us       0.000us         0.00%       1.498ms       0.534us          2805            --  \n",
      "                                    aten::empty_strided         1.39%      15.563ms         1.39%      15.563ms       5.177us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.42%      15.973ms         4.69%      52.630ms      14.364us       2.415ms         1.05%       2.415ms       0.659us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.121s\n",
      "Self CUDA time total: 230.725ms\n",
      "\n",
      "output: tensor([[    2,  6179,    64, 21981,     5,  3650,     9,    97,    82,    11,\n",
      "            10,   592,  1068,   694, 14885,    59,  4106, 14513,     8,  2113,\n",
      "           116, 50118, 50118,  1121,    42,  2225,     6,    52,  4830,     5,\n",
      "           774,     9,  4106, 14513,     8,  2113,    11,     5,  3650,     9,\n",
      "            97,    82,    11,    10,   592,  1068,     4,   166,   465,    14,\n",
      "          4106, 14513,     8,  2113,    32,    45,   129,   505,    13,     5,\n",
      "          3650,     9,    97,    82,     6,    53,    67,    13,     5,  3650,\n",
      "             9,    97,    82,    11,    10,   592,  1068,     4,   166,    67,\n",
      "           465,    14,  4106, 14513,     8,  2113,    32,    45,   129,   505,\n",
      "            13,     5,  3650,     9,    97,    82,     6,    53,    67,    13,\n",
      "             5,  3650,     9,    97,    82,    11,    10,   592,  1068,     4,\n",
      "         50118, 50118,   170,   465,    14,  4106, 14513,     8,  2113,    32,\n",
      "            45,   129,   505,    13,     5,  3650,     9,    97,    82,     6,\n",
      "            53,    67,    13,     5,  3650,     9,    97,    82,    11,    10,\n",
      "           592,  1068,     4,   166,    67,   465,    14,  4106, 14513,     8,\n",
      "          2113,    32,    45,   129,   505,    13,     5,  3650,     9,    97,\n",
      "            82,     6,    53,    67,    13,     5,  3650,     9,    97,    82,\n",
      "            11,    10,   592,  1068,     4, 50118, 50118,   170,   465,    14,\n",
      "          4106, 14513,     8,  2113,    32,    45,   129,   505,    13,     5,\n",
      "          3650,     9,    97,    82,     6,    53,    67,    13,     5,  3650,\n",
      "             9,    97,    82,    11,    10,   592,  1068,     4,   166,    67,\n",
      "           465,    14,  4106, 14513,     8,  2113,    32,    45,   129,   505,\n",
      "            13]], device='cuda:0')\n",
      "text_energy_per_token: [9.193226051432191, 9.304171810422009]\n",
      "output_tokens: 221\n",
      "flop: 55272875765\n",
      "energy_consumed:  2056.221970103264\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.57%     178.256ms        21.24%     243.130ms      16.884us     122.201ms        52.99%     122.201ms       8.486us         14400     37541.118  \n",
      "                                               aten::mm         0.37%       4.201ms         0.47%       5.438ms      27.191us      44.911ms        19.48%      44.911ms     224.555us           200     17065.132  \n",
      "                                              aten::bmm         5.28%      60.425ms         7.24%      82.899ms      17.271us       8.310ms         3.60%       8.310ms       1.731us          4800       912.826  \n",
      "                                              aten::add         5.79%      66.330ms         8.67%      99.224ms      12.400us       9.766ms         4.24%       9.766ms       1.220us          8002         7.810  \n",
      "                                              aten::mul         2.27%      25.934ms         3.37%      38.531ms      12.831us       3.247ms         1.41%       3.247ms       1.081us          3003         2.061  \n",
      "                                            aten::empty         4.77%      54.627ms         4.77%      54.627ms       2.897us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.36%       4.079ms         6.16%      70.546ms      16.749us       0.000us         0.00%       1.499ms       0.356us          4212            --  \n",
      "                                         aten::_to_copy         0.77%       8.812ms         5.81%      66.467ms      23.696us       0.000us         0.00%       1.499ms       0.534us          2805            --  \n",
      "                                    aten::empty_strided         1.25%      14.283ms         1.25%      14.283ms       4.752us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.40%      16.045ms         4.75%      54.408ms      14.849us       2.414ms         1.05%       2.414ms       0.659us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.145s\n",
      "Self CUDA time total: 230.599ms\n",
      "\n",
      "output: tensor([[    2,  8275,    52,    33,    10,  7654,  9061,     7,  5393,   980,\n",
      "             6,    50,   197,    52,  1056,    15, 15582,  3875,    18,  1272,\n",
      "            78,   116, 50118, 50118,   100,   437,    45,   686,    99,    47,\n",
      "          1266,    30,    22, 23242,  5137,   980,   845, 50118, 50118,   100,\n",
      "           437,    45,   686,    99,    47,  1266,    30,    22, 23242,  5137,\n",
      "           980,   845, 50118, 50118,   100,   437,    45,   686,    99,    47,\n",
      "          1266,    30,    22, 23242,  5137,   980,   845, 50118, 50118,   100,\n",
      "           437,    45,   686,    99,    47,  1266,    30,    22, 23242,  5137,\n",
      "           980,   845, 50118, 50118,   100,   437,    45,   686,    99,    47,\n",
      "          1266,    30,    22, 23242,  5137,   980,   845, 50118, 50118,   100,\n",
      "           437,    45,   686,    99,    47,  1266,    30,    22, 23242,  5137,\n",
      "           980,   845, 50118, 50118,   100,   437,    45,   686,    99,    47,\n",
      "          1266,    30,    22, 23242,  5137,   980,   845, 50118, 50118,   100,\n",
      "           437,    45,   686,    99,    47,  1266,    30,    22, 23242,  5137,\n",
      "           980,   845, 50118, 50118,   100,   437,    45,   686,    99,    47,\n",
      "          1266,    30,    22, 23242,  5137,   980,   845, 50118, 50118,   100,\n",
      "           437,    45,   686,    99,    47,  1266,    30,    22, 23242,  5137,\n",
      "           980,   845, 50118, 50118,   100,   437,    45,   686,    99,    47,\n",
      "          1266,    30,    22, 23242,  5137,   980,   845, 50118, 50118,   100,\n",
      "           437,    45,   686,    99,    47,  1266,    30,    22, 23242,  5137,\n",
      "           980,   845, 50118, 50118,   100,   437,    45,   686,    99,    47,\n",
      "          1266,    30,    22, 23242,  5137,   980,   845, 50118, 50118,   100,\n",
      "           437,    45]], device='cuda:0')\n",
      "text_energy_per_token: [9.402694862599631]\n",
      "output_tokens: 222\n",
      "flop: 55528947426\n",
      "energy_consumed:  2087.398259497118\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.32%     177.728ms        22.10%     240.637ms      16.711us     122.188ms        52.98%     122.188ms       8.485us         14400     37541.118  \n",
      "                                               aten::mm         0.26%       2.794ms         0.37%       4.038ms      20.191us      44.943ms        19.49%      44.943ms     224.716us           200     17065.132  \n",
      "                                              aten::bmm         5.16%      56.198ms         6.99%      76.145ms      15.864us       8.316ms         3.61%       8.316ms       1.733us          4800       912.826  \n",
      "                                              aten::add         5.68%      61.821ms         8.68%      94.553ms      11.816us       9.761ms         4.23%       9.761ms       1.220us          8002         7.810  \n",
      "                                              aten::mul         2.34%      25.474ms         3.47%      37.838ms      12.600us       3.247ms         1.41%       3.247ms       1.081us          3003         2.061  \n",
      "                                            aten::empty         5.00%      54.401ms         5.00%      54.401ms       2.885us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.36%       3.932ms         6.31%      68.760ms      16.325us       0.000us         0.00%       1.500ms       0.356us          4212            --  \n",
      "                                         aten::_to_copy         0.75%       8.153ms         5.95%      64.827ms      23.111us       0.000us         0.00%       1.500ms       0.535us          2805            --  \n",
      "                                    aten::empty_strided         1.40%      15.244ms         1.40%      15.244ms       5.071us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.46%      15.875ms         4.93%      53.662ms      14.646us       2.415ms         1.05%       2.415ms       0.659us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.089s\n",
      "Self CUDA time total: 230.621ms\n",
      "\n",
      "output: tensor([[    2,  8275,    52,    33,    10,  7654,  9061,     7,  5393,   980,\n",
      "             6,    50,   197,    52,  1056,    15, 15582,  3875,    18,  1272,\n",
      "            78,   116, 50118, 50118,   100,   437,    45,   686,    99,    47,\n",
      "          1266,    30,    22, 23242,  5137,   980,   845, 50118, 50118,   100,\n",
      "           437,    45,   686,    99,    47,  1266,    30,    22, 23242,  5137,\n",
      "           980,   845, 50118, 50118,   100,   437,    45,   686,    99,    47,\n",
      "          1266,    30,    22, 23242,  5137,   980,   845, 50118, 50118,   100,\n",
      "           437,    45,   686,    99,    47,  1266,    30,    22, 23242,  5137,\n",
      "           980,   845, 50118, 50118,   100,   437,    45,   686,    99,    47,\n",
      "          1266,    30,    22, 23242,  5137,   980,   845, 50118, 50118,   100,\n",
      "           437,    45,   686,    99,    47,  1266,    30,    22, 23242,  5137,\n",
      "           980,   845, 50118, 50118,   100,   437,    45,   686,    99,    47,\n",
      "          1266,    30,    22, 23242,  5137,   980,   845, 50118, 50118,   100,\n",
      "           437,    45,   686,    99,    47,  1266,    30,    22, 23242,  5137,\n",
      "           980,   845, 50118, 50118,   100,   437,    45,   686,    99,    47,\n",
      "          1266,    30,    22, 23242,  5137,   980,   845, 50118, 50118,   100,\n",
      "           437,    45,   686,    99,    47,  1266,    30,    22, 23242,  5137,\n",
      "           980,   845, 50118, 50118,   100,   437,    45,   686,    99,    47,\n",
      "          1266,    30,    22, 23242,  5137,   980,   845, 50118, 50118,   100,\n",
      "           437,    45,   686,    99,    47,  1266,    30,    22, 23242,  5137,\n",
      "           980,   845, 50118, 50118,   100,   437,    45,   686,    99,    47,\n",
      "          1266,    30,    22, 23242,  5137,   980,   845, 50118, 50118,   100,\n",
      "           437,    45]], device='cuda:0')\n",
      "text_energy_per_token: [9.402694862599631, 9.447853774532112]\n",
      "output_tokens: 222\n",
      "flop: 55528947426\n",
      "energy_consumed:  2097.423537946129\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.79%     174.698ms        21.52%     238.127ms      16.537us     122.223ms        52.99%     122.223ms       8.488us         14400     37710.987  \n",
      "                                               aten::mm         0.38%       4.172ms         0.49%       5.396ms      26.982us      44.917ms        19.47%      44.917ms     224.585us           200     17142.350  \n",
      "                                              aten::bmm         5.26%      58.245ms         7.09%      78.483ms      16.351us       8.327ms         3.61%       8.327ms       1.735us          4800       921.821  \n",
      "                                              aten::add         6.00%      66.373ms         8.96%      99.150ms      12.391us       9.759ms         4.23%       9.759ms       1.220us          8002         7.864  \n",
      "                                              aten::mul         2.31%      25.539ms         3.43%      37.918ms      12.627us       3.245ms         1.41%       3.245ms       1.081us          3003         2.071  \n",
      "                                            aten::empty         4.83%      53.427ms         4.83%      53.427ms       2.833us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.895ms         6.15%      68.089ms      16.166us       0.000us         0.00%       1.496ms       0.355us          4212            --  \n",
      "                                         aten::_to_copy         0.75%       8.300ms         5.80%      64.194ms      22.886us       0.000us         0.00%       1.496ms       0.533us          2805            --  \n",
      "                                    aten::empty_strided         1.29%      14.299ms         1.29%      14.299ms       4.757us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.44%      15.965ms         4.75%      52.561ms      14.345us       2.410ms         1.04%       2.410ms       0.658us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.106s\n",
      "Self CUDA time total: 230.643ms\n",
      "\n",
      "output: tensor([[    2,  1121,    10,   232,   147, 11767,    16,  1959,  3150, 18689,\n",
      "             6,    16,    24,    55,   505,     7, 21790,   633,  5012,    50,\n",
      "          9874,  2017,   116, 50118, 50118,   133,  1948,    16,  4420,     4,\n",
      "         50118, 50118,   133,   864,    16,    35,    16,    24,    55,   505,\n",
      "             7, 21790,   633,  5012,    50,  9874,  2017,   116, 50118, 50118,\n",
      "           133,  1948,    16,  4420,     4, 50118, 50118,   133,   864,    16,\n",
      "            35,    16,    24,    55,   505,     7, 21790,   633,  5012,    50,\n",
      "          9874,  2017,   116, 50118, 50118,   133,  1948,    16,  4420,     4,\n",
      "         50118, 50118,   133,   864,    16,    35,    16,    24,    55,   505,\n",
      "             7, 21790,   633,  5012,    50,  9874,  2017,   116, 50118, 50118,\n",
      "           133,  1948,    16,  4420,     4, 50118, 50118,   133,   864,    16,\n",
      "            35,    16,    24,    55,   505,     7, 21790,   633,  5012,    50,\n",
      "          9874,  2017,   116, 50118, 50118,   133,  1948,    16,  4420,     4,\n",
      "         50118, 50118,   133,   864,    16,    35,    16,    24,    55,   505,\n",
      "             7, 21790,   633,  5012,    50,  9874,  2017,   116, 50118, 50118,\n",
      "           133,  1948,    16,  4420,     4, 50118, 50118,   133,   864,    16,\n",
      "            35,    16,    24,    55,   505,     7, 21790,   633,  5012,    50,\n",
      "          9874,  2017,   116, 50118, 50118,   133,  1948,    16,  4420,     4,\n",
      "         50118, 50118,   133,   864,    16,    35,    16,    24,    55,   505,\n",
      "             7, 21790,   633,  5012,    50,  9874,  2017,   116, 50118, 50118,\n",
      "           133,  1948,    16,  4420,     4, 50118, 50118,   133,   864,    16,\n",
      "            35,    16,    24,    55,   505,     7, 21790,   633,  5012,    50,\n",
      "          9874,  2017,   116]], device='cuda:0')\n",
      "text_energy_per_token: [9.174475227424594]\n",
      "output_tokens: 223\n",
      "flop: 55785093103\n",
      "energy_consumed:  2045.9079757156846\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.87%     178.649ms        21.67%     243.987ms      16.944us     122.266ms        53.00%     122.266ms       8.491us         14400     37710.987  \n",
      "                                               aten::mm         0.25%       2.802ms         0.36%       4.078ms      20.389us      44.922ms        19.47%      44.922ms     224.612us           200     17142.350  \n",
      "                                              aten::bmm         5.07%      57.125ms         6.88%      77.444ms      16.134us       8.350ms         3.62%       8.350ms       1.740us          4800       921.821  \n",
      "                                              aten::add         5.59%      62.921ms         8.50%      95.729ms      11.963us       9.755ms         4.23%       9.755ms       1.219us          8002         7.864  \n",
      "                                              aten::mul         2.29%      25.744ms         3.40%      38.230ms      12.731us       3.245ms         1.41%       3.245ms       1.080us          3003         2.071  \n",
      "                                            aten::empty         5.32%      59.913ms         5.32%      59.913ms       3.177us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.36%       4.015ms         6.20%      69.817ms      16.576us       0.000us         0.00%       1.502ms       0.357us          4212            --  \n",
      "                                         aten::_to_copy         0.76%       8.609ms         5.84%      65.802ms      23.459us       0.000us         0.00%       1.502ms       0.536us          2805            --  \n",
      "                                    aten::empty_strided         1.38%      15.511ms         1.38%      15.511ms       5.160us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.41%      15.930ms         4.79%      53.931ms      14.719us       2.417ms         1.05%       2.417ms       0.660us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.126s\n",
      "Self CUDA time total: 230.710ms\n",
      "\n",
      "output: tensor([[    2,  1121,    10,   232,   147, 11767,    16,  1959,  3150, 18689,\n",
      "             6,    16,    24,    55,   505,     7, 21790,   633,  5012,    50,\n",
      "          9874,  2017,   116, 50118, 50118,   133,  1948,    16,  4420,     4,\n",
      "         50118, 50118,   133,   864,    16,    35,    16,    24,    55,   505,\n",
      "             7, 21790,   633,  5012,    50,  9874,  2017,   116, 50118, 50118,\n",
      "           133,  1948,    16,  4420,     4, 50118, 50118,   133,   864,    16,\n",
      "            35,    16,    24,    55,   505,     7, 21790,   633,  5012,    50,\n",
      "          9874,  2017,   116, 50118, 50118,   133,  1948,    16,  4420,     4,\n",
      "         50118, 50118,   133,   864,    16,    35,    16,    24,    55,   505,\n",
      "             7, 21790,   633,  5012,    50,  9874,  2017,   116, 50118, 50118,\n",
      "           133,  1948,    16,  4420,     4, 50118, 50118,   133,   864,    16,\n",
      "            35,    16,    24,    55,   505,     7, 21790,   633,  5012,    50,\n",
      "          9874,  2017,   116, 50118, 50118,   133,  1948,    16,  4420,     4,\n",
      "         50118, 50118,   133,   864,    16,    35,    16,    24,    55,   505,\n",
      "             7, 21790,   633,  5012,    50,  9874,  2017,   116, 50118, 50118,\n",
      "           133,  1948,    16,  4420,     4, 50118, 50118,   133,   864,    16,\n",
      "            35,    16,    24,    55,   505,     7, 21790,   633,  5012,    50,\n",
      "          9874,  2017,   116, 50118, 50118,   133,  1948,    16,  4420,     4,\n",
      "         50118, 50118,   133,   864,    16,    35,    16,    24,    55,   505,\n",
      "             7, 21790,   633,  5012,    50,  9874,  2017,   116, 50118, 50118,\n",
      "           133,  1948,    16,  4420,     4, 50118, 50118,   133,   864,    16,\n",
      "            35,    16,    24,    55,   505,     7, 21790,   633,  5012,    50,\n",
      "          9874,  2017,   116]], device='cuda:0')\n",
      "text_energy_per_token: [9.174475227424594, 9.121007144294405]\n",
      "output_tokens: 223\n",
      "flop: 55785093103\n",
      "energy_consumed:  2033.9845931776522\n",
      "Processing category: coding\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.24%     175.993ms        22.14%     239.960ms      16.664us     122.202ms        52.96%     122.202ms       8.486us         14400     38560.334  \n",
      "                                               aten::mm         0.26%       2.806ms         0.37%       4.023ms      20.116us      44.906ms        19.46%      44.906ms     224.529us           200     17528.439  \n",
      "                                              aten::bmm         5.18%      56.106ms         7.01%      75.946ms      15.822us       8.373ms         3.63%       8.373ms       1.744us          4800       967.901  \n",
      "                                              aten::add         5.68%      61.573ms         8.69%      94.170ms      11.768us       9.769ms         4.23%       9.769ms       1.221us          8002         8.140  \n",
      "                                              aten::mul         2.33%      25.261ms         3.46%      37.530ms      12.498us       3.248ms         1.41%       3.248ms       1.082us          3003         2.118  \n",
      "                                            aten::empty         5.22%      56.625ms         5.22%      56.625ms       3.003us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.755ms         6.18%      67.024ms      15.913us       0.000us         0.00%       1.503ms       0.357us          4212            --  \n",
      "                                         aten::_to_copy         0.74%       8.017ms         5.84%      63.269ms      22.556us       0.000us         0.00%       1.503ms       0.536us          2805            --  \n",
      "                                    aten::empty_strided         1.41%      15.255ms         1.41%      15.255ms       5.075us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.46%      15.862ms         4.81%      52.144ms      14.231us       2.420ms         1.05%       2.420ms       0.660us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.084s\n",
      "Self CUDA time total: 230.725ms\n",
      "\n",
      "output: tensor([[    2, 42627,    10,   230, 42964,   586,    14,  7005,    10,  2788,\n",
      "          2870,   516,    30,   516,     8,  3948,     5,   346,     9, 39036,\n",
      "             9,    10,  2167,  2136,    11,     5,  2870,     4, 50118, 50118,\n",
      "         44758,    10,   230, 42964,   586,    14,  7005,    10,  2788,  2870,\n",
      "           516,    30,   516,     8,  3948,     5,   346,     9, 39036,     9,\n",
      "            10,  2167,  2136,    11,     5,  2870,     4, 50118, 50118, 44758,\n",
      "            10,   230, 42964,   586,    14,  7005,    10,  2788,  2870,   516,\n",
      "            30,   516,     8,  3948,     5,   346,     9, 39036,     9,    10,\n",
      "          2167,  2136,    11,     5,  2870,     4, 50118, 50118, 44758,    10,\n",
      "           230, 42964,   586,    14,  7005,    10,  2788,  2870,   516,    30,\n",
      "           516,     8,  3948,     5,   346,     9, 39036,     9,    10,  2167,\n",
      "          2136,    11,     5,  2870,     4, 50118, 50118, 44758,    10,   230,\n",
      "         42964,   586,    14,  7005,    10,  2788,  2870,   516,    30,   516,\n",
      "             8,  3948,     5,   346,     9, 39036,     9,    10,  2167,  2136,\n",
      "            11,     5,  2870,     4, 50118, 50118, 44758,    10,   230, 42964,\n",
      "           586,    14,  7005,    10,  2788,  2870,   516,    30,   516,     8,\n",
      "          3948,     5,   346,     9, 39036,     9,    10,  2167,  2136,    11,\n",
      "             5,  2870,     4, 50118, 50118, 44758,    10,   230, 42964,   586,\n",
      "            14,  7005,    10,  2788,  2870,   516,    30,   516,     8,  3948,\n",
      "             5,   346,     9, 39036,     9,    10,  2167,  2136,    11,     5,\n",
      "          2870,     4, 50118, 50118, 44758,    10,   230, 42964,   586,    14,\n",
      "          7005,    10,  2788,  2870,   516,    30,   516,     8,  3948,     5,\n",
      "           346,     9, 39036,     9,    10,  2167,  2136,    11]],\n",
      "       device='cuda:0')\n",
      "text_energy_per_token: [9.069642663687468]\n",
      "output_tokens: 228\n",
      "flop: 57066931728\n",
      "energy_consumed:  2067.8785273207427\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.13%     180.704ms        21.95%     245.890ms      17.076us     122.219ms        52.97%     122.219ms       8.487us         14400     38560.334  \n",
      "                                               aten::mm         0.25%       2.816ms         0.36%       4.061ms      20.307us      44.902ms        19.46%      44.902ms     224.508us           200     17528.439  \n",
      "                                              aten::bmm         5.07%      56.842ms         6.88%      77.022ms      16.046us       8.375ms         3.63%       8.375ms       1.745us          4800       967.901  \n",
      "                                              aten::add         5.61%      62.853ms         8.54%      95.649ms      11.953us       9.760ms         4.23%       9.760ms       1.220us          8002         8.140  \n",
      "                                              aten::mul         2.29%      25.620ms         3.40%      38.068ms      12.677us       3.246ms         1.41%       3.246ms       1.081us          3003         2.118  \n",
      "                                            aten::empty         5.16%      57.780ms         5.16%      57.780ms       3.064us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.865ms         6.21%      69.533ms      16.508us       0.000us         0.00%       1.500ms       0.356us          4212            --  \n",
      "                                         aten::_to_copy         0.76%       8.511ms         5.86%      65.668ms      23.411us       0.000us         0.00%       1.500ms       0.535us          2805            --  \n",
      "                                    aten::empty_strided         1.34%      15.021ms         1.34%      15.021ms       4.997us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.43%      15.995ms         4.80%      53.753ms      14.671us       2.417ms         1.05%       2.417ms       0.660us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.120s\n",
      "Self CUDA time total: 230.746ms\n",
      "\n",
      "output: tensor([[    2, 42627,    10,   230, 42964,   586,    14,  7005,    10,  2788,\n",
      "          2870,   516,    30,   516,     8,  3948,     5,   346,     9, 39036,\n",
      "             9,    10,  2167,  2136,    11,     5,  2870,     4, 50118, 50118,\n",
      "         44758,    10,   230, 42964,   586,    14,  7005,    10,  2788,  2870,\n",
      "           516,    30,   516,     8,  3948,     5,   346,     9, 39036,     9,\n",
      "            10,  2167,  2136,    11,     5,  2870,     4, 50118, 50118, 44758,\n",
      "            10,   230, 42964,   586,    14,  7005,    10,  2788,  2870,   516,\n",
      "            30,   516,     8,  3948,     5,   346,     9, 39036,     9,    10,\n",
      "          2167,  2136,    11,     5,  2870,     4, 50118, 50118, 44758,    10,\n",
      "           230, 42964,   586,    14,  7005,    10,  2788,  2870,   516,    30,\n",
      "           516,     8,  3948,     5,   346,     9, 39036,     9,    10,  2167,\n",
      "          2136,    11,     5,  2870,     4, 50118, 50118, 44758,    10,   230,\n",
      "         42964,   586,    14,  7005,    10,  2788,  2870,   516,    30,   516,\n",
      "             8,  3948,     5,   346,     9, 39036,     9,    10,  2167,  2136,\n",
      "            11,     5,  2870,     4, 50118, 50118, 44758,    10,   230, 42964,\n",
      "           586,    14,  7005,    10,  2788,  2870,   516,    30,   516,     8,\n",
      "          3948,     5,   346,     9, 39036,     9,    10,  2167,  2136,    11,\n",
      "             5,  2870,     4, 50118, 50118, 44758,    10,   230, 42964,   586,\n",
      "            14,  7005,    10,  2788,  2870,   516,    30,   516,     8,  3948,\n",
      "             5,   346,     9, 39036,     9,    10,  2167,  2136,    11,     5,\n",
      "          2870,     4, 50118, 50118, 44758,    10,   230, 42964,   586,    14,\n",
      "          7005,    10,  2788,  2870,   516,    30,   516,     8,  3948,     5,\n",
      "           346,     9, 39036,     9,    10,  2167,  2136,    11]],\n",
      "       device='cuda:0')\n",
      "text_energy_per_token: [9.069642663687468, 9.213947685791108]\n",
      "output_tokens: 228\n",
      "flop: 57066931728\n",
      "energy_consumed:  2100.7800723603727\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.06%     183.124ms        21.85%     249.118ms      17.300us     122.387ms        53.01%     122.387ms       8.499us         14400     37371.249  \n",
      "                                               aten::mm         0.25%       2.802ms         0.35%       4.045ms      20.227us      44.965ms        19.48%      44.965ms     224.827us           200     16987.914  \n",
      "                                              aten::bmm         5.04%      57.417ms         6.83%      77.853ms      16.219us       8.311ms         3.60%       8.311ms       1.731us          4800       903.905  \n",
      "                                              aten::add         5.55%      63.310ms         8.44%      96.248ms      12.028us       9.763ms         4.23%       9.763ms       1.220us          8002         7.756  \n",
      "                                              aten::mul         2.27%      25.833ms         3.38%      38.530ms      12.830us       3.245ms         1.41%       3.245ms       1.081us          3003         2.052  \n",
      "                                            aten::empty         5.15%      58.746ms         5.15%      58.746ms       3.115us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       4.016ms         6.17%      70.307ms      16.692us       0.000us         0.00%       1.498ms       0.356us          4212            --  \n",
      "                                         aten::_to_copy         0.77%       8.780ms         5.81%      66.292ms      23.633us       0.000us         0.00%       1.498ms       0.534us          2805            --  \n",
      "                                    aten::empty_strided         1.38%      15.690ms         1.38%      15.690ms       5.219us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.40%      15.983ms         4.75%      54.140ms      14.776us       2.412ms         1.04%       2.412ms       0.658us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.140s\n",
      "Self CUDA time total: 230.856ms\n",
      "\n",
      "output: tensor([[    2, 20470, 40224,    10, 31886,  5043,     7,   465,     5,  6463,\n",
      "          1537, 49734,  4086,     9,    80,  8135, 22052,   634,  6878,  8326,\n",
      "             4, 50118, 50118, 44758,    10, 31886,  5043,     7,   465,     5,\n",
      "          6463,  1537, 49734,  4086,     9,    80,  8135, 22052,   634,  6878,\n",
      "          8326,     4, 50118, 50118, 44758,    10, 31886,  5043,     7,   465,\n",
      "             5,  6463,  1537, 49734,  4086,     9,    80,  8135, 22052,   634,\n",
      "          6878,  8326,     4, 50118, 50118, 44758,    10, 31886,  5043,     7,\n",
      "           465,     5,  6463,  1537, 49734,  4086,     9,    80,  8135, 22052,\n",
      "           634,  6878,  8326,     4, 50118, 50118, 44758,    10, 31886,  5043,\n",
      "             7,   465,     5,  6463,  1537, 49734,  4086,     9,    80,  8135,\n",
      "         22052,   634,  6878,  8326,     4, 50118, 50118, 44758,    10, 31886,\n",
      "          5043,     7,   465,     5,  6463,  1537, 49734,  4086,     9,    80,\n",
      "          8135, 22052,   634,  6878,  8326,     4, 50118, 50118, 44758,    10,\n",
      "         31886,  5043,     7,   465,     5,  6463,  1537, 49734,  4086,     9,\n",
      "            80,  8135, 22052,   634,  6878,  8326,     4, 50118, 50118, 44758,\n",
      "            10, 31886,  5043,     7,   465,     5,  6463,  1537, 49734,  4086,\n",
      "             9,    80,  8135, 22052,   634,  6878,  8326,     4, 50118, 50118,\n",
      "         44758,    10, 31886,  5043,     7,   465,     5,  6463,  1537, 49734,\n",
      "          4086,     9,    80,  8135, 22052,   634,  6878,  8326,     4, 50118,\n",
      "         50118, 44758,    10, 31886,  5043,     7,   465,     5,  6463,  1537,\n",
      "         49734,  4086,     9,    80,  8135, 22052,   634,  6878,  8326,     4,\n",
      "         50118, 50118, 44758,    10, 31886,  5043,     7,   465,     5,  6463,\n",
      "          1537]], device='cuda:0')\n",
      "text_energy_per_token: [9.799420747630197]\n",
      "output_tokens: 221\n",
      "flop: 55272875765\n",
      "energy_consumed:  2165.6719852262736\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.26%     177.942ms        22.11%     241.962ms      16.803us     122.344ms        53.01%     122.344ms       8.496us         14400     37371.249  \n",
      "                                               aten::mm         0.26%       2.799ms         0.37%       4.003ms      20.016us      44.943ms        19.47%      44.943ms     224.715us           200     16987.914  \n",
      "                                              aten::bmm         5.15%      56.378ms         6.98%      76.356ms      15.908us       8.308ms         3.60%       8.308ms       1.731us          4800       903.905  \n",
      "                                              aten::add         5.65%      61.789ms         8.63%      94.428ms      11.801us       9.771ms         4.23%       9.771ms       1.221us          8002         7.756  \n",
      "                                              aten::mul         2.32%      25.415ms         3.46%      37.901ms      12.621us       3.247ms         1.41%       3.247ms       1.081us          3003         2.052  \n",
      "                                            aten::empty         5.20%      56.874ms         5.20%      56.874ms       3.016us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.791ms         6.17%      67.544ms      16.036us       0.000us         0.00%       1.499ms       0.356us          4212            --  \n",
      "                                         aten::_to_copy         0.75%       8.234ms         5.83%      63.753ms      22.728us       0.000us         0.00%       1.499ms       0.534us          2805            --  \n",
      "                                    aten::empty_strided         1.40%      15.354ms         1.40%      15.354ms       5.108us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.45%      15.872ms         4.78%      52.338ms      14.285us       2.416ms         1.05%       2.416ms       0.659us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.094s\n",
      "Self CUDA time total: 230.781ms\n",
      "\n",
      "output: tensor([[    2, 20470, 40224,    10, 31886,  5043,     7,   465,     5,  6463,\n",
      "          1537, 49734,  4086,     9,    80,  8135, 22052,   634,  6878,  8326,\n",
      "             4, 50118, 50118, 44758,    10, 31886,  5043,     7,   465,     5,\n",
      "          6463,  1537, 49734,  4086,     9,    80,  8135, 22052,   634,  6878,\n",
      "          8326,     4, 50118, 50118, 44758,    10, 31886,  5043,     7,   465,\n",
      "             5,  6463,  1537, 49734,  4086,     9,    80,  8135, 22052,   634,\n",
      "          6878,  8326,     4, 50118, 50118, 44758,    10, 31886,  5043,     7,\n",
      "           465,     5,  6463,  1537, 49734,  4086,     9,    80,  8135, 22052,\n",
      "           634,  6878,  8326,     4, 50118, 50118, 44758,    10, 31886,  5043,\n",
      "             7,   465,     5,  6463,  1537, 49734,  4086,     9,    80,  8135,\n",
      "         22052,   634,  6878,  8326,     4, 50118, 50118, 44758,    10, 31886,\n",
      "          5043,     7,   465,     5,  6463,  1537, 49734,  4086,     9,    80,\n",
      "          8135, 22052,   634,  6878,  8326,     4, 50118, 50118, 44758,    10,\n",
      "         31886,  5043,     7,   465,     5,  6463,  1537, 49734,  4086,     9,\n",
      "            80,  8135, 22052,   634,  6878,  8326,     4, 50118, 50118, 44758,\n",
      "            10, 31886,  5043,     7,   465,     5,  6463,  1537, 49734,  4086,\n",
      "             9,    80,  8135, 22052,   634,  6878,  8326,     4, 50118, 50118,\n",
      "         44758,    10, 31886,  5043,     7,   465,     5,  6463,  1537, 49734,\n",
      "          4086,     9,    80,  8135, 22052,   634,  6878,  8326,     4, 50118,\n",
      "         50118, 44758,    10, 31886,  5043,     7,   465,     5,  6463,  1537,\n",
      "         49734,  4086,     9,    80,  8135, 22052,   634,  6878,  8326,     4,\n",
      "         50118, 50118, 44758,    10, 31886,  5043,     7,   465,     5,  6463,\n",
      "          1537]], device='cuda:0')\n",
      "text_energy_per_token: [9.799420747630197, 9.173735509780522]\n",
      "output_tokens: 221\n",
      "flop: 55272875765\n",
      "energy_consumed:  2027.3955476614954\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.25%     181.145ms        21.94%     244.586ms      16.985us     122.105ms        53.04%     122.105ms       8.480us         14400     36182.163  \n",
      "                                               aten::mm         0.26%       2.863ms         0.37%       4.091ms      20.456us      44.930ms        19.51%      44.930ms     224.648us           200     16447.390  \n",
      "                                              aten::bmm         5.09%      56.734ms         6.93%      77.283ms      16.101us       8.143ms         3.54%       8.143ms       1.696us          4800       843.522  \n",
      "                                              aten::add         5.74%      64.014ms         8.68%      96.724ms      12.087us       9.753ms         4.24%       9.753ms       1.219us          8002         7.385  \n",
      "                                              aten::mul         2.42%      26.930ms         3.54%      39.417ms      13.126us       3.242ms         1.41%       3.242ms       1.080us          3003         1.986  \n",
      "                                            aten::empty         5.17%      57.604ms         5.17%      57.604ms       3.055us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.34%       3.846ms         6.22%      69.356ms      16.466us       0.000us         0.00%       1.495ms       0.355us          4212            --  \n",
      "                                         aten::_to_copy         0.75%       8.406ms         5.88%      65.511ms      23.355us       0.000us         0.00%       1.495ms       0.533us          2805            --  \n",
      "                                    aten::empty_strided         1.39%      15.467ms         1.39%      15.467ms       5.145us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.43%      15.996ms         4.84%      53.943ms      14.722us       2.409ms         1.05%       2.409ms       0.658us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.115s\n",
      "Self CUDA time total: 230.232ms\n",
      "\n",
      "output: tensor([[    2, 20470, 40224,    10,  1675,  8151,    11, 31886,     7, 28754,\n",
      "            41,  1047,  1100,     4, 50118, 50118,   713,    16,    10,   182,\n",
      "          1537,   936,    19,  1047,  8480,     4, 50118, 50118,   133,    78,\n",
      "           631,     7,   109,    16,     7,  1045,    10,    92,  1047,  1100,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758]], device='cuda:0')\n",
      "text_energy_per_token: [9.152277589913394]\n",
      "output_tokens: 214\n",
      "flop: 53482446586\n",
      "energy_consumed:  1958.5874042414664\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.54%     174.328ms        21.36%     239.550ms      16.635us     122.065ms        53.02%     122.065ms       8.477us         14400     36182.163  \n",
      "                                               aten::mm         0.37%       4.138ms         0.47%       5.317ms      26.586us      44.953ms        19.53%      44.953ms     224.763us           200     16447.390  \n",
      "                                              aten::bmm         5.32%      59.712ms         7.13%      79.984ms      16.663us       8.133ms         3.53%       8.133ms       1.694us          4800       843.522  \n",
      "                                              aten::add         5.95%      66.680ms         8.86%      99.406ms      12.423us       9.761ms         4.24%       9.761ms       1.220us          8002         7.385  \n",
      "                                              aten::mul         2.40%      26.963ms         3.52%      39.471ms      13.144us       3.245ms         1.41%       3.245ms       1.081us          3003         1.986  \n",
      "                                            aten::empty         4.80%      53.780ms         4.80%      53.780ms       2.852us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.36%       3.992ms         6.22%      69.732ms      16.555us       0.000us         0.00%       1.490ms       0.354us          4212            --  \n",
      "                                         aten::_to_copy         0.76%       8.495ms         5.86%      65.740ms      23.437us       0.000us         0.00%       1.490ms       0.531us          2805            --  \n",
      "                                    aten::empty_strided         1.27%      14.270ms         1.27%      14.270ms       4.747us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.42%      15.946ms         4.82%      54.004ms      14.739us       2.404ms         1.04%       2.404ms       0.656us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.121s\n",
      "Self CUDA time total: 230.214ms\n",
      "\n",
      "output: tensor([[    2, 20470, 40224,    10,  1675,  8151,    11, 31886,     7, 28754,\n",
      "            41,  1047,  1100,     4, 50118, 50118,   713,    16,    10,   182,\n",
      "          1537,   936,    19,  1047,  8480,     4, 50118, 50118,   133,    78,\n",
      "           631,     7,   109,    16,     7,  1045,    10,    92,  1047,  1100,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758]], device='cuda:0')\n",
      "text_energy_per_token: [9.152277589913394, 9.803454567245902]\n",
      "output_tokens: 214\n",
      "flop: 53482446586\n",
      "energy_consumed:  2097.939277390623\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.68%     171.343ms        21.55%     235.470ms      16.352us     122.126ms        53.00%     122.126ms       8.481us         14400     36691.771  \n",
      "                                               aten::mm         0.26%       2.848ms         0.37%       4.088ms      20.441us      44.920ms        19.49%      44.920ms     224.600us           200     16679.043  \n",
      "                                              aten::bmm         5.52%      60.276ms         7.35%      80.376ms      16.745us       8.282ms         3.59%       8.282ms       1.726us          4800       868.958  \n",
      "                                              aten::add         5.79%      63.274ms         8.91%      97.339ms      12.164us       9.761ms         4.24%       9.761ms       1.220us          8002         7.542  \n",
      "                                              aten::mul         2.32%      25.383ms         3.45%      37.702ms      12.555us       3.244ms         1.41%       3.244ms       1.080us          3003         2.014  \n",
      "                                            aten::empty         4.83%      52.837ms         4.83%      52.837ms       2.802us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.792ms         6.42%      70.154ms      16.656us       0.000us         0.00%       1.497ms       0.355us          4212            --  \n",
      "                                         aten::_to_copy         0.75%       8.173ms         6.07%      66.362ms      23.659us       0.000us         0.00%       1.497ms       0.534us          2805            --  \n",
      "                                    aten::empty_strided         1.28%      13.975ms         1.28%      13.975ms       4.649us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.57%      17.200ms         5.04%      55.098ms      15.038us       2.412ms         1.05%       2.412ms       0.658us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.093s\n",
      "Self CUDA time total: 230.442ms\n",
      "\n",
      "output: tensor([[    2, 45714,    10,   586,     7,   465,     5,   295,   212, 28174,\n",
      "           261, 29522,   346,   634,  6878,  8326,     4, 50118, 50118, 44758,\n",
      "            10,   586,     7,   465,     5,   295,   212, 28174,   261, 29522,\n",
      "           346,   634,  6878,  8326,     4, 50118, 50118, 44758,    10,   586,\n",
      "             7,   465,     5,   295,   212, 28174,   261, 29522,   346,   634,\n",
      "          6878,  8326,     4, 50118, 50118, 44758,    10,   586,     7,   465,\n",
      "             5,   295,   212, 28174,   261, 29522,   346,   634,  6878,  8326,\n",
      "             4, 50118, 50118, 44758,    10,   586,     7,   465,     5,   295,\n",
      "           212, 28174,   261, 29522,   346,   634,  6878,  8326,     4, 50118,\n",
      "         50118, 44758,    10,   586,     7,   465,     5,   295,   212, 28174,\n",
      "           261, 29522,   346,   634,  6878,  8326,     4, 50118, 50118, 44758,\n",
      "            10,   586,     7,   465,     5,   295,   212, 28174,   261, 29522,\n",
      "           346,   634,  6878,  8326,     4, 50118, 50118, 44758,    10,   586,\n",
      "             7,   465,     5,   295,   212, 28174,   261, 29522,   346,   634,\n",
      "          6878,  8326,     4, 50118, 50118, 44758,    10,   586,     7,   465,\n",
      "             5,   295,   212, 28174,   261, 29522,   346,   634,  6878,  8326,\n",
      "             4, 50118, 50118, 44758,    10,   586,     7,   465,     5,   295,\n",
      "           212, 28174,   261, 29522,   346,   634,  6878,  8326,     4, 50118,\n",
      "         50118, 44758,    10,   586,     7,   465,     5,   295,   212, 28174,\n",
      "           261, 29522,   346,   634,  6878,  8326,     4, 50118, 50118, 44758,\n",
      "            10,   586,     7,   465,     5,   295,   212, 28174,   261, 29522,\n",
      "           346,   634,  6878,  8326,     4, 50118, 50118]], device='cuda:0')\n",
      "text_energy_per_token: [9.315637681095502]\n",
      "output_tokens: 217\n",
      "flop: 54249329281\n",
      "energy_consumed:  2021.493376797724\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.60%     175.741ms        21.74%     244.892ms      17.006us     122.053ms        52.98%     122.053ms       8.476us         14400     36691.771  \n",
      "                                               aten::mm         0.25%       2.815ms         0.36%       4.056ms      20.279us      44.909ms        19.49%      44.909ms     224.544us           200     16679.043  \n",
      "                                              aten::bmm         5.19%      58.448ms         7.00%      78.869ms      16.431us       8.278ms         3.59%       8.278ms       1.725us          4800       868.958  \n",
      "                                              aten::add         5.58%      62.866ms         8.61%      97.036ms      12.126us       9.760ms         4.24%       9.760ms       1.220us          8002         7.542  \n",
      "                                              aten::mul         2.28%      25.731ms         3.38%      38.123ms      12.695us       3.246ms         1.41%       3.246ms       1.081us          3003         2.014  \n",
      "                                            aten::empty         4.92%      55.419ms         4.92%      55.419ms       2.939us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.941ms         6.24%      70.276ms      16.685us       0.000us         0.00%       1.498ms       0.356us          4212            --  \n",
      "                                         aten::_to_copy         0.76%       8.607ms         5.89%      66.335ms      23.649us       0.000us         0.00%       1.498ms       0.534us          2805            --  \n",
      "                                    aten::empty_strided         1.29%      14.519ms         1.29%      14.519ms       4.830us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.42%      16.040ms         4.81%      54.207ms      14.795us       2.413ms         1.05%       2.413ms       0.658us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.126s\n",
      "Self CUDA time total: 230.369ms\n",
      "\n",
      "output: tensor([[    2, 45714,    10,   586,     7,   465,     5,   295,   212, 28174,\n",
      "           261, 29522,   346,   634,  6878,  8326,     4, 50118, 50118, 44758,\n",
      "            10,   586,     7,   465,     5,   295,   212, 28174,   261, 29522,\n",
      "           346,   634,  6878,  8326,     4, 50118, 50118, 44758,    10,   586,\n",
      "             7,   465,     5,   295,   212, 28174,   261, 29522,   346,   634,\n",
      "          6878,  8326,     4, 50118, 50118, 44758,    10,   586,     7,   465,\n",
      "             5,   295,   212, 28174,   261, 29522,   346,   634,  6878,  8326,\n",
      "             4, 50118, 50118, 44758,    10,   586,     7,   465,     5,   295,\n",
      "           212, 28174,   261, 29522,   346,   634,  6878,  8326,     4, 50118,\n",
      "         50118, 44758,    10,   586,     7,   465,     5,   295,   212, 28174,\n",
      "           261, 29522,   346,   634,  6878,  8326,     4, 50118, 50118, 44758,\n",
      "            10,   586,     7,   465,     5,   295,   212, 28174,   261, 29522,\n",
      "           346,   634,  6878,  8326,     4, 50118, 50118, 44758,    10,   586,\n",
      "             7,   465,     5,   295,   212, 28174,   261, 29522,   346,   634,\n",
      "          6878,  8326,     4, 50118, 50118, 44758,    10,   586,     7,   465,\n",
      "             5,   295,   212, 28174,   261, 29522,   346,   634,  6878,  8326,\n",
      "             4, 50118, 50118, 44758,    10,   586,     7,   465,     5,   295,\n",
      "           212, 28174,   261, 29522,   346,   634,  6878,  8326,     4, 50118,\n",
      "         50118, 44758,    10,   586,     7,   465,     5,   295,   212, 28174,\n",
      "           261, 29522,   346,   634,  6878,  8326,     4, 50118, 50118, 44758,\n",
      "            10,   586,     7,   465,     5,   295,   212, 28174,   261, 29522,\n",
      "           346,   634,  6878,  8326,     4, 50118, 50118]], device='cuda:0')\n",
      "text_energy_per_token: [9.315637681095502, 9.742478944642754]\n",
      "output_tokens: 217\n",
      "flop: 54249329281\n",
      "energy_consumed:  2114.1179309874774\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.60%     177.336ms        21.74%     247.063ms      17.157us     122.152ms        52.99%     122.152ms       8.483us         14400     36691.771  \n",
      "                                               aten::mm         0.25%       2.815ms         0.36%       4.047ms      20.235us      44.927ms        19.49%      44.927ms     224.637us           200     16679.043  \n",
      "                                              aten::bmm         5.17%      58.810ms         6.97%      79.229ms      16.506us       8.284ms         3.59%       8.284ms       1.726us          4800       868.958  \n",
      "                                              aten::add         5.58%      63.414ms         8.60%      97.727ms      12.213us       9.764ms         4.24%       9.764ms       1.220us          8002         7.542  \n",
      "                                              aten::mul         2.27%      25.791ms         3.36%      38.192ms      12.718us       3.250ms         1.41%       3.250ms       1.082us          3003         2.014  \n",
      "                                            aten::empty         4.91%      55.788ms         4.91%      55.788ms       2.958us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.938ms         6.20%      70.454ms      16.727us       0.000us         0.00%       1.505ms       0.357us          4212            --  \n",
      "                                         aten::_to_copy         0.77%       8.788ms         5.85%      66.516ms      23.713us       0.000us         0.00%       1.505ms       0.537us          2805            --  \n",
      "                                    aten::empty_strided         1.26%      14.346ms         1.26%      14.346ms       4.772us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.41%      16.075ms         4.78%      54.349ms      14.833us       2.421ms         1.05%       2.421ms       0.661us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.136s\n",
      "Self CUDA time total: 230.524ms\n",
      "\n",
      "output: tensor([[    2, 20470, 40224,    10, 32771,  1707, 17194,     7,   465,    10,\n",
      "          2167,  7510,    11,    10, 24713,  8932,     4, 50118, 50118,   713,\n",
      "            16,    10,   182,  2007,     8,  1365,   169,     7,   465,    10,\n",
      "          1989,  7510,    11,    10, 24713,  8932,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4]], device='cuda:0')\n",
      "text_energy_per_token: [9.731287730321576]\n",
      "output_tokens: 217\n",
      "flop: 54249329281\n",
      "energy_consumed:  2111.689437479782\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.70%     172.706ms        21.82%     239.984ms      16.666us     122.210ms        53.00%     122.210ms       8.487us         14400     36691.771  \n",
      "                                               aten::mm         0.25%       2.803ms         0.37%       4.035ms      20.177us      44.915ms        19.48%      44.915ms     224.574us           200     16679.043  \n",
      "                                              aten::bmm         5.25%      57.742ms         7.07%      77.755ms      16.199us       8.285ms         3.59%       8.285ms       1.726us          4800       868.958  \n",
      "                                              aten::add         5.66%      62.301ms         8.75%      96.228ms      12.026us       9.766ms         4.24%       9.766ms       1.220us          8002         7.542  \n",
      "                                              aten::mul         2.32%      25.520ms         3.44%      37.858ms      12.607us       3.245ms         1.41%       3.245ms       1.081us          3003         2.014  \n",
      "                                            aten::empty         4.95%      54.413ms         4.95%      54.413ms       2.885us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.836ms         6.17%      67.862ms      16.112us       0.000us         0.00%       1.503ms       0.357us          4212            --  \n",
      "                                         aten::_to_copy         0.76%       8.335ms         5.82%      64.026ms      22.826us       0.000us         0.00%       1.503ms       0.536us          2805            --  \n",
      "                                    aten::empty_strided         1.28%      14.044ms         1.28%      14.044ms       4.672us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.45%      15.909ms         4.78%      52.588ms      14.353us       2.419ms         1.05%       2.419ms       0.660us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.100s\n",
      "Self CUDA time total: 230.597ms\n",
      "\n",
      "output: tensor([[    2, 20470, 40224,    10, 32771,  1707, 17194,     7,   465,    10,\n",
      "          2167,  7510,    11,    10, 24713,  8932,     4, 50118, 50118,   713,\n",
      "            16,    10,   182,  2007,     8,  1365,   169,     7,   465,    10,\n",
      "          1989,  7510,    11,    10, 24713,  8932,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4]], device='cuda:0')\n",
      "text_energy_per_token: [9.731287730321576, 9.83710998038762]\n",
      "output_tokens: 217\n",
      "flop: 54249329281\n",
      "energy_consumed:  2134.6528657441136\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.99%     180.027ms        21.66%     243.847ms      16.934us     122.053ms        53.02%     122.053ms       8.476us         14400     36012.294  \n",
      "                                               aten::mm         0.25%       2.855ms         0.36%       4.093ms      20.464us      44.933ms        19.52%      44.933ms     224.664us           200     16370.172  \n",
      "                                              aten::bmm         5.07%      57.095ms         6.87%      77.371ms      16.119us       8.131ms         3.53%       8.131ms       1.694us          4800       835.191  \n",
      "                                              aten::add         5.71%      64.282ms         8.63%      97.157ms      12.142us       9.755ms         4.24%       9.755ms       1.219us          8002         7.334  \n",
      "                                              aten::mul         2.40%      27.039ms         3.52%      39.631ms      13.197us       3.245ms         1.41%       3.245ms       1.081us          3003         1.977  \n",
      "                                            aten::empty         5.15%      57.948ms         5.15%      57.948ms       3.073us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.947ms         6.22%      70.046ms      16.630us       0.000us         0.00%       1.497ms       0.356us          4212            --  \n",
      "                                         aten::_to_copy         0.75%       8.488ms         5.87%      66.099ms      23.565us       0.000us         0.00%       1.497ms       0.534us          2805            --  \n",
      "                                    aten::empty_strided         1.41%      15.834ms         1.41%      15.834ms       5.267us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.42%      15.993ms         4.80%      54.090ms      14.763us       2.412ms         1.05%       2.412ms       0.658us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.126s\n",
      "Self CUDA time total: 230.198ms\n",
      "\n",
      "output: tensor([[    2, 20470, 40224,    10, 21021,   414,  3184,   634,    80, 32201,\n",
      "            11, 31886,     4, 50118, 50118,   100,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414]], device='cuda:0')\n",
      "text_energy_per_token: [9.46018960715952]\n",
      "output_tokens: 213\n",
      "flop: 53226967053\n",
      "energy_consumed:  2015.020386324978\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.61%     174.230ms        21.41%     238.962ms      16.595us     122.033ms        53.04%     122.033ms       8.475us         14400     36012.294  \n",
      "                                               aten::mm         0.37%       4.133ms         0.48%       5.339ms      26.697us      44.905ms        19.52%      44.905ms     224.524us           200     16370.172  \n",
      "                                              aten::bmm         5.35%      59.684ms         7.16%      79.923ms      16.651us       8.134ms         3.54%       8.134ms       1.695us          4800       835.191  \n",
      "                                              aten::add         5.87%      65.563ms         8.82%      98.425ms      12.300us       9.745ms         4.24%       9.745ms       1.218us          8002         7.334  \n",
      "                                              aten::mul         2.32%      25.866ms         3.44%      38.441ms      12.801us       3.243ms         1.41%       3.243ms       1.080us          3003         1.977  \n",
      "                                            aten::empty         4.82%      53.806ms         4.82%      53.806ms       2.853us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.940ms         6.26%      69.839ms      16.581us       0.000us         0.00%       1.482ms       0.352us          4212            --  \n",
      "                                         aten::_to_copy         0.76%       8.481ms         5.90%      65.899ms      23.493us       0.000us         0.00%       1.482ms       0.528us          2805            --  \n",
      "                                    aten::empty_strided         1.27%      14.140ms         1.27%      14.140ms       4.704us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.43%      15.930ms         4.86%      54.249ms      14.806us       2.392ms         1.04%       2.392ms       0.653us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.116s\n",
      "Self CUDA time total: 230.064ms\n",
      "\n",
      "output: tensor([[    2, 20470, 40224,    10, 21021,   414,  3184,   634,    80, 32201,\n",
      "            11, 31886,     4, 50118, 50118,   100,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414]], device='cuda:0')\n",
      "text_energy_per_token: [9.46018960715952, 9.178588360875427]\n",
      "output_tokens: 213\n",
      "flop: 53226967053\n",
      "energy_consumed:  1955.039320866466\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.37%     178.609ms        22.24%     242.615ms      16.848us     122.206ms        53.00%     122.206ms       8.486us         14400     37201.379  \n",
      "                                               aten::mm         0.26%       2.805ms         0.37%       4.053ms      20.267us      44.915ms        19.48%      44.915ms     224.575us           200     16910.696  \n",
      "                                              aten::bmm         5.15%      56.170ms         6.97%      76.064ms      15.847us       8.310ms         3.60%       8.310ms       1.731us          4800       895.058  \n",
      "                                              aten::add         5.66%      61.785ms         8.65%      94.371ms      11.793us       9.759ms         4.23%       9.759ms       1.220us          8002         7.702  \n",
      "                                              aten::mul         2.32%      25.340ms         3.46%      37.788ms      12.584us       3.246ms         1.41%       3.246ms       1.081us          3003         2.043  \n",
      "                                            aten::empty         5.01%      54.657ms         5.01%      54.657ms       2.898us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.822ms         6.17%      67.320ms      15.983us       0.000us         0.00%       1.503ms       0.357us          4212            --  \n",
      "                                         aten::_to_copy         0.74%       8.058ms         5.82%      63.498ms      22.637us       0.000us         0.00%       1.503ms       0.536us          2805            --  \n",
      "                                    aten::empty_strided         1.40%      15.263ms         1.40%      15.263ms       5.078us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.46%      15.883ms         4.80%      52.406ms      14.303us       2.418ms         1.05%       2.418ms       0.660us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.091s\n",
      "Self CUDA time total: 230.576ms\n",
      "\n",
      "output: tensor([[    2, 20470, 40224,    10,   586,     7,   465,     5,  1537,  4785,\n",
      "            11,    80, 42156,   396,   634,   143,  1823,   414,  6609,     4,\n",
      "         50118, 50118, 44758,    10,    92,  8932,    19,     5,   276,   766,\n",
      "             8,     5,   276,   766,     4, 50118, 50118, 44758,    10,    92,\n",
      "          8932,    19,     5,   276,   766,     8,     5,   276,   766,     4,\n",
      "         50118, 50118, 44758,    10,    92,  8932,    19,     5,   276,   766,\n",
      "             8,     5,   276,   766,     4, 50118, 50118, 44758,    10,    92,\n",
      "          8932,    19,     5,   276,   766,     8,     5,   276,   766,     4,\n",
      "         50118, 50118, 44758,    10,    92,  8932,    19,     5,   276,   766,\n",
      "             8,     5,   276,   766,     4, 50118, 50118, 44758,    10,    92,\n",
      "          8932,    19,     5,   276,   766,     8,     5,   276,   766,     4,\n",
      "         50118, 50118, 44758,    10,    92,  8932,    19,     5,   276,   766,\n",
      "             8,     5,   276,   766,     4, 50118, 50118, 44758,    10,    92,\n",
      "          8932,    19,     5,   276,   766,     8,     5,   276,   766,     4,\n",
      "         50118, 50118, 44758,    10,    92,  8932,    19,     5,   276,   766,\n",
      "             8,     5,   276,   766,     4, 50118, 50118, 44758,    10,    92,\n",
      "          8932,    19,     5,   276,   766,     8,     5,   276,   766,     4,\n",
      "         50118, 50118, 44758,    10,    92,  8932,    19,     5,   276,   766,\n",
      "             8,     5,   276,   766,     4, 50118, 50118, 44758,    10,    92,\n",
      "          8932,    19,     5,   276,   766,     8,     5,   276,   766,     4,\n",
      "         50118, 50118, 44758,    10,    92,  8932,    19,     5,   276,   766,\n",
      "             8,     5,   276,   766,     4, 50118, 50118, 44758,    10,    92]],\n",
      "       device='cuda:0')\n",
      "text_energy_per_token: [9.653923306991512]\n",
      "output_tokens: 220\n",
      "flop: 55016878120\n",
      "energy_consumed:  2123.8631275381326\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.92%     180.196ms        21.73%     245.895ms      17.076us     122.171ms        52.98%     122.171ms       8.484us         14400     37201.379  \n",
      "                                               aten::mm         0.25%       2.821ms         0.36%       4.062ms      20.312us      44.946ms        19.49%      44.946ms     224.729us           200     16910.696  \n",
      "                                              aten::bmm         5.06%      57.260ms         6.86%      77.608ms      16.168us       8.309ms         3.60%       8.309ms       1.731us          4800       895.058  \n",
      "                                              aten::add         5.60%      63.383ms         8.51%      96.301ms      12.035us       9.764ms         4.23%       9.764ms       1.220us          8002         7.702  \n",
      "                                              aten::mul         2.29%      25.967ms         3.41%      38.560ms      12.840us       3.247ms         1.41%       3.247ms       1.081us          3003         2.043  \n",
      "                                            aten::empty         5.16%      58.385ms         5.16%      58.385ms       3.096us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.36%       4.046ms         6.24%      70.625ms      16.767us       0.000us         0.00%       1.504ms       0.357us          4212            --  \n",
      "                                         aten::_to_copy         0.76%       8.656ms         5.88%      66.579ms      23.736us       0.000us         0.00%       1.504ms       0.536us          2805            --  \n",
      "                                    aten::empty_strided         1.41%      15.957ms         1.41%      15.957ms       5.308us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.42%      16.063ms         4.80%      54.323ms      14.826us       2.422ms         1.05%       2.422ms       0.661us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.132s\n",
      "Self CUDA time total: 230.602ms\n",
      "\n",
      "output: tensor([[    2, 20470, 40224,    10,   586,     7,   465,     5,  1537,  4785,\n",
      "            11,    80, 42156,   396,   634,   143,  1823,   414,  6609,     4,\n",
      "         50118, 50118, 44758,    10,    92,  8932,    19,     5,   276,   766,\n",
      "             8,     5,   276,   766,     4, 50118, 50118, 44758,    10,    92,\n",
      "          8932,    19,     5,   276,   766,     8,     5,   276,   766,     4,\n",
      "         50118, 50118, 44758,    10,    92,  8932,    19,     5,   276,   766,\n",
      "             8,     5,   276,   766,     4, 50118, 50118, 44758,    10,    92,\n",
      "          8932,    19,     5,   276,   766,     8,     5,   276,   766,     4,\n",
      "         50118, 50118, 44758,    10,    92,  8932,    19,     5,   276,   766,\n",
      "             8,     5,   276,   766,     4, 50118, 50118, 44758,    10,    92,\n",
      "          8932,    19,     5,   276,   766,     8,     5,   276,   766,     4,\n",
      "         50118, 50118, 44758,    10,    92,  8932,    19,     5,   276,   766,\n",
      "             8,     5,   276,   766,     4, 50118, 50118, 44758,    10,    92,\n",
      "          8932,    19,     5,   276,   766,     8,     5,   276,   766,     4,\n",
      "         50118, 50118, 44758,    10,    92,  8932,    19,     5,   276,   766,\n",
      "             8,     5,   276,   766,     4, 50118, 50118, 44758,    10,    92,\n",
      "          8932,    19,     5,   276,   766,     8,     5,   276,   766,     4,\n",
      "         50118, 50118, 44758,    10,    92,  8932,    19,     5,   276,   766,\n",
      "             8,     5,   276,   766,     4, 50118, 50118, 44758,    10,    92,\n",
      "          8932,    19,     5,   276,   766,     8,     5,   276,   766,     4,\n",
      "         50118, 50118, 44758,    10,    92,  8932,    19,     5,   276,   766,\n",
      "             8,     5,   276,   766,     4, 50118, 50118, 44758,    10,    92]],\n",
      "       device='cuda:0')\n",
      "text_energy_per_token: [9.653923306991512, 9.578274821559516]\n",
      "output_tokens: 220\n",
      "flop: 55016878120\n",
      "energy_consumed:  2107.2204607430936\n",
      "Processing category: math\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.13%     182.327ms        21.84%     246.821ms      17.140us     122.385ms        53.01%     122.385ms       8.499us         14400     38220.595  \n",
      "                                               aten::mm         0.25%       2.812ms         0.36%       4.079ms      20.394us      44.897ms        19.45%      44.897ms     224.486us           200     17374.003  \n",
      "                                              aten::bmm         5.07%      57.306ms         6.87%      77.690ms      16.185us       8.354ms         3.62%       8.354ms       1.740us          4800       949.248  \n",
      "                                              aten::add         5.59%      63.217ms         8.49%      95.986ms      11.995us       9.765ms         4.23%       9.765ms       1.220us          8002         8.029  \n",
      "                                              aten::mul         2.29%      25.885ms         3.40%      38.419ms      12.794us       3.245ms         1.41%       3.245ms       1.080us          3003         2.099  \n",
      "                                            aten::empty         5.17%      58.463ms         5.17%      58.463ms       3.100us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.954ms         6.19%      70.015ms      16.623us       0.000us         0.00%       1.500ms       0.356us          4212            --  \n",
      "                                         aten::_to_copy         0.76%       8.575ms         5.85%      66.061ms      23.551us       0.000us         0.00%       1.500ms       0.535us          2805            --  \n",
      "                                    aten::empty_strided         1.29%      14.524ms         1.29%      14.524ms       4.832us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.42%      16.015ms         4.78%      54.054ms      14.753us       2.416ms         1.05%       2.416ms       0.659us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.130s\n",
      "Self CUDA time total: 230.870ms\n",
      "\n",
      "output: tensor([[    2, 18377,    14,   856,  1640,  1178,    43,  5457,   195,  1178,\n",
      "         35227,   246,   111,   132,  1178,  2055,   155,     6,   465,     5,\n",
      "           923,     9,   856,  1640,   176,   322, 50118, 50118,  1640,  1178,\n",
      "            43,  5457,   195,  1178, 35227,   246,  2055,   132,  1178,  2055,\n",
      "           155, 50118, 50118,  1640,  1178,    43,  5457,   195,  1178, 35227,\n",
      "           246,  2055,   132,  1178,  2055,   155, 50118, 50118,  1640,  1178,\n",
      "            43,  5457,   195,  1178, 35227,   246,  2055,   132,  1178,  2055,\n",
      "           155, 50118, 50118,  1640,  1178,    43,  5457,   195,  1178, 35227,\n",
      "           246,  2055,   132,  1178,  2055,   155, 50118, 50118,  1640,  1178,\n",
      "            43,  5457,   195,  1178, 35227,   246,  2055,   132,  1178,  2055,\n",
      "           155, 50118, 50118,  1640,  1178,    43,  5457,   195,  1178, 35227,\n",
      "           246,  2055,   132,  1178,  2055,   155, 50118, 50118,  1640,  1178,\n",
      "            43,  5457,   195,  1178, 35227,   246,  2055,   132,  1178,  2055,\n",
      "           155, 50118, 50118,  1640,  1178,    43,  5457,   195,  1178, 35227,\n",
      "           246,  2055,   132,  1178,  2055,   155, 50118, 50118,  1640,  1178,\n",
      "            43,  5457,   195,  1178, 35227,   246,  2055,   132,  1178,  2055,\n",
      "           155, 50118, 50118,  1640,  1178,    43,  5457,   195,  1178, 35227,\n",
      "           246,  2055,   132,  1178,  2055,   155, 50118, 50118,  1640,  1178,\n",
      "            43,  5457,   195,  1178, 35227,   246,  2055,   132,  1178,  2055,\n",
      "           155, 50118, 50118,  1640,  1178,    43,  5457,   195,  1178, 35227,\n",
      "           246,  2055,   132,  1178,  2055,   155, 50118, 50118,  1640,  1178,\n",
      "            43,  5457,   195,  1178, 35227,   246,  2055,   132,  1178,  2055,\n",
      "           155, 50118, 50118,  1640,  1178,    43]], device='cuda:0')\n",
      "text_energy_per_token: [9.36969823276469]\n",
      "output_tokens: 226\n",
      "flop: 56553974230\n",
      "energy_consumed:  2117.55180060482\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.10%     177.006ms        21.97%     241.566ms      16.775us     122.239ms        52.97%     122.239ms       8.489us         14400     38220.595  \n",
      "                                               aten::mm         0.26%       2.807ms         0.37%       4.032ms      20.159us      44.923ms        19.47%      44.923ms     224.613us           200     17374.003  \n",
      "                                              aten::bmm         5.13%      56.376ms         6.95%      76.398ms      15.916us       8.353ms         3.62%       8.353ms       1.740us          4800       949.248  \n",
      "                                              aten::add         5.66%      62.248ms         8.63%      94.862ms      11.855us       9.774ms         4.24%       9.774ms       1.221us          8002         8.029  \n",
      "                                              aten::mul         2.32%      25.498ms         3.44%      37.774ms      12.579us       3.247ms         1.41%       3.247ms       1.081us          3003         2.099  \n",
      "                                            aten::empty         5.22%      57.388ms         5.22%      57.388ms       3.043us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.837ms         6.15%      67.630ms      16.057us       0.000us         0.00%       1.501ms       0.356us          4212            --  \n",
      "                                         aten::_to_copy         0.75%       8.222ms         5.80%      63.793ms      22.743us       0.000us         0.00%       1.501ms       0.535us          2805            --  \n",
      "                                    aten::empty_strided         1.40%      15.424ms         1.40%      15.424ms       5.131us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.44%      15.871ms         4.76%      52.385ms      14.297us       2.418ms         1.05%       2.418ms       0.660us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.100s\n",
      "Self CUDA time total: 230.774ms\n",
      "\n",
      "output: tensor([[    2, 18377,    14,   856,  1640,  1178,    43,  5457,   195,  1178,\n",
      "         35227,   246,   111,   132,  1178,  2055,   155,     6,   465,     5,\n",
      "           923,     9,   856,  1640,   176,   322, 50118, 50118,  1640,  1178,\n",
      "            43,  5457,   195,  1178, 35227,   246,  2055,   132,  1178,  2055,\n",
      "           155, 50118, 50118,  1640,  1178,    43,  5457,   195,  1178, 35227,\n",
      "           246,  2055,   132,  1178,  2055,   155, 50118, 50118,  1640,  1178,\n",
      "            43,  5457,   195,  1178, 35227,   246,  2055,   132,  1178,  2055,\n",
      "           155, 50118, 50118,  1640,  1178,    43,  5457,   195,  1178, 35227,\n",
      "           246,  2055,   132,  1178,  2055,   155, 50118, 50118,  1640,  1178,\n",
      "            43,  5457,   195,  1178, 35227,   246,  2055,   132,  1178,  2055,\n",
      "           155, 50118, 50118,  1640,  1178,    43,  5457,   195,  1178, 35227,\n",
      "           246,  2055,   132,  1178,  2055,   155, 50118, 50118,  1640,  1178,\n",
      "            43,  5457,   195,  1178, 35227,   246,  2055,   132,  1178,  2055,\n",
      "           155, 50118, 50118,  1640,  1178,    43,  5457,   195,  1178, 35227,\n",
      "           246,  2055,   132,  1178,  2055,   155, 50118, 50118,  1640,  1178,\n",
      "            43,  5457,   195,  1178, 35227,   246,  2055,   132,  1178,  2055,\n",
      "           155, 50118, 50118,  1640,  1178,    43,  5457,   195,  1178, 35227,\n",
      "           246,  2055,   132,  1178,  2055,   155, 50118, 50118,  1640,  1178,\n",
      "            43,  5457,   195,  1178, 35227,   246,  2055,   132,  1178,  2055,\n",
      "           155, 50118, 50118,  1640,  1178,    43,  5457,   195,  1178, 35227,\n",
      "           246,  2055,   132,  1178,  2055,   155, 50118, 50118,  1640,  1178,\n",
      "            43,  5457,   195,  1178, 35227,   246,  2055,   132,  1178,  2055,\n",
      "           155, 50118, 50118,  1640,  1178,    43]], device='cuda:0')\n",
      "text_energy_per_token: [9.36969823276469, 9.175488579528205]\n",
      "output_tokens: 226\n",
      "flop: 56553974230\n",
      "energy_consumed:  2073.6604189733744\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.07%     180.282ms        21.88%     245.455ms      17.046us     122.203ms        53.00%     122.203ms       8.486us         14400     37031.510  \n",
      "                                               aten::mm         0.25%       2.821ms         0.36%       4.046ms      20.228us      44.919ms        19.48%      44.919ms     224.595us           200     16833.479  \n",
      "                                              aten::bmm         5.09%      57.091ms         6.89%      77.327ms      16.110us       8.303ms         3.60%       8.303ms       1.730us          4800       886.284  \n",
      "                                              aten::add         5.61%      62.917ms         8.53%      95.693ms      11.959us       9.761ms         4.23%       9.761ms       1.220us          8002         7.648  \n",
      "                                              aten::mul         2.29%      25.713ms         3.41%      38.244ms      12.735us       3.246ms         1.41%       3.246ms       1.081us          3003         2.033  \n",
      "                                            aten::empty         5.17%      58.001ms         5.17%      58.001ms       3.076us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.919ms         6.21%      69.643ms      16.534us       0.000us         0.00%       1.497ms       0.355us          4212            --  \n",
      "                                         aten::_to_copy         0.76%       8.516ms         5.86%      65.723ms      23.431us       0.000us         0.00%       1.497ms       0.534us          2805            --  \n",
      "                                    aten::empty_strided         1.38%      15.511ms         1.38%      15.511ms       5.160us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.42%      15.952ms         4.81%      53.972ms      14.730us       2.413ms         1.05%       2.413ms       0.659us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.122s\n",
      "Self CUDA time total: 230.561ms\n",
      "\n",
      "output: tensor([[    2,   104, 18224,    13,  3023,    11,     5, 19587,   155,  1178,\n",
      "          2055,   158,  5457,   195,  1640,  1178,   111,   132,   322, 50118,\n",
      "            12,   246, 50118,   104, 18224,   111,   176,  3226,   119,  5457,\n",
      "           111,   176,  3226,   119,  2055,   204,  3226,   329,   111,   290,\n",
      "             6,   111,   306,  3226,   119,  2055,   204,  3226,   329,  5457,\n",
      "           111,   398,    13,   475,     4, 50118,    12,   176, 50118,   104,\n",
      "         18224,   111,   176,  3226,   642,  2055,   204,  3226,   642,  5457,\n",
      "           111,   306,  3226,   338,  2055,   290,     6,   111,   306,  3226,\n",
      "           642,  5457,   111,   306,  3226,   338,   111,   290,    13,   910,\n",
      "             4, 50118,    12,   176, 50118,   104, 18224,   111,   176,  3226,\n",
      "           282,  2055,   204,  3226,   329,  5457,   111,   288,  3226,   282,\n",
      "          2055,   204,     6,   111,   306,  3226,   282,  2055,   204,  3226,\n",
      "           329,  5457,   111,   306,    13,   295,     4, 50118,   288, 50118,\n",
      "           104, 18224,   111,   176,  3226,   257,  2055,   204,  3226,   257,\n",
      "          5457,   111,   306,  3226,    90,   111,   290,     6,   111,   306,\n",
      "          3226,    90,  5457,   111,   306,  3226,   257,   111,   290,    13,\n",
      "           326,     4, 50118,    12,   176, 50118,   104, 18224,   111,   176,\n",
      "          3226,   282,  2055,   155,  3226,   282,  5457,   111,   245,  3226,\n",
      "           139,   111,   158,     6,   111,   245,  3226,   282,  5457,   111,\n",
      "           245,  3226,   139,   111,   379,    13,   295,     4, 50118,    12,\n",
      "           245, 50118,   104, 18224,   111,   176,  3226,   257,  2055,   155,\n",
      "          3226,  1178,  5457,   111,   288,  3226,  1178,   111,   195]],\n",
      "       device='cuda:0')\n",
      "text_energy_per_token: [9.354718925371236]\n",
      "output_tokens: 219\n",
      "flop: 54760954491\n",
      "energy_consumed:  2048.6834446563007\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.17%     180.682ms        21.99%     245.787ms      17.069us     122.214ms        53.02%     122.214ms       8.487us         14400     37031.510  \n",
      "                                               aten::mm         0.25%       2.822ms         0.36%       4.046ms      20.228us      44.893ms        19.47%      44.893ms     224.467us           200     16833.479  \n",
      "                                              aten::bmm         5.11%      57.099ms         6.92%      77.291ms      16.102us       8.315ms         3.61%       8.315ms       1.732us          4800       886.284  \n",
      "                                              aten::add         5.63%      62.920ms         8.56%      95.695ms      11.959us       9.756ms         4.23%       9.756ms       1.219us          8002         7.648  \n",
      "                                              aten::mul         2.30%      25.735ms         3.43%      38.331ms      12.764us       3.244ms         1.41%       3.244ms       1.080us          3003         2.033  \n",
      "                                            aten::empty         5.07%      56.659ms         5.07%      56.659ms       3.005us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.949ms         6.05%      67.565ms      16.041us       0.000us         0.00%       1.492ms       0.354us          4212            --  \n",
      "                                         aten::_to_copy         0.76%       8.498ms         5.69%      63.615ms      22.679us       0.000us         0.00%       1.492ms       0.532us          2805            --  \n",
      "                                    aten::empty_strided         1.21%      13.543ms         1.21%      13.543ms       4.505us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.43%      15.946ms         4.82%      53.839ms      14.694us       2.407ms         1.04%       2.407ms       0.657us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.118s\n",
      "Self CUDA time total: 230.527ms\n",
      "\n",
      "output: tensor([[    2,   104, 18224,    13,  3023,    11,     5, 19587,   155,  1178,\n",
      "          2055,   158,  5457,   195,  1640,  1178,   111,   132,   322, 50118,\n",
      "            12,   246, 50118,   104, 18224,   111,   176,  3226,   119,  5457,\n",
      "           111,   176,  3226,   119,  2055,   204,  3226,   329,   111,   290,\n",
      "             6,   111,   306,  3226,   119,  2055,   204,  3226,   329,  5457,\n",
      "           111,   398,    13,   475,     4, 50118,    12,   176, 50118,   104,\n",
      "         18224,   111,   176,  3226,   642,  2055,   204,  3226,   642,  5457,\n",
      "           111,   306,  3226,   338,  2055,   290,     6,   111,   306,  3226,\n",
      "           642,  5457,   111,   306,  3226,   338,   111,   290,    13,   910,\n",
      "             4, 50118,    12,   176, 50118,   104, 18224,   111,   176,  3226,\n",
      "           282,  2055,   204,  3226,   329,  5457,   111,   288,  3226,   282,\n",
      "          2055,   204,     6,   111,   306,  3226,   282,  2055,   204,  3226,\n",
      "           329,  5457,   111,   306,    13,   295,     4, 50118,   288, 50118,\n",
      "           104, 18224,   111,   176,  3226,   257,  2055,   204,  3226,   257,\n",
      "          5457,   111,   306,  3226,    90,   111,   290,     6,   111,   306,\n",
      "          3226,    90,  5457,   111,   306,  3226,   257,   111,   290,    13,\n",
      "           326,     4, 50118,    12,   176, 50118,   104, 18224,   111,   176,\n",
      "          3226,   282,  2055,   155,  3226,   282,  5457,   111,   245,  3226,\n",
      "           139,   111,   158,     6,   111,   245,  3226,   282,  5457,   111,\n",
      "           245,  3226,   139,   111,   379,    13,   295,     4, 50118,    12,\n",
      "           245, 50118,   104, 18224,   111,   176,  3226,   257,  2055,   155,\n",
      "          3226,  1178,  5457,   111,   288,  3226,  1178,   111,   195]],\n",
      "       device='cuda:0')\n",
      "text_energy_per_token: [9.354718925371236, 9.28511736420246]\n",
      "output_tokens: 219\n",
      "flop: 54760954491\n",
      "energy_consumed:  2033.4407027603388\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.78%     172.813ms        21.51%     235.656ms      16.365us     122.359ms        52.98%     122.359ms       8.497us         14400     38900.072  \n",
      "                                               aten::mm         0.38%       4.181ms         0.49%       5.409ms      27.046us      44.902ms        19.44%      44.902ms     224.509us           200     17682.874  \n",
      "                                              aten::bmm         5.40%      59.097ms         7.23%      79.200ms      16.500us       8.405ms         3.64%       8.405ms       1.751us          4800       986.849  \n",
      "                                              aten::add         6.04%      66.208ms         9.02%      98.764ms      12.342us       9.766ms         4.23%       9.766ms       1.220us          8002         8.252  \n",
      "                                              aten::mul         2.32%      25.374ms         3.44%      37.729ms      12.564us       3.245ms         1.41%       3.245ms       1.081us          3003         2.137  \n",
      "                                            aten::empty         4.84%      52.973ms         4.84%      52.973ms       2.809us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.798ms         6.14%      67.284ms      15.974us       0.000us         0.00%       1.507ms       0.358us          4212            --  \n",
      "                                         aten::_to_copy         0.75%       8.201ms         5.80%      63.486ms      22.633us       0.000us         0.00%       1.507ms       0.537us          2805            --  \n",
      "                                    aten::empty_strided         1.28%      14.002ms         1.28%      14.002ms       4.658us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.45%      15.923ms         4.76%      52.149ms      14.233us       2.422ms         1.05%       2.422ms       0.661us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.095s\n",
      "Self CUDA time total: 230.963ms\n",
      "\n",
      "output: tensor([[    2,  1106,     5,   253, 21996,     9,    10,   516,  2835,    32,\n",
      "            36,   176,     6,   111,   176,    43,     8,    36,   698,     6,\n",
      "           204,   238,    99,    16,     5,  5933,     9,     5,  2835,   116,\n",
      "         50118, 50118,   133,  5933,     9,     5,  2835,    16,     5,  5933,\n",
      "             9,     5,  2835,     4, 50118, 50118,   133,  5933,     9,     5,\n",
      "          2835,    16,     5,  5933,     9,     5,  2835,     4, 50118, 50118,\n",
      "           133,  5933,     9,     5,  2835,    16,     5,  5933,     9,     5,\n",
      "          2835,     4, 50118, 50118,   133,  5933,     9,     5,  2835,    16,\n",
      "             5,  5933,     9,     5,  2835,     4, 50118, 50118,   133,  5933,\n",
      "             9,     5,  2835,    16,     5,  5933,     9,     5,  2835,     4,\n",
      "         50118, 50118,   133,  5933,     9,     5,  2835,    16,     5,  5933,\n",
      "             9,     5,  2835,     4, 50118, 50118,   133,  5933,     9,     5,\n",
      "          2835,    16,     5,  5933,     9,     5,  2835,     4, 50118, 50118,\n",
      "           133,  5933,     9,     5,  2835,    16,     5,  5933,     9,     5,\n",
      "          2835,     4, 50118, 50118,   133,  5933,     9,     5,  2835,    16,\n",
      "             5,  5933,     9,     5,  2835,     4, 50118, 50118,   133,  5933,\n",
      "             9,     5,  2835,    16,     5,  5933,     9,     5,  2835,     4,\n",
      "         50118, 50118,   133,  5933,     9,     5,  2835,    16,     5,  5933,\n",
      "             9,     5,  2835,     4, 50118, 50118,   133,  5933,     9,     5,\n",
      "          2835,    16,     5,  5933,     9,     5,  2835,     4, 50118, 50118,\n",
      "           133,  5933,     9,     5,  2835,    16,     5,  5933,     9,     5,\n",
      "          2835,     4, 50118, 50118,   133,  5933,     9,     5,  2835,    16,\n",
      "             5,  5933,     9,     5,  2835,     4, 50118, 50118,   133,  5933]],\n",
      "       device='cuda:0')\n",
      "text_energy_per_token: [8.857062986285788]\n",
      "output_tokens: 230\n",
      "flop: 57580185290\n",
      "energy_consumed:  2037.1244868457313\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.05%     183.740ms        21.83%     249.878ms      17.353us     122.221ms        52.95%     122.221ms       8.488us         14400     38900.072  \n",
      "                                               aten::mm         0.25%       2.845ms         0.36%       4.078ms      20.388us      44.927ms        19.46%      44.927ms     224.636us           200     17682.874  \n",
      "                                              aten::bmm         5.03%      57.606ms         6.82%      78.043ms      16.259us       8.398ms         3.64%       8.398ms       1.750us          4800       986.849  \n",
      "                                              aten::add         5.56%      63.601ms         8.43%      96.522ms      12.062us       9.770ms         4.23%       9.770ms       1.221us          8002         8.252  \n",
      "                                              aten::mul         2.27%      25.974ms         3.36%      38.493ms      12.818us       3.247ms         1.41%       3.247ms       1.081us          3003         2.137  \n",
      "                                            aten::empty         5.11%      58.538ms         5.11%      58.538ms       3.104us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       4.052ms         6.14%      70.311ms      16.693us       0.000us         0.00%       1.499ms       0.356us          4212            --  \n",
      "                                         aten::_to_copy         0.78%       8.880ms         5.79%      66.259ms      23.622us       0.000us         0.00%       1.499ms       0.534us          2805            --  \n",
      "                                    aten::empty_strided         1.37%      15.664ms         1.37%      15.664ms       5.211us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.41%      16.097ms         4.73%      54.112ms      14.769us       2.416ms         1.05%       2.416ms       0.659us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.145s\n",
      "Self CUDA time total: 230.816ms\n",
      "\n",
      "output: tensor([[    2,  1106,     5,   253, 21996,     9,    10,   516,  2835,    32,\n",
      "            36,   176,     6,   111,   176,    43,     8,    36,   698,     6,\n",
      "           204,   238,    99,    16,     5,  5933,     9,     5,  2835,   116,\n",
      "         50118, 50118,   133,  5933,     9,     5,  2835,    16,     5,  5933,\n",
      "             9,     5,  2835,     4, 50118, 50118,   133,  5933,     9,     5,\n",
      "          2835,    16,     5,  5933,     9,     5,  2835,     4, 50118, 50118,\n",
      "           133,  5933,     9,     5,  2835,    16,     5,  5933,     9,     5,\n",
      "          2835,     4, 50118, 50118,   133,  5933,     9,     5,  2835,    16,\n",
      "             5,  5933,     9,     5,  2835,     4, 50118, 50118,   133,  5933,\n",
      "             9,     5,  2835,    16,     5,  5933,     9,     5,  2835,     4,\n",
      "         50118, 50118,   133,  5933,     9,     5,  2835,    16,     5,  5933,\n",
      "             9,     5,  2835,     4, 50118, 50118,   133,  5933,     9,     5,\n",
      "          2835,    16,     5,  5933,     9,     5,  2835,     4, 50118, 50118,\n",
      "           133,  5933,     9,     5,  2835,    16,     5,  5933,     9,     5,\n",
      "          2835,     4, 50118, 50118,   133,  5933,     9,     5,  2835,    16,\n",
      "             5,  5933,     9,     5,  2835,     4, 50118, 50118,   133,  5933,\n",
      "             9,     5,  2835,    16,     5,  5933,     9,     5,  2835,     4,\n",
      "         50118, 50118,   133,  5933,     9,     5,  2835,    16,     5,  5933,\n",
      "             9,     5,  2835,     4, 50118, 50118,   133,  5933,     9,     5,\n",
      "          2835,    16,     5,  5933,     9,     5,  2835,     4, 50118, 50118,\n",
      "           133,  5933,     9,     5,  2835,    16,     5,  5933,     9,     5,\n",
      "          2835,     4, 50118, 50118,   133,  5933,     9,     5,  2835,    16,\n",
      "             5,  5933,     9,     5,  2835,     4, 50118, 50118,   133,  5933]],\n",
      "       device='cuda:0')\n",
      "text_energy_per_token: [8.857062986285788, 9.533727485721629]\n",
      "output_tokens: 230\n",
      "flop: 57580185290\n",
      "energy_consumed:  2192.7573217159747\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch\n",
    "import pynvml\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.profiler import profile, ProfilerActivity\n",
    "\n",
    "# Specify the GPU device you want to use\n",
    "device = \"cuda:0\"  # Change this to your preferred GPU\n",
    "\n",
    "# Initialize NVML for power measurement\n",
    "def initialize_nvml():\n",
    "    pynvml.nvmlInit()\n",
    "\n",
    "def shutdown_nvml():\n",
    "    pynvml.nvmlShutdown()\n",
    "\n",
    "def get_gpu_handle(gpu_index=0):\n",
    "    return pynvml.nvmlDeviceGetHandleByIndex(gpu_index)\n",
    "\n",
    "# Measure GPU power consumption over a period of time\n",
    "def measure_power_consumption(handle, duration_sec=1.0, interval_sec=0.1):\n",
    "    power_readings = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while (time.time() - start_time) < duration_sec:\n",
    "        power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0  # Convert from mW to W\n",
    "        power_readings.append(power)\n",
    "        time.sleep(interval_sec)\n",
    "    \n",
    "    return sum(power_readings) / len(power_readings) if power_readings else 0\n",
    "\n",
    "# Measure energy consumed during inference and FLOPs\n",
    "def measure_energy_during_inference(handle, inference_function, model, inputs, max_new_tokens=200):\n",
    "    power_start = measure_power_consumption(handle, duration_sec=0.5)\n",
    "    \n",
    "    # Start time for inference\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Measure FLOPs using PyTorch profiler\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], with_flops=True) as prof:\n",
    "        with torch.no_grad():\n",
    "            result = inference_function(inputs['input_ids'], max_new_tokens=max_new_tokens)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    power_end = measure_power_consumption(handle, duration_sec=0.5)\n",
    "    \n",
    "    avg_power = (power_start + power_end) / 2\n",
    "    elapsed_time = end_time - start_time\n",
    "    energy_consumed = avg_power * elapsed_time\n",
    "    print(\"prof keys flops table\")\n",
    "    print(prof.key_averages().table(sort_by=\"flops\", row_limit=10)) \n",
    "    # Calculate FLOPs\n",
    "    flops = sum([event.flops for event in prof.key_averages() if event.flops is not None])\n",
    "\n",
    "    return energy_consumed, elapsed_time, flops, result\n",
    "\n",
    "# Measure energy consumed during inference and FLOPs\n",
    "def NOTWORKING_measure_energy_during_inferenceWRONG_NOTWORKING(handle, inference_function, model, inputs, max_new_tokens=200):\n",
    "    # Measure initial power consumption\n",
    "    power_start = measure_power_consumption(handle, duration_sec=0.2)\n",
    "\n",
    "    # Start time for inference\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Measure FLOPs using PyTorch profiler\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], with_flops=True) as prof:\n",
    "        with torch.no_grad():\n",
    "            result = inference_function(model, inputs['input_ids'], max_new_tokens=max_new_tokens)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    # Measure final power consumption\n",
    "    power_end = measure_power_consumption(handle, duration_sec=0.2)\n",
    "\n",
    "    # Calculate average power and elapsed time\n",
    "    avg_power = (power_start + power_end) / 2\n",
    "    elapsed_time = end_time - start_time\n",
    "    energy_consumed = avg_power * elapsed_time\n",
    "    print(\"prof keys flops table\")\n",
    "    print(prof.key_averages().table(sort_by=\"flops\", row_limit=10))\n",
    "    # Calculate FLOPs\n",
    "    flops = sum(event.flops for event in prof.key_averages() if event.flops is not None)\n",
    "\n",
    "    return energy_consumed, elapsed_time, flops, result\n",
    "\n",
    "\n",
    "\n",
    "# Calculate perplexity for generated text\n",
    "def calculate_perplexity(model, input_text, tokenizer):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)  # Ensure input is on the same device\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "        perplexity = torch.exp(loss)\n",
    "    return perplexity.item()\n",
    "\n",
    "# Run the experiment for a list of texts\n",
    "def run_experiment_for_texts(texts, bootstrapping, handle, model, tokenizer):\n",
    "    latencies = []\n",
    "    energy_per_token = []\n",
    "    energy_per_flops = []\n",
    "    energy_per_task = []\n",
    "    throughputs = []\n",
    "    generated_texts = []\n",
    "    perplexities = []\n",
    "\n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\").to(device)  # Ensure input is on the same device\n",
    "        text_latencies = []\n",
    "        text_energy_per_token = []\n",
    "        text_energy_per_flops = []\n",
    "        text_energy_per_task = []\n",
    "        text_throughput = []\n",
    "        text_generated = []\n",
    "        text_perplexities = []\n",
    "\n",
    "        for _ in range(bootstrapping):\n",
    "            energy_consumed, latency, flops, output = measure_energy_during_inference(\n",
    "                handle, model.generate, model, inputs, max_new_tokens=200\n",
    "            )\n",
    "            text_latencies.append(latency)\n",
    "\n",
    "            print(\"output:\", output)\n",
    "            output_tokens = output.size(-1)\n",
    "            energy_token = energy_consumed / output_tokens if output_tokens > 0 else 0\n",
    "            text_energy_per_token.append(energy_token)\n",
    "\n",
    "            # Energy per FLOPs calculation\n",
    "\n",
    "            print(\"text_energy_per_token:\", text_energy_per_token)\n",
    "            print(\"output_tokens:\", output_tokens)\n",
    "            print(\"flop:\", flops)\n",
    "            print(\"energy_consumed: \",energy_consumed)\n",
    "            energy_flop = energy_consumed / flops #if flops > 0 else 0\n",
    "            text_energy_per_flops.append(energy_flop)\n",
    "\n",
    "            # Energy per task (full inference energy)\n",
    "            text_energy_per_task.append(energy_consumed)\n",
    "\n",
    "            throughput = output_tokens / latency\n",
    "            text_throughput.append(throughput)\n",
    "\n",
    "            perplexity = calculate_perplexity(model, text, tokenizer)\n",
    "            text_perplexities.append(perplexity)\n",
    "\n",
    "            generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            filtered_generated_text = generated_text.replace(text, \"\").strip()\n",
    "            text_generated.append(filtered_generated_text)\n",
    "\n",
    "        latencies.append(text_latencies)\n",
    "        energy_per_token.append(text_energy_per_token)\n",
    "        energy_per_flops.append(text_energy_per_flops)\n",
    "        energy_per_task.append(text_energy_per_task)\n",
    "        throughputs.append(text_throughput)\n",
    "        generated_texts.append(text_generated)\n",
    "        perplexities.append(text_perplexities)\n",
    "\n",
    "    return latencies, energy_per_token, energy_per_flops, energy_per_task, throughputs, generated_texts, perplexities\n",
    "\n",
    "# Collect metrics for each category\n",
    "def collect_metrics_for_categories(df, categories, bootstrapping, model, tokenizer):\n",
    "    category_metrics = {}\n",
    "    handle = get_gpu_handle(gpu_index=0)\n",
    "\n",
    "    for category in categories:\n",
    "        print(f\"Processing category: {category}\")\n",
    "        texts = filter_texts_by_category(df, category)\n",
    "        latencies, energy_per_token, energy_per_flops, energy_per_task, throughputs, generated_texts, perplexities = run_experiment_for_texts(\n",
    "            texts, bootstrapping, handle, model, tokenizer\n",
    "        )\n",
    "\n",
    "        category_metrics[category] = {\n",
    "            \"latencies\": latencies,\n",
    "            \"energy_per_token\": energy_per_token,\n",
    "            \"energy_per_flops\": energy_per_flops,\n",
    "            \"energy_per_task\": energy_per_task,\n",
    "            \"throughput\": throughputs,\n",
    "            \"generated_texts\": generated_texts,\n",
    "            \"perplexities\": perplexities\n",
    "        }\n",
    "\n",
    "    shutdown_nvml()  \n",
    "    return category_metrics\n",
    "\n",
    "def filter_texts_by_category(df, category):\n",
    "    return df[df['category'] == category]['text'].values\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Example Usage\n",
    "file_path = \"./question.jsonl\"\n",
    "# bootstrapping = 2 \n",
    "df_mtconversation = load_dataset(file_path)\n",
    "\n",
    "#categories = [ 'common-sense']\n",
    "categories = ['knowledge', 'common-sense', 'coding', 'math']\n",
    "\n",
    "initialize_nvml()\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "metrics = collect_metrics_for_categories(df_mtconversation, categories, bootstrapping, model, tokenizer)\n",
    "\n",
    "# (Optionally, you can visualize the collected metrics here)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"flop_{model_name.replace('/','-').replace('.', '_')}_bootstrapping={bootstrapping}_metrics.json\", \"w\") as json_file:\n",
    "    json.dump(metrics, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x800 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to plot the metrics\n",
    "def plot_metrics(metrics, categories):\n",
    "    num_categories = len(categories)\n",
    "\n",
    "    # Prepare the data for plotting\n",
    "    avg_latencies = []\n",
    "    avg_perplexities = []\n",
    "    avg_energy_per_flops = []\n",
    "    avg_energy_per_token = []\n",
    "    avg_energy_per_task = []\n",
    "\n",
    "    for category in categories:\n",
    "        if category in metrics:\n",
    "            avg_latencies.append(np.mean(metrics[category][\"latencies\"]))\n",
    "            avg_perplexities.append(np.mean(metrics[category][\"perplexities\"]))\n",
    "            avg_energy_per_flops.append(np.mean(metrics[category][\"energy_per_flops\"]))\n",
    "            avg_energy_per_token.append(np.mean(metrics[category][\"energy_per_token\"]))\n",
    "            avg_energy_per_task.append(np.mean(metrics[category][\"energy_per_task\"]))\n",
    "        else:\n",
    "            avg_latencies.append(0)\n",
    "            avg_perplexities.append(0)\n",
    "            avg_energy_per_flops.append(0)\n",
    "            avg_energy_per_token.append(0)\n",
    "            avg_energy_per_task.append(0)\n",
    "\n",
    "    x = np.arange(num_categories)  # the label locations\n",
    "    width = 0.15  # the width of the bars\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "    # Plot latencies\n",
    "    bars1 = ax1.bar(x - 2*width, avg_latencies, width, label='Average Latency (s)', color='b')\n",
    "    ax1.set_ylabel('Average Latency (s)', color='b')\n",
    "    ax1.tick_params(axis='y', labelcolor='b')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(categories)\n",
    "\n",
    "    # Create a second y-axis for perplexities\n",
    "    ax2 = ax1.twinx()\n",
    "    bars2 = ax2.bar(x - width, avg_perplexities, width, label='Average Perplexity', color='g')\n",
    "    ax2.set_ylabel('Average Perplexity', color='g')\n",
    "    ax2.tick_params(axis='y', labelcolor='g')\n",
    "\n",
    "    # Create a third y-axis for energy per FLOPs\n",
    "    ax3 = ax1.twinx()\n",
    "    bars3 = ax3.bar(x, avg_energy_per_flops, width, label='Energy per FLOP (Joules)', color='r')\n",
    "    ax3.spines['right'].set_position(('outward', 60))  # move the third y-axis to the right\n",
    "    ax3.set_ylabel('Energy per FLOP (Joules)', color='r')\n",
    "    ax3.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "    # Create a fourth y-axis for energy per token\n",
    "    ax4 = ax1.twinx()\n",
    "    bars4 = ax4.bar(x + width, avg_energy_per_token, width, label='Energy per Token (Joules)', color='purple')\n",
    "    ax4.spines['right'].set_position(('outward', 120))  # move the fourth y-axis to the right\n",
    "    ax4.set_ylabel('Energy per Token (Joules)', color='purple')\n",
    "    ax4.tick_params(axis='y', labelcolor='purple')\n",
    "\n",
    "\n",
    "plot_metrics(metrics, categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# energy_per_flops with asynchronous energy measuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"facebook/opt-125m\"\n",
    "bootstrapping = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import threading\n",
    "import torch\n",
    "import pynvml\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.profiler import profile, ProfilerActivity\n",
    "\n",
    "# Specify the GPU device you want to use\n",
    "device = \"cuda:0\"  # Change this to your preferred GPU\n",
    "\n",
    "# Initialize NVML for power measurement\n",
    "def initialize_nvml():\n",
    "    pynvml.nvmlInit()\n",
    "\n",
    "def shutdown_nvml():\n",
    "    pynvml.nvmlShutdown()\n",
    "\n",
    "def get_gpu_handle(gpu_index=0):\n",
    "    return pynvml.nvmlDeviceGetHandleByIndex(gpu_index)\n",
    "\n",
    "def start_power_monitoring(handle, interval_sec=0.1):\n",
    "    power_readings = []\n",
    "    running = True\n",
    "\n",
    "    def monitor():\n",
    "        while running:\n",
    "            power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0  # Convert from mW to W\n",
    "            timestamp = time.time()\n",
    "            power_readings.append((timestamp, power))\n",
    "            time.sleep(interval_sec)\n",
    "\n",
    "    thread = threading.Thread(target=monitor)\n",
    "    thread.start()\n",
    "\n",
    "    def stop():\n",
    "        nonlocal running\n",
    "        running = False\n",
    "        thread.join()\n",
    "\n",
    "    return power_readings, stop\n",
    "\n",
    "\n",
    "# Measure energy consumed during inference and FLOPs\n",
    "def measure_energy_during_inference(handle, inference_function, model, inputs, max_new_tokens=200):\n",
    "    # Start power monitoring\n",
    "    power_readings, stop_monitoring = start_power_monitoring(handle, interval_sec=0.05)\n",
    "    \n",
    "    \n",
    "    # Start time for inference\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Measure FLOPs using PyTorch profiler\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], with_flops=True) as prof:\n",
    "        with torch.no_grad():\n",
    "            result = inference_function(inputs['input_ids'], max_new_tokens=max_new_tokens)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Stop power monitoring\n",
    "    stop_monitoring()\n",
    "\n",
    "    # Filter power readings during inference\n",
    "    power_during_inference = [p for t, p in power_readings if start_time <= t <= end_time]\n",
    "\n",
    "    \n",
    "    # Calculate average power and energy consumed\n",
    "    if power_during_inference:\n",
    "        avg_power = sum(power_during_inference) / len(power_during_inference)\n",
    "        elapsed_time = end_time - start_time\n",
    "        energy_consumed = avg_power * elapsed_time\n",
    "    else:\n",
    "        avg_power = 0\n",
    "        energy_consumed = 0\n",
    "        elapsed_time = end_time - start_time\n",
    "    print(\"prof keys flops table\")\n",
    "    print(prof.key_averages().table(sort_by=\"flops\", row_limit=10)) \n",
    "    # Calculate FLOPs\n",
    "    flops = sum([event.flops for event in prof.key_averages() if event.flops is not None])\n",
    "\n",
    "    return energy_consumed, elapsed_time, flops, result\n",
    "\n",
    "# Measure energy consumed during inference and FLOPs\n",
    "def NOTWORKING_measure_energy_during_inferenceWRONG_NOTWORKING(handle, inference_function, model, inputs, max_new_tokens=200):\n",
    "    # Measure initial power consumption\n",
    "    power_start = measure_power_consumption(handle, duration_sec=0.2)\n",
    "\n",
    "    # Start time for inference\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Measure FLOPs using PyTorch profiler\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], with_flops=True) as prof:\n",
    "        with torch.no_grad():\n",
    "            result = inference_function(model, inputs['input_ids'], max_new_tokens=max_new_tokens)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    # Measure final power consumption\n",
    "    power_end = measure_power_consumption(handle, duration_sec=0.2)\n",
    "\n",
    "    # Calculate average power and elapsed time\n",
    "    avg_power = (power_start + power_end) / 2\n",
    "    elapsed_time = end_time - start_time\n",
    "    energy_consumed = avg_power * elapsed_time\n",
    "    print(\"prof keys flops table\")\n",
    "    print(prof.key_averages().table(sort_by=\"flops\", row_limit=10))\n",
    "    # Calculate FLOPs\n",
    "    flops = sum(event.flops for event in prof.key_averages() if event.flops is not None)\n",
    "\n",
    "    return energy_consumed, elapsed_time, flops, result\n",
    "\n",
    "\n",
    "\n",
    "# Calculate perplexity for generated text\n",
    "def calculate_perplexity(model, input_text, tokenizer):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)  # Ensure input is on the same device\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "        perplexity = torch.exp(loss)\n",
    "    return perplexity.item()\n",
    "\n",
    "# Run the experiment for a list of texts\n",
    "def run_experiment_for_texts(texts, bootstrapping, handle, model, tokenizer):\n",
    "    latencies = []\n",
    "    energy_per_token = []\n",
    "    energy_per_flops = []\n",
    "    energy_per_task = []\n",
    "    throughputs = []\n",
    "    generated_texts = []\n",
    "    perplexities = []\n",
    "\n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\").to(device)  # Ensure input is on the same device\n",
    "        text_latencies = []\n",
    "        text_energy_per_token = []\n",
    "        text_energy_per_flops = []\n",
    "        text_energy_per_task = []\n",
    "        text_throughput = []\n",
    "        text_generated = []\n",
    "        text_perplexities = []\n",
    "\n",
    "        for _ in range(bootstrapping):\n",
    "            energy_consumed, latency, flops, output = measure_energy_during_inference(\n",
    "                handle, model.generate, model, inputs, max_new_tokens=200\n",
    "            )\n",
    "            text_latencies.append(latency)\n",
    "\n",
    "            print(\"output:\", output)\n",
    "            output_tokens = output.size(-1)\n",
    "            energy_token = energy_consumed / output_tokens if output_tokens > 0 else 0\n",
    "            text_energy_per_token.append(energy_token)\n",
    "\n",
    "            # Energy per FLOPs calculation\n",
    "\n",
    "            print(\"text_energy_per_token:\", text_energy_per_token)\n",
    "            print(\"output_tokens:\", output_tokens)\n",
    "            print(\"flop:\", flops)\n",
    "            print(\"energy_consumed: \",energy_consumed)\n",
    "            energy_flop = energy_consumed / flops #if flops > 0 else 0\n",
    "            text_energy_per_flops.append(energy_flop)\n",
    "\n",
    "            # Energy per task (full inference energy)\n",
    "            text_energy_per_task.append(energy_consumed)\n",
    "\n",
    "            throughput = output_tokens / latency\n",
    "            text_throughput.append(throughput)\n",
    "\n",
    "            perplexity = calculate_perplexity(model, text, tokenizer)\n",
    "            text_perplexities.append(perplexity)\n",
    "\n",
    "            generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            filtered_generated_text = generated_text.replace(text, \"\").strip()\n",
    "            text_generated.append(filtered_generated_text)\n",
    "\n",
    "        latencies.append(text_latencies)\n",
    "        energy_per_token.append(text_energy_per_token)\n",
    "        energy_per_flops.append(text_energy_per_flops)\n",
    "        energy_per_task.append(text_energy_per_task)\n",
    "        throughputs.append(text_throughput)\n",
    "        generated_texts.append(text_generated)\n",
    "        perplexities.append(text_perplexities)\n",
    "\n",
    "    return latencies, energy_per_token, energy_per_flops, energy_per_task, throughputs, generated_texts, perplexities\n",
    "\n",
    "# Collect metrics for each category\n",
    "def collect_metrics_for_categories(df, categories, bootstrapping, model, tokenizer):\n",
    "    category_metrics = {}\n",
    "    handle = get_gpu_handle(gpu_index=0)\n",
    "\n",
    "    for category in categories:\n",
    "        print(f\"Processing category: {category}\")\n",
    "        texts = filter_texts_by_category(df, category)\n",
    "        latencies, energy_per_token, energy_per_flops, energy_per_task, throughputs, generated_texts, perplexities = run_experiment_for_texts(\n",
    "            texts, bootstrapping, handle, model, tokenizer\n",
    "        )\n",
    "\n",
    "        category_metrics[category] = {\n",
    "            \"latencies\": latencies,\n",
    "            \"energy_per_token\": energy_per_token,\n",
    "            \"energy_per_flops\": energy_per_flops,\n",
    "            \"energy_per_task\": energy_per_task,\n",
    "            \"throughput\": throughputs,\n",
    "            \"generated_texts\": generated_texts,\n",
    "            \"perplexities\": perplexities\n",
    "        }\n",
    "\n",
    "    shutdown_nvml()  \n",
    "    return category_metrics\n",
    "\n",
    "def filter_texts_by_category(df, category):\n",
    "    return df[df['category'] == category]['text'].values\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Example Usage\n",
    "file_path = \"./question.jsonl\"\n",
    "# bootstrapping = 2 \n",
    "df_mtconversation = load_dataset(file_path)\n",
    "\n",
    "#categories = [ 'common-sense']\n",
    "categories = ['knowledge', 'common-sense', 'coding', 'math']\n",
    "\n",
    "initialize_nvml()\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "metrics = collect_metrics_for_categories(df_mtconversation, categories, bootstrapping, model, tokenizer)\n",
    "\n",
    "# (Optionally, you can visualize the collected metrics here)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"flop_async_{model_name.replace('/','-').replace('.', '_')}_bootstrapping={bootstrapping}_metrics.json\", \"w\") as json_file:\n",
    "    json.dump(metrics, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the metrics\n",
    "def plot_metrics(metrics, categories):\n",
    "    num_categories = len(categories)\n",
    "\n",
    "    # Prepare the data for plotting\n",
    "    avg_latencies = []\n",
    "    avg_perplexities = []\n",
    "    avg_energy_per_flops = []\n",
    "    avg_energy_per_token = []\n",
    "    avg_energy_per_task = []\n",
    "\n",
    "    for category in categories:\n",
    "        if category in metrics:\n",
    "            avg_latencies.append(np.mean(metrics[category][\"latencies\"]))\n",
    "            avg_perplexities.append(np.mean(metrics[category][\"perplexities\"]))\n",
    "            avg_energy_per_flops.append(np.mean(metrics[category][\"energy_per_flops\"]))\n",
    "            avg_energy_per_token.append(np.mean(metrics[category][\"energy_per_token\"]))\n",
    "            avg_energy_per_task.append(np.mean(metrics[category][\"energy_per_task\"]))\n",
    "        else:\n",
    "            avg_latencies.append(0)\n",
    "            avg_perplexities.append(0)\n",
    "            avg_energy_per_flops.append(0)\n",
    "            avg_energy_per_token.append(0)\n",
    "            avg_energy_per_task.append(0)\n",
    "\n",
    "    x = np.arange(num_categories)  # the label locations\n",
    "    width = 0.15  # the width of the bars\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "    # Plot latencies\n",
    "    bars1 = ax1.bar(x - 2*width, avg_latencies, width, label='Average Latency (s)', color='b')\n",
    "    ax1.set_ylabel('Average Latency (s)', color='b')\n",
    "    ax1.tick_params(axis='y', labelcolor='b')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(categories)\n",
    "\n",
    "    # Create a second y-axis for perplexities\n",
    "    ax2 = ax1.twinx()\n",
    "    bars2 = ax2.bar(x - width, avg_perplexities, width, label='Average Perplexity', color='g')\n",
    "    ax2.set_ylabel('Average Perplexity', color='g')\n",
    "    ax2.tick_params(axis='y', labelcolor='g')\n",
    "\n",
    "    # Create a third y-axis for energy per FLOPs\n",
    "    ax3 = ax1.twinx()\n",
    "    bars3 = ax3.bar(x, avg_energy_per_flops, width, label='Energy per FLOP (Joules)', color='r')\n",
    "    ax3.spines['right'].set_position(('outward', 60))  # move the third y-axis to the right\n",
    "    ax3.set_ylabel('Energy per FLOP (Joules)', color='r')\n",
    "    ax3.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "    # Create a fourth y-axis for energy per token\n",
    "    ax4 = ax1.twinx()\n",
    "    bars4 = ax4.bar(x + width, avg_energy_per_token, width, label='Energy per Token (Joules)', color='purple')\n",
    "    ax4.spines['right'].set_position(('outward', 120))  # move the fourth y-axis to the right\n",
    "    ax4.set_ylabel('Energy per Token (Joules)', color='purple')\n",
    "    ax4.tick_params(axis='y', labelcolor='purple')\n",
    "\n",
    "\n",
    "plot_metrics(metrics, categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# energy_per_flops with asynchronous energy measuring MMLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Science, Technology, Engineering, Mathematics = stem\n",
    "stem = [\"clinical_knowledge\",\n",
    "\"medical_genetics\", \n",
    "\"high_school_physics\",\n",
    "\"virology\",\n",
    "\"high_school_biology\",\n",
    "\"abstract_algebra\",\n",
    "\"professional_medicine\",\n",
    "\"nutrition\",\n",
    "\"machine_learning\",\n",
    "\"anatomy\",\n",
    "\"college_medicine\",\n",
    "\"college_chemistry\",\n",
    "\"elementary_mathematics\",\n",
    "\"human_aging\",\n",
    "\"college_mathematics\",\n",
    "\"high_school_statistics\",\n",
    "\"high_school_mathematics\",\n",
    "\"high_school_computer_science\",\n",
    "\"conceptual_physics\",\n",
    "\"high_school_chemistry\",\n",
    "\"college_physics\",\n",
    "\"electrical_engineering\",\n",
    "\"astronomy\",\n",
    "\"college_biology\",\n",
    "\"computer_security\"]\n",
    "\n",
    "humanities= [\"high_school_european_history\",\n",
    "\"high_school_us_history\",\n",
    "\"high_school_world_history\",\n",
    "\"philosophy\",\n",
    "\"global_facts\",\n",
    "\"security_studies\",\n",
    "\"prehistory\",\n",
    "\"high_school_government_and_politics\",\n",
    "\"logical_fallacies\",\n",
    "\"international_law\",\n",
    "\"jurisprudence\",\n",
    "\"world_religions\",\n",
    "\"us_foreign_policy\",\n",
    "\"moral_scenarios\",\n",
    "\"moral_disputes\"\n",
    "]\n",
    "\n",
    "sociology = [\"sociology\",\n",
    "\"professional_psychology\",\n",
    "\"high_school_psychology\",\n",
    "\"human_sexuality\"]\n",
    "\n",
    "others = [\"business_ethics\",\n",
    "\"high_school_microeconomics\",\n",
    "\"econometrics\",\n",
    "\"professional_accounting\",\n",
    "\"public_relations\",\n",
    "\"marketing\",\n",
    "\"professional_law\",\n",
    "\"management\",\n",
    "\"miscellaneous\",\n",
    "\"high_school_macroeconomics\"]\n",
    "\n",
    "math = [\"abstract_algebra\",\n",
    "\t\"college_mathematics\",\n",
    "\t\"elementary_mathematics\",\n",
    "\t\"high_school_mathematics\",\n",
    "\t\"high_school_statistics\"]\n",
    "\n",
    "computer_science = [\"college_computer_science\",\n",
    "\t\"computer_security\",\n",
    "\t\"high_school_computer_science\",\n",
    "\t\"machine_learning\"]\n",
    "\n",
    "health = [\"anatomy\",\n",
    "\t\"clinical_knowledge\",\n",
    "\t\"college_medicine\",\n",
    "\t\"human_aging\",\n",
    "\t\"medical_genetics\",\n",
    "\t\"nutrition\",\n",
    "\t\"professional_medicine\",\n",
    "\t\"virology\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import threading\n",
    "import torch\n",
    "import pynvml\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.profiler import profile, ProfilerActivity\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import subprocess\n",
    "\n",
    "# Initialize NVML for power measurement\n",
    "def initialize_nvml():\n",
    "    pynvml.nvmlInit()\n",
    "\n",
    "def shutdown_nvml():\n",
    "    pynvml.nvmlShutdown()\n",
    "\n",
    "def get_gpu_handle(gpu_index=0):\n",
    "    return pynvml.nvmlDeviceGetHandleByIndex(gpu_index)\n",
    "\n",
    "def start_power_monitoring(handle, interval_sec=0.1):\n",
    "    power_readings = []\n",
    "    running = True\n",
    "\n",
    "    def monitor():\n",
    "        while running:\n",
    "            power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0  # Convert from mW to W\n",
    "            timestamp = time.time()\n",
    "            power_readings.append((timestamp, power))\n",
    "            time.sleep(interval_sec)\n",
    "\n",
    "    thread = threading.Thread(target=monitor)\n",
    "    thread.start()\n",
    "\n",
    "    def stop():\n",
    "        nonlocal running\n",
    "        running = False\n",
    "        thread.join()\n",
    "\n",
    "    return power_readings, stop\n",
    "\n",
    "# Map generated text to one of the options A, B, C, D\n",
    "def map_generated_text_to_option(generated_text):\n",
    "    valid_options = ['A', 'B', 'C', 'D']\n",
    "    generated_text = generated_text.strip().upper()\n",
    "    if generated_text in valid_options:\n",
    "        return generated_text\n",
    "    #else:\n",
    "        # Attempt to extract the option from the text\n",
    "        #for option in valid_options:\n",
    "            #if option in generated_text:\n",
    "                #return option\n",
    "        # If no valid option is found, return None\n",
    "    return None\n",
    "\n",
    "# Measure energy consumed during inference and FLOPs\n",
    "def measure_energy_during_inference(handle, inference_function, model, inputs, max_new_tokens=1):\n",
    "    print(f\"tokens: {max_new_tokens}\")\n",
    "    \n",
    "    # Start power monitoring\n",
    "    power_readings, stop_monitoring = start_power_monitoring(handle, interval_sec=0.05)\n",
    "    \n",
    "    # Start time for inference\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Measure FLOPs using PyTorch profiler\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], with_flops=True, record_shapes=False) as prof:\n",
    "        with torch.no_grad():\n",
    "            result = inference_function(inputs['input_ids'], max_new_tokens=max_new_tokens, do_sample=False )#num_beams=1)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Stop power monitoring\n",
    "    stop_monitoring()\n",
    "\n",
    "    # Filter power readings during inference\n",
    "    power_during_inference = [p for t, p in power_readings if start_time <= t <= end_time]\n",
    "\n",
    "    # Calculate average power and energy consumed\n",
    "    if power_during_inference:\n",
    "        avg_power = sum(power_during_inference) / len(power_during_inference)\n",
    "        elapsed_time = end_time - start_time\n",
    "        energy_consumed = avg_power * elapsed_time\n",
    "    else:\n",
    "        avg_power = 0\n",
    "        energy_consumed = 0\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "    # Calculate FLOPs\n",
    "    flops = sum([event.flops for event in prof.key_averages() if event.flops is not None])\n",
    "\n",
    "    return energy_consumed, elapsed_time, flops, result\n",
    "\n",
    "# Load the MMLU dataset for specified categories\n",
    "def load_mmlu_data(categories):\n",
    "    category_dataframes = {}  # Dictionary to store DataFrames for each category\n",
    "        \n",
    "    for category in categories:\n",
    "        print(\"Loading Data for category: \", category)\n",
    "            \n",
    "        # Load the dataset for the given category\n",
    "        mmlu_dataset = load_dataset(\"lukaemon/mmlu\", category, split='validation', trust_remote_code=True)\n",
    "        \n",
    "        # Create a DataFrame for the current category\n",
    "        df_category = pd.DataFrame({\n",
    "            'input': mmlu_dataset['input'],  # The question or prompt\n",
    "            'A': mmlu_dataset['A'],          # Option A\n",
    "            'B': mmlu_dataset['B'],          # Option B\n",
    "            'C': mmlu_dataset['C'],          # Option C\n",
    "            'D': mmlu_dataset['D'],          # Option D\n",
    "            'target': mmlu_dataset['target'] # The correct answer (e.g., 'A', 'B', 'C', 'D')\n",
    "        })\n",
    "        \n",
    "        # Store the DataFrame in the dictionary, with the category as the key\n",
    "        category_dataframes[category] = df_category\n",
    "        \n",
    "    return category_dataframes\n",
    "\n",
    "# Run the experiment for a category in the MMLU dataset\n",
    "def run_experiment_for_mmlu_category(data, bootstrapping, handle, model, tokenizer, max_new_tokens):\n",
    "    latencies = []\n",
    "    energy_per_token = []\n",
    "    energy_per_flops = []\n",
    "    energy_per_task = []\n",
    "    throughputs = []\n",
    "    generated_texts = []\n",
    "    accuracies = []\n",
    "    \n",
    "    for idx, row in data.iterrows():\n",
    "        # Construct the prompt\n",
    "        prompt = f\"Question: {row['input']}\\nA) {row['A']}\\nB) {row['B']}\\nC) {row['C']}\\nD) {row['D']}\\nAnswer:\"\n",
    "        #prompt = \"Hello, how are you my friend?\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")  # Ensure input is on the same device\n",
    "        text_latencies = []\n",
    "        text_energy_per_token = []\n",
    "        text_energy_per_flops = []\n",
    "        text_energy_per_task = []\n",
    "        text_throughput = []\n",
    "        text_generated = []\n",
    "        correct_predictions = 0  # To calculate accuracy\n",
    "\n",
    "        for _ in range(bootstrapping):\n",
    "            energy_consumed, latency, flops, output = measure_energy_during_inference(\n",
    "                handle, model.generate, model, inputs, max_new_tokens=max_new_tokens\n",
    "            )\n",
    "            text_latencies.append(latency)\n",
    "            output_tokens = output.size(-1) - inputs['input_ids'].size(-1)\n",
    "            energy_token = energy_consumed / output_tokens if output_tokens > 0 else 0\n",
    "            text_energy_per_token.append(energy_token)\n",
    "\n",
    "            energy_flop = energy_consumed / flops if flops > 0 else 0\n",
    "            text_energy_per_flops.append(energy_flop)\n",
    "            text_energy_per_task.append(energy_consumed)\n",
    "            throughput = output_tokens / latency if latency > 0 else 0\n",
    "            text_throughput.append(throughput)\n",
    "\n",
    "            # Decode the generated token\n",
    "            generated_text = tokenizer.decode(output[0][inputs['input_ids'].size(-1):], skip_special_tokens=True)\n",
    "            generated_text = generated_text.strip()\n",
    "            print(f\"generated text: {generated_text}\")\n",
    "            text_generated.append(generated_text)\n",
    "\n",
    "            # Map the generated text to an option\n",
    "            mapped_answer = map_generated_text_to_option(generated_text)\n",
    "            print(f\"Generated answer: '{mapped_answer}' | Correct answer: '{row['target']}'\")\n",
    "            if mapped_answer == row['target']:\n",
    "                print(\"Adding to correct predictions\")\n",
    "                correct_predictions += 1\n",
    "\n",
    "        accuracy = correct_predictions / bootstrapping\n",
    "        accuracies.append(accuracy)\n",
    "        latencies.append(text_latencies)\n",
    "        energy_per_token.append(text_energy_per_token)\n",
    "        energy_per_flops.append(text_energy_per_flops)\n",
    "        energy_per_task.append(text_energy_per_task)\n",
    "        throughputs.append(text_throughput)\n",
    "        generated_texts.append(text_generated)\n",
    "\n",
    "    overall_accuracy = np.mean(accuracies)\n",
    "    return latencies, energy_per_token, energy_per_flops, energy_per_task, throughputs, generated_texts, overall_accuracy\n",
    "\n",
    "# Collect metrics for each category\n",
    "def collect_metrics_for_categories(data_dict, categories, bootstrapping, model, tokenizer, max_new_tokens):\n",
    "    category_metrics = {}\n",
    "    handle = get_gpu_handle(gpu_index=0)\n",
    "\n",
    "    for category in categories:\n",
    "        print(f\"Processing category: {category}\")\n",
    "        data = data_dict[category]\n",
    "        latencies, energy_per_token, energy_per_flops, energy_per_task, throughputs, generated_texts, overall_accuracy = run_experiment_for_mmlu_category(\n",
    "            data, bootstrapping, handle, model, tokenizer, max_new_tokens\n",
    "        )\n",
    "\n",
    "        category_metrics[category] = {\n",
    "            \"latencies\": latencies,\n",
    "            \"energy_per_token\": energy_per_token,\n",
    "            \"energy_per_flops\": energy_per_flops,\n",
    "            \"energy_per_task\": energy_per_task,\n",
    "            \"throughput\": throughputs,\n",
    "            \"generated_texts\": generated_texts,\n",
    "            \"accuracy\": overall_accuracy\n",
    "        }\n",
    "\n",
    "    shutdown_nvml()  \n",
    "    return category_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7552cdad766d451a9b2113f503ea5fab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data for category:  abstract_algebra\n",
      "Loading Data for category:  college_mathematics\n",
      "Loading Data for category:  elementary_mathematics\n",
      "Loading Data for category:  high_school_mathematics\n",
      "Loading Data for category:  high_school_statistics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing category: abstract_algebra\n",
      "tokens: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated text: I hope you are doing well. I am fine. I am writing to you because I am looking for a friend. I am a simple girl. I am looking for a friend who is honest, sincere, loving, caring, and understanding. I\n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated text: I hope you are doing well. I am fine. I am writing to you because I am looking for a friend. I am a simple girl. I am looking for a friend who is honest, sincere, loving, caring, and understanding. I\n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated text: I hope you are doing well. I am fine. I am writing to you because I am looking for a friend. I am a simple girl. I am looking for a friend who is honest, sincere, loving, caring, and understanding. I\n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated text: I hope you are doing well. I am fine. I am writing to you because I am looking for a friend. I am a simple girl. I am looking for a friend who is honest, sincere, loving, caring, and understanding. I\n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m data_dict \u001b[38;5;241m=\u001b[39m load_mmlu_data(categories)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Collect metrics\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m flop_mmlu_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mcollect_metrics_for_categories\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbootstrapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 193\u001b[0m, in \u001b[0;36mcollect_metrics_for_categories\u001b[0;34m(data_dict, categories, bootstrapping, model, tokenizer, max_new_tokens)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing category: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    192\u001b[0m     data \u001b[38;5;241m=\u001b[39m data_dict[category]\n\u001b[0;32m--> 193\u001b[0m     latencies, energy_per_token, energy_per_flops, energy_per_task, throughputs, generated_texts, overall_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiment_for_mmlu_category\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbootstrapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     category_metrics[category] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatencies\u001b[39m\u001b[38;5;124m\"\u001b[39m: latencies,\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menergy_per_token\u001b[39m\u001b[38;5;124m\"\u001b[39m: energy_per_token,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: overall_accuracy\n\u001b[1;32m    205\u001b[0m     }\n\u001b[1;32m    207\u001b[0m shutdown_nvml()  \n",
      "Cell \u001b[0;32mIn[12], line 146\u001b[0m, in \u001b[0;36mrun_experiment_for_mmlu_category\u001b[0;34m(data, bootstrapping, handle, model, tokenizer, max_new_tokens)\u001b[0m\n\u001b[1;32m    143\u001b[0m correct_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# To calculate accuracy\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(bootstrapping):\n\u001b[0;32m--> 146\u001b[0m     energy_consumed, latency, flops, output \u001b[38;5;241m=\u001b[39m \u001b[43mmeasure_energy_during_inference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m     text_latencies\u001b[38;5;241m.\u001b[39mappend(latency)\n\u001b[1;32m    150\u001b[0m     output_tokens \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 70\u001b[0m, in \u001b[0;36mmeasure_energy_during_inference\u001b[0;34m(handle, inference_function, model, inputs, max_new_tokens)\u001b[0m\n\u001b[1;32m     67\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Measure FLOPs using PyTorch profiler\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profile(activities\u001b[38;5;241m=\u001b[39m[ProfilerActivity\u001b[38;5;241m.\u001b[39mCPU, ProfilerActivity\u001b[38;5;241m.\u001b[39mCUDA], with_flops\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, record_shapes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m prof:\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     72\u001b[0m         result \u001b[38;5;241m=\u001b[39m inference_function(inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m], max_new_tokens\u001b[38;5;241m=\u001b[39mmax_new_tokens, do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m )\u001b[38;5;66;03m#num_beams=1)\u001b[39;00m\n",
      "File \u001b[0;32m~/energy_per_token/.venv/lib/python3.10/site-packages/torch/profiler/profiler.py:706\u001b[0m, in \u001b[0;36mprofile.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[0;32m--> 706\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     prof\u001b[38;5;241m.\u001b[39mKinetoStepTracker\u001b[38;5;241m.\u001b[39merase_step_count(PROFILER_STEP_NAME)\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_trace_observer:\n",
      "File \u001b[0;32m~/energy_per_token/.venv/lib/python3.10/site-packages/torch/profiler/profiler.py:722\u001b[0m, in \u001b[0;36mprofile.stop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecord_steps \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_rec_fn:\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_rec_fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 722\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transit_action\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/energy_per_token/.venv/lib/python3.10/site-packages/torch/profiler/profiler.py:751\u001b[0m, in \u001b[0;36mprofile._transit_action\u001b[0;34m(self, prev_action, current_action)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action_list:\n\u001b[1;32m    750\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m action_list:\n\u001b[0;32m--> 751\u001b[0m         \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/energy_per_token/.venv/lib/python3.10/site-packages/torch/profiler/profiler.py:206\u001b[0m, in \u001b[0;36m_KinetoProfile.stop_trace\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_trace_observer\u001b[38;5;241m.\u001b[39mstop()\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprofiler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__exit__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/energy_per_token/.venv/lib/python3.10/site-packages/torch/autograd/profiler.py:359\u001b[0m, in \u001b[0;36mprofile.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    357\u001b[0m _run_on_profiler_stop()\n\u001b[1;32m    358\u001b[0m t0 \u001b[38;5;241m=\u001b[39m perf_counter_ns()\n\u001b[0;32m--> 359\u001b[0m parsed_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_kineto_results\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkineto_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    360\u001b[0m t1 \u001b[38;5;241m=\u001b[39m perf_counter_ns()\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stats\u001b[38;5;241m.\u001b[39mparse_kineto_call_duration_us \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m((t1 \u001b[38;5;241m-\u001b[39m t0) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1000\u001b[39m)\n",
      "File \u001b[0;32m~/energy_per_token/.venv/lib/python3.10/site-packages/torch/autograd/profiler.py:523\u001b[0m, in \u001b[0;36mprofile._parse_kineto_results\u001b[0;34m(self, result)\u001b[0m\n\u001b[1;32m    517\u001b[0m         mem_record[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    519\u001b[0m is_async \u001b[38;5;241m=\u001b[39m kineto_event\u001b[38;5;241m.\u001b[39mis_async() \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m    520\u001b[0m     kineto_event\u001b[38;5;241m.\u001b[39mstart_thread_id() \u001b[38;5;241m!=\u001b[39m kineto_event\u001b[38;5;241m.\u001b[39mend_thread_id()\n\u001b[1;32m    521\u001b[0m )\n\u001b[0;32m--> 523\u001b[0m fe \u001b[38;5;241m=\u001b[39m \u001b[43mFunctionEvent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkineto_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorrelation_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_rewrite_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkineto_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_wildcard\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrace_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_rewrite_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkineto_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_wildcard\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthread\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkineto_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_thread_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_us\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_start_ns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mend_us\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_end_ns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfwd_thread\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkineto_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfwd_thread_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkineto_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshapes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconcrete_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkineto_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcrete_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstack\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m        \u001b[49m\u001b[43mentry\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mentry\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkineto_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_filter_stack_entry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mentry\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscope\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkineto_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscope\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_device\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcpu_memory_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcpu_memory_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_memory_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_memory_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_async\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_async\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m    \u001b[49m\u001b[43msequence_nr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkineto_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msequence_nr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkineto_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkineto_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_resource_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkineto_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_resource_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflops\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkineto_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflops\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    549\u001b[0m max_evt_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(max_evt_id, fe\u001b[38;5;241m.\u001b[39mid)\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fe\u001b[38;5;241m.\u001b[39mdevice_type \u001b[38;5;241m==\u001b[39m DeviceType\u001b[38;5;241m.\u001b[39mCPU \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fe\u001b[38;5;241m.\u001b[39mis_async:\n",
      "File \u001b[0;32m~/energy_per_token/.venv/lib/python3.10/site-packages/torch/autograd/profiler_util.py:476\u001b[0m, in \u001b[0;36mFunctionEvent.__init__\u001b[0;34m(self, id, name, thread, start_us, end_us, fwd_thread, input_shapes, stack, scope, use_device, cpu_memory_usage, device_memory_usage, is_async, is_remote, sequence_nr, node_id, device_type, device_index, device_resource_id, is_legacy, flops, trace_name, concrete_inputs)\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m name\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrace_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m trace_name\n\u001b[0;32m--> 476\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_range: Interval \u001b[38;5;241m=\u001b[39m \u001b[43mInterval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_us\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_us\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthread: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m thread\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfwd_thread: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m fwd_thread\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "categories = math # math computer_science health\n",
    "category_text = \"math\"\n",
    "\n",
    "# Bootstrapping iterations\n",
    "bootstrapping = 2\n",
    "\n",
    "# max new output tokens\n",
    "max_new_tokens = 50\n",
    "\n",
    "initialize_nvml()\n",
    "\n",
    "# HF Access Token\n",
    "access_token = \"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\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"meta-llama/Llama-3.1-8B\" \n",
    "            #\"meta-llama/Llama-3.1-8B\"  \n",
    "            #\"facebook/opt-125m\"\n",
    "            #\"tiiuae/falcon-7b\"\n",
    "            #\"ProbeMedicalYonseiMAILab/medllama3-v20\"\n",
    "            #\"NTQAI/Nxcode-CQ-7B-orpo\"\n",
    "            #\"MathLLMs/MathCoder-L-7B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", token=access_token)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=access_token)\n",
    "\n",
    "# Load MMLU data\n",
    "data_dict = load_mmlu_data(categories)\n",
    "\n",
    "# Collect metrics\n",
    "flop_mmlu_metrics = collect_metrics_for_categories(data_dict, categories, bootstrapping, model, tokenizer, max_new_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save metrics to JSON file\n",
    "with open(f\"flop_MMLU_model={model_name.replace('/','-').replace('.', '_')}_maxnewtokens={max_new_tokens}_bootstrapping={bootstrapping}_category={category_text}_metrics.json\", \"w\") as json_file:\n",
    "    json.dump(flop_mmlu_metrics, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# energy_per_flops with asynchronous energy measuring MMLU quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Science, Technology, Engineering, Mathematics = stem\n",
    "stem = [\"clinical_knowledge\",\n",
    "\"medical_genetics\", \n",
    "\"high_school_physics\",\n",
    "\"virology\",\n",
    "\"high_school_biology\",\n",
    "\"abstract_algebra\",\n",
    "\"professional_medicine\",\n",
    "\"nutrition\",\n",
    "\"machine_learning\",\n",
    "\"anatomy\",\n",
    "\"college_medicine\",\n",
    "\"college_chemistry\",\n",
    "\"elementary_mathematics\",\n",
    "\"human_aging\",\n",
    "\"college_mathematics\",\n",
    "\"high_school_statistics\",\n",
    "\"high_school_mathematics\",\n",
    "\"high_school_computer_science\",\n",
    "\"conceptual_physics\",\n",
    "\"high_school_chemistry\",\n",
    "\"college_physics\",\n",
    "\"electrical_engineering\",\n",
    "\"astronomy\",\n",
    "\"college_biology\",\n",
    "\"computer_security\"]\n",
    "\n",
    "humanities= [\"high_school_european_history\",\n",
    "\"high_school_us_history\",\n",
    "\"high_school_world_history\",\n",
    "\"philosophy\",\n",
    "\"global_facts\",\n",
    "\"security_studies\",\n",
    "\"prehistory\",\n",
    "\"high_school_government_and_politics\",\n",
    "\"logical_fallacies\",\n",
    "\"international_law\",\n",
    "\"jurisprudence\",\n",
    "\"world_religions\",\n",
    "\"us_foreign_policy\",\n",
    "\"moral_scenarios\",\n",
    "\"moral_disputes\"\n",
    "]\n",
    "\n",
    "sociology = [\"sociology\",\n",
    "\"professional_psychology\",\n",
    "\"high_school_psychology\",\n",
    "\"human_sexuality\"]\n",
    "\n",
    "others = [\"business_ethics\",\n",
    "\"high_school_microeconomics\",\n",
    "\"econometrics\",\n",
    "\"professional_accounting\",\n",
    "\"public_relations\",\n",
    "\"marketing\",\n",
    "\"professional_law\",\n",
    "\"management\",\n",
    "\"miscellaneous\",\n",
    "\"high_school_macroeconomics\"]\n",
    "\n",
    "math = [\"abstract_algebra\",\n",
    "\t\"college_mathematics\",\n",
    "\t\"elementary_mathematics\",\n",
    "\t\"high_school_mathematics\",\n",
    "\t\"high_school_statistics\"]\n",
    "\n",
    "computer_science = [\"college_computer_science\",\n",
    "\t\"computer_security\",\n",
    "\t\"high_school_computer_science\",\n",
    "\t\"machine_learning\"]\n",
    "\n",
    "health = [\"anatomy\",\n",
    "\t\"clinical_knowledge\",\n",
    "\t\"college_medicine\",\n",
    "\t\"human_aging\",\n",
    "\t\"medical_genetics\",\n",
    "\t\"nutrition\",\n",
    "\t\"professional_medicine\",\n",
    "\t\"virology\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import threading\n",
    "import torch\n",
    "import pynvml\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from torch.profiler import profile, ProfilerActivity\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import bitsandbytes\n",
    "\n",
    "# Initialize NVML for power measurement\n",
    "def initialize_nvml():\n",
    "    pynvml.nvmlInit()\n",
    "\n",
    "def shutdown_nvml():\n",
    "    pynvml.nvmlShutdown()\n",
    "\n",
    "def get_gpu_handle(gpu_index=0):\n",
    "    return pynvml.nvmlDeviceGetHandleByIndex(gpu_index)\n",
    "\n",
    "def start_power_monitoring(handle, interval_sec=0.1):\n",
    "    power_readings = []\n",
    "    running = True\n",
    "\n",
    "    def monitor():\n",
    "        while running:\n",
    "            power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0  # Convert from mW to W\n",
    "            timestamp = time.time()\n",
    "            power_readings.append((timestamp, power))\n",
    "            time.sleep(interval_sec)\n",
    "\n",
    "    thread = threading.Thread(target=monitor)\n",
    "    thread.start()\n",
    "\n",
    "    def stop():\n",
    "        nonlocal running\n",
    "        running = False\n",
    "        thread.join()\n",
    "\n",
    "    return power_readings, stop\n",
    "\n",
    "# Map generated text to one of the options A, B, C, D\n",
    "def map_generated_text_to_option(generated_text):\n",
    "    valid_options = ['A', 'B', 'C', 'D']\n",
    "    generated_text = generated_text.strip().upper()\n",
    "    if generated_text in valid_options:\n",
    "        return generated_text\n",
    "    #else:\n",
    "        # Attempt to extract the option from the text\n",
    "        #for option in valid_options:\n",
    "            #if option in generated_text:\n",
    "                #return option\n",
    "        # If no valid option is found, return None\n",
    "    return None\n",
    "\n",
    "# Measure energy consumed during inference and FLOPs\n",
    "def measure_energy_during_inference(handle, inference_function, model, inputs, max_new_tokens=1):\n",
    "    # Start power monitoring\n",
    "    power_readings, stop_monitoring = start_power_monitoring(handle, interval_sec=0.05)\n",
    "    \n",
    "    # Start time for inference\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Measure FLOPs using PyTorch profiler\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], with_flops=True, record_shapes=False) as prof:\n",
    "        with torch.no_grad():\n",
    "            result = inference_function(inputs['input_ids'], max_new_tokens=max_new_tokens, do_sample=False, num_beams=1)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Stop power monitoring\n",
    "    stop_monitoring()\n",
    "\n",
    "    # Filter power readings during inference\n",
    "    power_during_inference = [p for t, p in power_readings if start_time <= t <= end_time]\n",
    "\n",
    "    # Calculate average power and energy consumed\n",
    "    if power_during_inference:\n",
    "        avg_power = sum(power_during_inference) / len(power_during_inference)\n",
    "        elapsed_time = end_time - start_time\n",
    "        energy_consumed = avg_power * elapsed_time\n",
    "    else:\n",
    "        avg_power = 0\n",
    "        energy_consumed = 0\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "    # Calculate FLOPs\n",
    "    flops = sum([event.flops for event in prof.key_averages() if event.flops is not None])\n",
    "\n",
    "    return energy_consumed, elapsed_time, flops, result\n",
    "\n",
    "# Load the MMLU dataset for specified categories\n",
    "def load_mmlu_data(categories):\n",
    "    category_dataframes = {}  # Dictionary to store DataFrames for each category\n",
    "        \n",
    "    for category in categories:\n",
    "        print(\"Loading Data for category: \", category)\n",
    "            \n",
    "        # Load the dataset for the given category\n",
    "        mmlu_dataset = load_dataset(\"lukaemon/mmlu\", category, split='validation', trust_remote_code=True)\n",
    "        \n",
    "        # Create a DataFrame for the current category\n",
    "        df_category = pd.DataFrame({\n",
    "            'input': mmlu_dataset['input'],  # The question or prompt\n",
    "            'A': mmlu_dataset['A'],          # Option A\n",
    "            'B': mmlu_dataset['B'],          # Option B\n",
    "            'C': mmlu_dataset['C'],          # Option C\n",
    "            'D': mmlu_dataset['D'],          # Option D\n",
    "            'target': mmlu_dataset['target'] # The correct answer (e.g., 'A', 'B', 'C', 'D')\n",
    "        })\n",
    "        \n",
    "        # Store the DataFrame in the dictionary, with the category as the key\n",
    "        category_dataframes[category] = df_category\n",
    "        \n",
    "    return category_dataframes\n",
    "\n",
    "# Run the experiment for a category in the MMLU dataset\n",
    "def run_experiment_for_mmlu_category(data, bootstrapping, handle, model, tokenizer):\n",
    "    latencies = []\n",
    "    energy_per_token = []\n",
    "    energy_per_flops = []\n",
    "    energy_per_task = []\n",
    "    throughputs = []\n",
    "    generated_texts = []\n",
    "    accuracies = []\n",
    "    \n",
    "    for idx, row in data.iterrows():\n",
    "        # Construct the prompt\n",
    "        prompt = f\"Question: {row['input']}\\nA) {row['A']}\\nB) {row['B']}\\nC) {row['C']}\\nD) {row['D']}\\nAnswer:\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")  # Ensure input is on the same device\n",
    "        text_latencies = []\n",
    "        text_energy_per_token = []\n",
    "        text_energy_per_flops = []\n",
    "        text_energy_per_task = []\n",
    "        text_throughput = []\n",
    "        text_generated = []\n",
    "        correct_predictions = 0  # To calculate accuracy\n",
    "\n",
    "        for _ in range(bootstrapping):\n",
    "            energy_consumed, latency, flops, output = measure_energy_during_inference(\n",
    "                handle, model.generate, model, inputs, max_new_tokens=1\n",
    "            )\n",
    "            text_latencies.append(latency)\n",
    "            output_tokens = output.size(-1) - inputs['input_ids'].size(-1)\n",
    "            energy_token = energy_consumed / output_tokens if output_tokens > 0 else 0\n",
    "            text_energy_per_token.append(energy_token)\n",
    "\n",
    "            energy_flop = energy_consumed / flops if flops > 0 else 0\n",
    "            text_energy_per_flops.append(energy_flop)\n",
    "            text_energy_per_task.append(energy_consumed)\n",
    "            throughput = output_tokens / latency if latency > 0 else 0\n",
    "            text_throughput.append(throughput)\n",
    "\n",
    "            # Decode the generated token\n",
    "            generated_text = tokenizer.decode(output[0][inputs['input_ids'].size(-1):], skip_special_tokens=True)\n",
    "            generated_text = generated_text.strip()\n",
    "            text_generated.append(generated_text)\n",
    "\n",
    "            # Map the generated text to an option\n",
    "            mapped_answer = map_generated_text_to_option(generated_text)\n",
    "            print(f\"Generated answer: '{mapped_answer}' | Correct answer: '{row['target']}'\")\n",
    "            if mapped_answer == row['target']:\n",
    "                correct_predictions += 1\n",
    "\n",
    "        accuracy = correct_predictions / bootstrapping\n",
    "        accuracies.append(accuracy)\n",
    "        latencies.append(text_latencies)\n",
    "        energy_per_token.append(text_energy_per_token)\n",
    "        energy_per_flops.append(text_energy_per_flops)\n",
    "        energy_per_task.append(text_energy_per_task)\n",
    "        throughputs.append(text_throughput)\n",
    "        generated_texts.append(text_generated)\n",
    "\n",
    "    overall_accuracy = np.mean(accuracies)\n",
    "    return latencies, energy_per_token, energy_per_flops, energy_per_task, throughputs, generated_texts, overall_accuracy\n",
    "\n",
    "# Collect metrics for each category\n",
    "def collect_metrics_for_categories(data_dict, categories, bootstrapping, model, tokenizer):\n",
    "    category_metrics = {}\n",
    "    handle = get_gpu_handle(gpu_index=0)\n",
    "\n",
    "    for category in categories:\n",
    "        print(f\"Processing category: {category}\")\n",
    "        data = data_dict[category]\n",
    "        latencies, energy_per_token, energy_per_flops, energy_per_task, throughputs, generated_texts, overall_accuracy = run_experiment_for_mmlu_category(\n",
    "            data, bootstrapping, handle, model, tokenizer\n",
    "        )\n",
    "\n",
    "        category_metrics[category] = {\n",
    "            \"latencies\": latencies,\n",
    "            \"energy_per_token\": energy_per_token,\n",
    "            \"energy_per_flops\": energy_per_flops,\n",
    "            \"energy_per_task\": energy_per_task,\n",
    "            \"throughput\": throughputs,\n",
    "            \"generated_texts\": generated_texts,\n",
    "            \"accuracy\": overall_accuracy\n",
    "        }\n",
    "\n",
    "    shutdown_nvml()  \n",
    "    return category_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = math # math computer_science health\n",
    "category_text = \"math\"\n",
    "\n",
    "# Bootstrapping iterations\n",
    "bootstrapping = 2\n",
    "\n",
    "initialize_nvml()\n",
    "\n",
    "# HF Access Token\n",
    "access_token = \"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\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"NTQAI/Nxcode-CQ-7B-orpo\"  \n",
    "            #\"meta-llama/Llama-3.1-8B\"  \n",
    "            #\"facebook/opt-125m\"\n",
    "            #\"tiiuae/falcon-7b\"\n",
    "            #\"ProbeMedicalYonseiMAILab/medllama3-v20\"\n",
    "            #\"NTQAI/Nxcode-CQ-7B-orpo\"\n",
    "            #\"MathLLMs/MathCoder-L-7B\"\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",  # Specify the quantization type\n",
    "    bnb_4bit_use_double_quant=True,  # Use double quantization if needed\n",
    "    bnb_4bit_compute_dtype=torch.float16  # Specify computation dtype\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", quantization_config=quant_config, token=access_token)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=access_token)\n",
    "\n",
    "# Load MMLU data\n",
    "data_dict = load_mmlu_data(categories)\n",
    "\n",
    "# Collect metrics\n",
    "flop_mmlu_metrics = collect_metrics_for_categories(data_dict, categories, bootstrapping, model, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save metrics to JSON file\n",
    "with open(f\"flop_MMLU_NF4_{model_name.replace('/','-').replace('.', '_')}_bootstrapping={bootstrapping}_category={category_text}_metrics.json\", \"w\") as json_file:\n",
    "    json.dump(flop_mmlu_metrics, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# energy_per_flops\n",
    "\n",
    "# with nvidia nsights - nvtx (does not work yet): youtube:\n",
    "short video: https://www.youtube.com/watch?v=5Gxx59Q0g6o\n",
    "hands on : https://www.youtube.com/watch?v=3DAYN-onSzY\n",
    "\n",
    "\n",
    "multiple different videos available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 77\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Example usage with a loop over your text inputs\u001b[39;00m\n\u001b[1;32m     76\u001b[0m texts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample input 1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample input 2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample input 3\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# Replace with actual text inputs\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m()  \u001b[38;5;66;03m# Your model loading logic\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Running the experiment to collect metrics\u001b[39;00m\n\u001b[1;32m     80\u001b[0m latencies, energy_per_task, energy_per_flop \u001b[38;5;241m=\u001b[39m run_experiment_for_texts(texts, model, handle)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_model' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import nvtx\n",
    "import pynvml\n",
    "from torch.profiler import profile, ProfilerActivity\n",
    "\n",
    "# Initialize NVML for GPU power usage\n",
    "pynvml.nvmlInit()\n",
    "handle = pynvml.nvmlDeviceGetHandleByIndex(0)  # Assuming single GPU\n",
    "\n",
    "# Function to measure GPU energy consumption (in Joules)\n",
    "def get_gpu_energy(handle, duration_sec):\n",
    "    power_draw = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000  # Convert from mW to W\n",
    "    return power_draw * duration_sec  # Energy in Joules\n",
    "\n",
    "# Function to calculate FLOPs using PyTorch's profiler\n",
    "def calculate_flops(model, inputs):\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], profile_memory=True, record_shapes=True) as prof:\n",
    "        with torch.no_grad():\n",
    "            model(inputs)\n",
    "    # Sum the FLOPs from the profiler\n",
    "    flops = sum([event.cpu_time for event in prof.key_averages()])\n",
    "    return flops\n",
    "    \n",
    "def calculate_perplexity(model, input_text, tokenizer):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)  # Ensure input is on the same device\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "        perplexity = torch.exp(loss)\n",
    "    return perplexity.item()\n",
    "\n",
    "# Function to run the experiment for each text (inference)\n",
    "def run_experiment_for_texts(texts, model, handle):\n",
    "    latencies = []\n",
    "    energy_per_task = []\n",
    "    energy_per_flop = []\n",
    "    perplexities = []\n",
    "    \n",
    "    for text in texts:\n",
    "        input_tensor = preprocess_text(text)\n",
    "        \n",
    "        # Start energy and time measurement\n",
    "        start_time = time.time()\n",
    "        nvtx.range_push(\"Task Inference\")\n",
    "\n",
    "        # Measure initial GPU energy\n",
    "        initial_energy = get_gpu_energy(handle, 0)\n",
    "\n",
    "        # Run inference\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "\n",
    "        # End NVTX range\n",
    "        nvtx.range_pop()\n",
    "\n",
    "        # Measure final GPU energy\n",
    "        end_time = time.time()\n",
    "        final_energy = get_gpu_energy(handle, end_time - start_time)\n",
    "\n",
    "        # Calculate task energy consumption\n",
    "        task_energy = final_energy - initial_energy\n",
    "\n",
    "        # Measure latency\n",
    "        latency = end_time - start_time\n",
    "        latencies.append(latency)\n",
    "\n",
    "        # Calculate FLOPs for this task\n",
    "        task_flops = calculate_flops(model, input_tensor)\n",
    "\n",
    "        # Calculate Energy per Task (Joules per task)\n",
    "        energy_per_task.append(task_energy)\n",
    "\n",
    "        # Calculate Energy per FLOP (Joules per FLOP)\n",
    "        if task_flops > 0:\n",
    "            energy_per_flop.append(task_energy / task_flops)\n",
    "        else:\n",
    "            energy_per_flop.append(np.nan)  # Handle case where FLOP estimation is zero\n",
    "        \n",
    "    return latencies, energy_per_task, energy_per_flop\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# chat gpt:\n",
    "# Example usage with a loop over your text inputs\n",
    "texts = [\"Sample input 1\", \"Sample input 2\", \"Sample input 3\"]  # Replace with actual text inputs\n",
    "model = load_model()  # Your model loading logic\n",
    "\n",
    "# Running the experiment to collect metrics\n",
    "latencies, energy_per_task, energy_per_flop = run_experiment_for_texts(texts, model, handle)\n",
    "\n",
    "# Example output\n",
    "for i, text in enumerate(texts):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Latency: {latencies[i]:.4f} seconds\")\n",
    "    print(f\"Energy per Task: {energy_per_task[i]:.4f} Joules\")\n",
    "    print(f\"Energy per FLOP: {energy_per_flop[i]:.4e} Joules/FLOP\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Clean up NVML\n",
    "pynvml.nvmlShutdown()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MT_Bench with vLLM\n",
    "\n",
    "\n",
    "### Das funktioniert noch nicht mit vLLM!!!  out of memory error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Patrick/vllmenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-09-26 16:22:31,750\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import transformers\n",
    "import accelerate\n",
    "import vllm\n",
    "import bitsandbytes\n",
    "from vllm import LLM, SamplingParams\n",
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib\n",
    "from collections import Counter\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "#matplotlib.use('TkAgg')\n",
    "#from awq import AutoAWQForCausalLM\n",
    "#from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MMLU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Science, Technology, Engineering, Mathematics = stem\n",
    "stem = [\"clinical_knowledge\",\n",
    "\"medical_genetics\", \n",
    "\"high_school_physics\",\n",
    "\"virology\",\n",
    "\"high_school_biology\",\n",
    "\"abstract_algebra\",\n",
    "\"professional_medicine\",\n",
    "\"nutrition\",\n",
    "\"machine_learning\",\n",
    "\"anatomy\",\n",
    "\"college_medicine\",\n",
    "\"college_chemistry\",\n",
    "\"elementary_mathematics\",\n",
    "\"human_aging\",\n",
    "\"college_mathematics\",\n",
    "\"high_school_statistics\",\n",
    "\"high_school_mathematics\",\n",
    "\"high_school_computer_science\",\n",
    "\"conceptual_physics\",\n",
    "\"high_school_chemistry\",\n",
    "\"college_physics\",\n",
    "\"electrical_engineering\",\n",
    "\"astronomy\",\n",
    "\"college_biology\",\n",
    "\"computer_security\"]\n",
    "\n",
    "humanities= [\"high_school_european_history\",\n",
    "\"high_school_us_history\",\n",
    "\"high_school_world_history\",\n",
    "\"philosophy\",\n",
    "\"global_facts\",\n",
    "\"security_studies\",\n",
    "\"prehistory\",\n",
    "\"high_school_government_and_politics\",\n",
    "\"logical_fallacies\",\n",
    "\"international_law\",\n",
    "\"jurisprudence\",\n",
    "\"world_religions\",\n",
    "\"us_foreign_policy\",\n",
    "\"moral_scenarios\",\n",
    "\"moral_disputes\"\n",
    "]\n",
    "\n",
    "sociology = [\"sociology\",\n",
    "\"professional_psychology\",\n",
    "\"high_school_psychology\",\n",
    "\"human_sexuality\"]\n",
    "\n",
    "others = [\"business_ethics\",\n",
    "\"high_school_microeconomics\",\n",
    "\"econometrics\",\n",
    "\"professional_accounting\",\n",
    "\"public_relations\",\n",
    "\"marketing\",\n",
    "\"professional_law\",\n",
    "\"management\",\n",
    "\"miscellaneous\",\n",
    "\"high_school_macroeconomics\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "def convert_to_dataframe(categories):\n",
    "    category_dataframes = {}  # Dictionary to store DataFrames for each category\n",
    "    \n",
    "    for category in categories:\n",
    "        print(\"Loading Data for category: \", category)\n",
    "        \n",
    "        # Load the dataset for the given category\n",
    "        mmlu_dataset = load_dataset(\"lukaemon/mmlu\", category, split='validation', trust_remote_code=True)\n",
    "        \n",
    "        # Create a DataFrame for the current category\n",
    "        df_category = pd.DataFrame({\n",
    "            'input': mmlu_dataset['input'],  # The question or prompt\n",
    "            'A': mmlu_dataset['A'],          # Option A\n",
    "            'B': mmlu_dataset['B'],          # Option B\n",
    "            'C': mmlu_dataset['C'],          # Option C\n",
    "            'D': mmlu_dataset['D'],          # Option D\n",
    "            'target': mmlu_dataset['target'] # The correct answer (e.g., 'A', 'B', 'C', 'D')\n",
    "        })\n",
    "        \n",
    "        # Store the DataFrame in the dictionary, with the category as the key\n",
    "        category_dataframes[category] = df_category\n",
    "    \n",
    "    return category_dataframes\n",
    "\n",
    "# Access a DataFrame for a specific category\n",
    "#print(category_dfs.head())  # Example for checking 'high_school_biology'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "stem, humanities ..etc\n",
    "\"\"\"\n",
    "\n",
    "# Example list of categories\n",
    "categories = ['high_school_biology', 'abstract_algebra', 'professional_medicine', 'nutrition']\n",
    "\n",
    "# Call the function and get the dictionary of DataFrames\n",
    "category_dfs = convert_to_dataframe(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from datasets import load_dataset\n",
    "\n",
    "#cataegory = \"sociology\"\n",
    "# Load the MMLU dataset\n",
    "#def load_mmlu_dataset():\n",
    "#    mmlu_dataset = load_dataset(\"lukaemon/mmlu\",category, split='validation',trust_remote_code=True)\n",
    "#    return mmlu_dataset\n",
    "\n",
    "\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\", device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")\n",
    "\n",
    "def get_gpu_power(gpu_index=1): #measuring for gpu1\n",
    "    \"\"\"Fetches the current power consumption of the GPU using nvidia-smi.\"\"\"\n",
    "    result = subprocess.run(['nvidia-smi', '--query-gpu=power.draw', '--format=csv,noheader,nounits', '-i',str(gpu_index)], \n",
    "                            stdout=subprocess.PIPE, text=True)\n",
    "    power = float(result.stdout.strip())  # Power in watts\n",
    "    return power\n",
    "\n",
    "def convert_to_dataframe(categories):\n",
    "    category_dataframes = {}  # Dictionary to store DataFrames for each category\n",
    "    \n",
    "    for category in categories:\n",
    "        print(\"Loading Data for category: \", category)\n",
    "        \n",
    "        # Load the dataset for the given category\n",
    "        mmlu_dataset = load_dataset(\"lukaemon/mmlu\",category, split='validation',trust_remote_code=True)\n",
    "        #print(type(mmlu_dataset))\n",
    "        # Create a DataFrame for the current category\n",
    "        df_category = pd.DataFrame({\n",
    "            'input': mmlu_dataset['input'],  # The question or prompt\n",
    "            'A': mmlu_dataset['A'],          # Option A\n",
    "            'B': mmlu_dataset['B'],          # Option B\n",
    "            'C': mmlu_dataset['C'],          # Option C\n",
    "            'D': mmlu_dataset['D'],          # Option D\n",
    "            'target': mmlu_dataset['target'] # The correct answer (e.g., 'A', 'B', 'C', 'D')\n",
    "        })\n",
    "        \n",
    "        # Store the DataFrame in the dictionary, with the category as the key\n",
    "        category_dataframes[category] = df_category\n",
    "    print(\"loading_ data finish\")\n",
    "    return category_dataframes\n",
    "\n",
    "# Filter dataset by category\n",
    "#def filter_texts_by_category(df, category):\n",
    "#    return df[df['category'] == category]['text'].values\n",
    "\n",
    "# Filter dataset by subject category (e.g., 'high_school_biology', 'abstract_algebra')\n",
    "def filter_dict_by_category(df, category):\n",
    "    return df[category] \n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "# Run the bootstrapping experiment for multiple-choice questions in a given category\n",
    "def run_experiment_for_texts(datadictionary,categories, bootstrapping ):\n",
    "    category_latencies = []\n",
    "    category_energy_per_token = []\n",
    "    \n",
    "    \n",
    "    #print(categories)\n",
    "    category_accuracy= []\n",
    "\n",
    "\n",
    "\n",
    "    # task in one category\n",
    "    for category in categories:\n",
    "        data = datadictionary[category]#filter_dict_by_category(datadictionary, category)\n",
    "        question_text = data['input'].values\n",
    "        choices = [data['A'].values, data['B'].values, data['C'].values, data['D'].values]\n",
    "        correct_answer = data['target'].values\n",
    "        \n",
    "        task_accuracy= []\n",
    "        task_latencies = []\n",
    "        task_energy_per_token = []\n",
    "        #print(\" type question_text: \", question_text)\n",
    "        #print(\"len question_text: \", len(question_text))\n",
    "        print(\"processing category: \", category)\n",
    "\n",
    "\n",
    "        # Prompts of one tasks\n",
    "        for i, tasks in enumerate (question_text):\n",
    "\n",
    "\n",
    "\n",
    "            print(\"i : \", i)\n",
    "            # Concatenate question with options for LLM input\n",
    "            full_input = f\"Question: {question_text[i]}\\nA) {choices[0][i]}\\nB) ,{choices[1][i]}\\nC) ,{choices[2][i]}\\nD) ,{choices[3][i]}\"\n",
    "            #print(\"full input: \", full_input)\n",
    "            inputs = tokenizer(full_input, return_tensors=\"pt\").to(\"cuda\")  # Prepare input tensors\n",
    "\n",
    "            text_latencies = []\n",
    "            text_energy_per_token = []\n",
    "            correct_predictions = 0  # To calculate accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # Prompt Bootstrapping \n",
    "            for _ in range(bootstrapping):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                print(_)\n",
    "                power_start = get_gpu_power()  # Assuming a function to get GPU power\n",
    "                start_time = time.time()\n",
    "\n",
    "                # Generate the model's response\n",
    "                output = model.generate(inputs['input_ids'], max_new_tokens=200, do_sample=False)  # Adjust tokens if necessary\n",
    "\n",
    "                end_time = time.time()\n",
    "                power_end = get_gpu_power()\n",
    "\n",
    "                # Measure latency\n",
    "                latency = end_time - start_time\n",
    "                text_latencies.append(latency)\n",
    "\n",
    "                # Calculate energy consumption\n",
    "                avg_power = (power_start + power_end) / 2\n",
    "                energy = avg_power * latency\n",
    "\n",
    "                # Token count from output (assuming a tensor output)\n",
    "                output_tokens = output[0].shape[0]\n",
    "                energy_token = energy / output_tokens if output_tokens > 0 else 0\n",
    "                text_energy_per_token.append(energy_token)\n",
    "\n",
    "                # Decode the model's generated answer (you might need to adjust based on model output format)\n",
    "                generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "                # Check if the model's generated answer matches the correct answer\n",
    "                if correct_answer[i] in generated_text:\n",
    "                    correct_predictions += 1\n",
    "\n",
    "            task_latencies.append(np.mean(text_latencies))\n",
    "            task_energy_per_token.append(np.mean(text_energy_per_token))\n",
    "            accuracy = correct_predictions / bootstrapping \n",
    "            task_accuracy.append(accuracy)\n",
    "    category_accuracy.append(np.array(task_accuracy)/len(question_text))\n",
    "    return task_latencies, task_energy_per_token, category_accuracy\n",
    "\n",
    "\n",
    "# Store and collect metrics for each category\n",
    "# Collect metrics for each category\n",
    "def collect_metrics_for_categories(df, categories, bootstrapping):\n",
    "    category_metrics = {}\n",
    "\n",
    "    \"\"\"    for category in categories:\n",
    "            print(f\"Processing category: {category}\")\n",
    "            texts = filter_texts_by_category(df, category)\n",
    "\n",
    "            if texts.empty:\n",
    "                print(f\"No texts found for category {category}\")\n",
    "                continue\n",
    "    \"\"\"\n",
    "    latencies, energy_per_token, accuracy = run_experiment_for_texts(data_dict, categories, bootstrapping)\n",
    "\n",
    "    # Store metrics for each category\n",
    "    category_metrics[category] = {\n",
    "        \"latencies\": latencies,\n",
    "        \"energy_per_token\": energy_per_token,\n",
    "        \"accuracy\": accuracy\n",
    "    }\n",
    "\n",
    "    return category_metrics\n",
    "\n",
    "# Plot the energy consumption per token comparison\n",
    "def plot_energy_vs_latency(metrics, categories):\n",
    "    for category in categories:\n",
    "        category_data = metrics[category]\n",
    "        energy_per_token = category_data[\"energy_per_token\"]\n",
    "        latencies = category_data[\"latencies\"]\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "\n",
    "        # Plot energy per token\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot([sum(x)/len(x) for x in energy_per_token], marker='o', color='blue', label='Energy per Token (J)')\n",
    "        plt.title(f\"Energy per Token for {category}\")\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Energy (J)')\n",
    "        plt.legend()\n",
    "\n",
    "        # Plot latencies\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot([sum(x)/len(x) for x in latencies], marker='o', color='green', label='Latency (s)')\n",
    "        plt.title(f\"Latency for {category}\")\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Latency (s)')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    #plot_energy_vs_latency(metrics, categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Processing data\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "categories = stem\n",
    "categories = sociology  #sociology is shorter / less prompts\n",
    "#mmlu_dataset = load_mmlu_dataset()\n",
    "data_dict = convert_to_dataframe(categories)\n",
    "print(\"data_dict geladen\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run Experiments\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "bootstrapping = 3  # Number of iterations for each prompt\n",
    "\n",
    "# Collect metrics for each category\n",
    "metrics = collect_metrics_for_categories(data_dict, categories, bootstrapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BACK UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import pynvml\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch.profiler\n",
    "\n",
    "# Specify the GPU device\n",
    "device = \"cuda:0\"  # Change to the appropriate GPU if needed\n",
    "\n",
    "# Initialize NVML for power measurement\n",
    "def initialize_nvml():\n",
    "    pynvml.nvmlInit()\n",
    "\n",
    "def shutdown_nvml():\n",
    "    pynvml.nvmlShutdown()\n",
    "\n",
    "def get_gpu_handle(gpu_index=0):\n",
    "    return pynvml.nvmlDeviceGetHandleByIndex(gpu_index)\n",
    "\n",
    "# Measure GPU power consumption over a period of time\n",
    "def measure_power_consumption(handle, duration_sec=1.0, interval_sec=0.1):\n",
    "    power_readings = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    while (time.time() - start_time) < duration_sec:\n",
    "        power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0  # Convert from mW to W\n",
    "        power_readings.append(power)\n",
    "        time.sleep(interval_sec)\n",
    "    \n",
    "    return sum(power_readings) / len(power_readings) if power_readings else 0\n",
    "\n",
    "\n",
    "#inputs['input_ids'], max_new_tokens=200, do_sample=True\n",
    "# Measure energy consumed during inference\n",
    "def measure_energy_during_inference(handle, inference_function):\n",
    "    power_start = measure_power_consumption(handle, duration_sec=0.5)  # Measure power before inference\n",
    "    \n",
    "    start_time = time.time()  # Start time for inference\n",
    "    result = inference_function()  # Run inference\n",
    "    end_time = time.time()  # End time for inference\n",
    "    \n",
    "    power_end = measure_power_consumption(handle, duration_sec=0.5)  # Measure power after inference\n",
    "    \n",
    "    # Average the power readings before and after inference\n",
    "    avg_power = (power_start + power_end) / 2\n",
    "    elapsed_time = end_time - start_time  # Total inference time\n",
    "    energy_consumed = avg_power * elapsed_time  # Energy consumed in Joules\n",
    "\n",
    "    return energy_consumed, elapsed_time, result\n",
    "\n",
    "# Count FLOPs using PyTorch Profiler\n",
    "def count_flops(model, input_ids):\n",
    "    # Use torch.profiler to count FLOPs\n",
    "    with torch.profiler.profile(\n",
    "        activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],\n",
    "        with_stack=True,\n",
    "        record_shapes=True,\n",
    "    ) as prof:\n",
    "        with torch.no_grad():\n",
    "            model.generate(input_ids)\n",
    "\n",
    "    # Sum the FLOPs from profiler events\n",
    "    flops = sum(event.flops for event in prof.events() if event.flops is not None)\n",
    "    return flops\n",
    "\n",
    "# Calculate perplexity for generated text\n",
    "def calculate_perplexity(model, input_text, tokenizer):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)  # Tokenize input and move to device\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "        perplexity = torch.exp(loss)\n",
    "    return perplexity.item()\n",
    "\n",
    "# Main function to run experiments and measure energy & FLOPs\n",
    "def run_experiment_for_texts(texts, bootstrapping, handle, model, tokenizer):\n",
    "    latencies = []\n",
    "    energy_per_task = []\n",
    "    energy_per_flop = []\n",
    "    generated_texts = []\n",
    "    perplexities = []\n",
    "\n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\").to(device)  # Tokenize and move input to device\n",
    "        text_latencies = []\n",
    "        text_energy_per_task = []\n",
    "        text_energy_per_flop = []\n",
    "        text_generated = []\n",
    "        text_perplexities = []\n",
    "\n",
    "        for _ in range(bootstrapping):\n",
    "            # Count FLOPs for the input\n",
    "            flops = count_flops(model, inputs['input_ids'])\n",
    "\n",
    "            # Measure energy during inference\n",
    "            energy_consumed, latency, output = measure_energy_during_inference(\n",
    "                handle, model.generate(inputs['input_ids'], max_new_tokens=200, do_sample=True)\n",
    "            )\n",
    "            text_latencies.append(latency)\n",
    "\n",
    "            # Energy per Task\n",
    "            text_energy_per_task.append(energy_consumed)\n",
    "\n",
    "            # Energy per FLOP\n",
    "            energy_flop = energy_consumed / flops if flops > 0 else 0\n",
    "            text_energy_per_flop.append(energy_flop)\n",
    "\n",
    "            # Generate text\n",
    "            generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            filtered_generated_text = generated_text.replace(text, \"\").strip()\n",
    "            text_generated.append(filtered_generated_text)\n",
    "\n",
    "            # Calculate perplexity\n",
    "            perplexity = calculate_perplexity(model, text, tokenizer)\n",
    "            text_perplexities.append(perplexity)\n",
    "\n",
    "        latencies.append(text_latencies)\n",
    "        energy_per_task.append(text_energy_per_task)\n",
    "        energy_per_flop.append(text_energy_per_flop)\n",
    "        generated_texts.append(text_generated)\n",
    "        perplexities.append(text_perplexities)\n",
    "\n",
    "    return latencies, energy_per_task, energy_per_flop, generated_texts, perplexities\n",
    "\n",
    "# Function to collect metrics for multiple text categories\n",
    "def collect_metrics_for_categories(df, categories, bootstrapping, model, tokenizer):\n",
    "    category_metrics = {}\n",
    "    handle = get_gpu_handle(gpu_index=0)\n",
    "\n",
    "    for category in categories:\n",
    "        print(f\"Processing category: {category}\")\n",
    "        texts = filter_texts_by_category(df, category)\n",
    "        latencies, energy_per_task, energy_per_flop, generated_texts, perplexities = run_experiment_for_texts(\n",
    "            texts, bootstrapping, handle, model, tokenizer\n",
    "        )\n",
    "        \n",
    "\n",
    "        category_metrics[category] = {\n",
    "            \"latencies\": latencies,\n",
    "            \"energy_per_task\": energy_per_task,\n",
    "            \"energy_per_flop\": energy_per_flop,\n",
    "            \"generated_texts\": generated_texts,\n",
    "            \"perplexities\": perplexities,\n",
    "        }\n",
    "\n",
    "    shutdown_nvml()  # Close NVML after measurements\n",
    "    return category_metrics\n",
    "\n",
    "# Function to filter texts based on category\n",
    "def filter_texts_by_category(df, category):\n",
    "    return df[df['category'] == category]['text'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
