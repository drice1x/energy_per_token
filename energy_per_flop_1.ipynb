{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Patrick/projects/energy_per_token/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import transformers\n",
    "import accelerate\n",
    "#import vllm\n",
    "import bitsandbytes\n",
    "#from vllm import LLM, SamplingParams\n",
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib\n",
    "from collections import Counter\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "#matplotlib.use('TkAgg')\n",
    "#from awq import AutoAWQForCausalLM\n",
    "#from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>How can I improve my time management skills?</td>\n",
       "      <td>generic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>What are the most effective ways to deal with ...</td>\n",
       "      <td>generic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>What are the main differences between Python a...</td>\n",
       "      <td>generic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>How can I increase my productivity while worki...</td>\n",
       "      <td>generic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Can you explain the basics of quantum computing?</td>\n",
       "      <td>generic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>76</td>\n",
       "      <td>Write a script for a YouTube video exploring t...</td>\n",
       "      <td>writing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>77</td>\n",
       "      <td>Compose an engaging travel blog post about a r...</td>\n",
       "      <td>writing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>78</td>\n",
       "      <td>Write a captivating movie review for a recentl...</td>\n",
       "      <td>writing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>79</td>\n",
       "      <td>Structure a podcast script for an episode disc...</td>\n",
       "      <td>writing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>80</td>\n",
       "      <td>Write a symphony concert review, discussing th...</td>\n",
       "      <td>writing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    question_id                                               text category\n",
       "0             1       How can I improve my time management skills?  generic\n",
       "1             2  What are the most effective ways to deal with ...  generic\n",
       "2             3  What are the main differences between Python a...  generic\n",
       "3             4  How can I increase my productivity while worki...  generic\n",
       "4             5   Can you explain the basics of quantum computing?  generic\n",
       "..          ...                                                ...      ...\n",
       "75           76  Write a script for a YouTube video exploring t...  writing\n",
       "76           77  Compose an engaging travel blog post about a r...  writing\n",
       "77           78  Write a captivating movie review for a recentl...  writing\n",
       "78           79  Structure a podcast script for an episode disc...  writing\n",
       "79           80  Write a symphony concert review, discussing th...  writing\n",
       "\n",
       "[80 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Multitask Benchmark datenset json\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "file_path = \"question.jsonl\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "    #for line in f:\n",
    "    #    data = json.loads(line)\n",
    "    #    print(data)\n",
    "df_mtconversation = pd.DataFrame(data)\n",
    "df_mtconversation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['generic' 'knowledge' 'roleplay' 'common-sense' 'fermi' 'counterfactual'\n",
      " 'coding' 'math' 'writing']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "file_path = \"question.jsonl\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "    #for line in f:\n",
    "    #    data = json.loads(line)\n",
    "    #    print(data)\n",
    "df_mtconversation = pd.DataFrame(data)\n",
    "df_mtconversation\n",
    "\n",
    "# Categories:\n",
    "print(df_mtconversation.category.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpu_power(gpu_index=1): #measuring for gpu1\n",
    "    \"\"\"Fetches the current power consumption of the GPU using nvidia-smi.\"\"\"\n",
    "    result = subprocess.run(['nvidia-smi', '--query-gpu=power.draw', '--format=csv,noheader,nounits', '-i',str(gpu_index)], \n",
    "                            stdout=subprocess.PIPE, text=True)\n",
    "    power = float(result.stdout.strip())  # Power in watts\n",
    "    return power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hier mit simplem MT_Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (NEW) Energy per flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing category: coding\n",
      "Processing category: math\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch\n",
    "import subprocess\n",
    "import pynvml\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Specify the GPU device you want to use\n",
    "device = \"cuda:0\"  # Change this to your preferred GPU\n",
    "\n",
    "# Load model and tokenizer on the specified device\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")\n",
    "\n",
    "# Initialize NVML for power measurement\n",
    "def initialize_nvml():\n",
    "    pynvml.nvmlInit()\n",
    "\n",
    "def shutdown_nvml():\n",
    "    pynvml.nvmlShutdown()\n",
    "\n",
    "def get_gpu_handle(gpu_index=0):\n",
    "    return pynvml.nvmlDeviceGetHandleByIndex(gpu_index)\n",
    "\n",
    "def measure_power_consumption(handle, duration_sec=1.0, interval_sec=0.1):\n",
    "    power_readings = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while (time.time() - start_time) < duration_sec:\n",
    "        power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0  # Convert from mW to W\n",
    "        power_readings.append(power)\n",
    "        time.sleep(interval_sec)\n",
    "    \n",
    "    return sum(power_readings) / len(power_readings) if power_readings else 0\n",
    "\n",
    "def measure_energy_during_inference(handle, inference_function, *args, **kwargs):\n",
    "    power_start = measure_power_consumption(handle, duration_sec=0.5)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    result = inference_function(*args, **kwargs)  \n",
    "    end_time = time.time()\n",
    "    \n",
    "    power_end = measure_power_consumption(handle, duration_sec=0.5)\n",
    "    \n",
    "    avg_power = (power_start + power_end) / 2\n",
    "    elapsed_time = end_time - start_time  \n",
    "    energy_consumed = avg_power * elapsed_time  \n",
    "    \n",
    "    return energy_consumed, elapsed_time, result\n",
    "\n",
    "def calculate_perplexity(model, input_text, tokenizer):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)  # Ensure input is on the same device\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "        perplexity = torch.exp(loss)\n",
    "    return perplexity.item()\n",
    "\n",
    "def run_experiment_for_texts(texts, bootstrapping, handle):\n",
    "    latencies = []\n",
    "    energy_per_token = []\n",
    "    throughputs = []\n",
    "    generated_texts = []\n",
    "    perplexities = []\n",
    "\n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\").to(device)  # Ensure input is on the same device\n",
    "        text_latencies = []\n",
    "        text_energy_per_token = []\n",
    "        text_throughput = []\n",
    "        text_generated = []\n",
    "        text_perplexities = []\n",
    "\n",
    "        for _ in range(bootstrapping):\n",
    "            energy_consumed, latency, output = measure_energy_during_inference(handle, model.generate, inputs['input_ids'], max_new_tokens=200)\n",
    "            text_latencies.append(latency)\n",
    "\n",
    "            output_tokens = output.size(-1)\n",
    "            energy_token = energy_consumed / output_tokens if output_tokens > 0 else 0\n",
    "            text_energy_per_token.append(energy_token)\n",
    "\n",
    "            throughput = output_tokens / latency\n",
    "            text_throughput.append(throughput)\n",
    "\n",
    "            perplexity = calculate_perplexity(model, text, tokenizer)\n",
    "            text_perplexities.append(perplexity)\n",
    "\n",
    "            generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            filtered_generated_text = generated_text.replace(text, \"\").strip()\n",
    "            text_generated.append(filtered_generated_text)\n",
    "\n",
    "        latencies.append(text_latencies)\n",
    "        energy_per_token.append(text_energy_per_token)\n",
    "        throughputs.append(text_throughput)\n",
    "        generated_texts.append(text_generated)\n",
    "        perplexities.append(text_perplexities)\n",
    "\n",
    "    return latencies, energy_per_token, throughputs, generated_texts, perplexities\n",
    "\n",
    "def collect_metrics_for_categories(df, categories, bootstrapping):\n",
    "    category_metrics = {}\n",
    "    handle = get_gpu_handle(gpu_index=0)\n",
    "\n",
    "    for category in categories:\n",
    "        print(f\"Processing category: {category}\")\n",
    "        texts = filter_texts_by_category(df, category)\n",
    "        latencies, energy_per_token, throughputs, generated_texts, perplexities = run_experiment_for_texts(texts, bootstrapping, handle)\n",
    "\n",
    "        category_metrics[category] = {\n",
    "            \"latencies\": latencies,\n",
    "            \"energy_per_token\": energy_per_token,\n",
    "            \"throughput\": throughputs,\n",
    "            \"generated_texts\": generated_texts,\n",
    "            \"perplexities\": perplexities\n",
    "        }\n",
    "\n",
    "    shutdown_nvml()  \n",
    "    return category_metrics\n",
    "\n",
    "def filter_texts_by_category(df, category):\n",
    "    return df[df['category'] == category]['text'].values\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Example Usage\n",
    "file_path = \"../projects/question.jsonl\"\n",
    "bootstrapping = 2  \n",
    "df_mtconversation = load_dataset(file_path)\n",
    "\n",
    "categories = [  'coding', 'math']\n",
    "\n",
    "initialize_nvml()\n",
    "\n",
    "metrics = collect_metrics_for_categories(df_mtconversation, categories, bootstrapping)\n",
    "\n",
    "# (Optionally, you can visualize the collected metrics here)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABKUAAAJRCAYAAACUbgR+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAC70UlEQVR4nOzdd1xW5f/H8dctGxRQQRFFcY/cpuYM98iVm9w5GlqZpV/9Zo6WZctKy2+ZqzKzHJm5TTNHztQcuXIn7gWoKFy/P86PG+8ABQRu1Pfz+7gfcq5znet8zrlv4Mun6/ocmzHGICIiIiIiIiIikomyOTsAERERERERERF58CgpJSIiIiIiIiIimU5JKRERERERERERyXRKSomIiIiIiIiISKZTUkpERERERERERDKdklIiIiIiIiIiIpLplJQSEREREREREZFMp6SUiIiIiIiIiIhkOiWlREREREREREQk0ykpJSKSBYSGhmKz2bDZbLzwwgu37fvuu+/a+7q6umZShClz+PBhbDYboaGhzg4lWcuWLaNXr16UKFECX19fPDw8yJcvH40aNeLDDz/kzJkzzg7xnnIvvOd3a86cOfbvuZdeesnZ4WRZPXv2tN+n1LwOHz6cYTGtWrUKm81GWFhYuo9948YNpkyZQps2bShYsCBeXl54e3tTpEgR2rdvzzfffENMTEy6n/d+F/85mjp1qrNDERGRTJC1/poRERG++eYb3n33Xdzd3ZPcP3ny5HQ/5+HDhylcuDCFChXK0D8Qnens2bOEh4ezfPlywEoE1qtXDx8fHyIiIli3bh3Lly9nxIgRLF++nOrVqzs5YskqvvzyS/vXX3/9NW+//TZubm5OjChrql27dpLtP/zwA1FRUdSqVYtixYol2p89e/aMDi3dbd26lfbt23Po0CFsNhsVKlSgWrVqZMuWjcOHDzNv3jxmz57NK6+8wu7du/H29r6r89lsNgCMMekRvoiISJahpJSISBby8MMPs3nzZn788Uc6dOiQaP+6dev466+/qFq1Kps2bXJChLeXP39+9uzZk+X+YL906RK1a9dm7969lCpVis8//5w6deo49Ll+/TrTpk1j5MiRnDx50kmR3nuy6nueXk6cOMGSJUtwcXEhMDCQiIgIfvrpJ9q2bevs0LKcPn360KdPn0Ttq1atIioqij59+tCzZ8/MDyydbd26lTp16hAdHU2LFi34+OOPKVy4sEOfM2fO8OGHH/L+++8TExNz10mpB8mYMWMYOnQo+fLlc3YoIiKSCbR8T0QkC3nyySeB5GdDxc/YiO+X1bi5uVGqVCmKFi3q7FAcPPfcc+zdu5fQ0FDWrl2bKCEF4OHhQb9+/di2bRulS5d2QpT3pqz6nqeXqVOnEhsbS+PGjXn66acBx5lT8mC5ceMGHTp0IDo6mjZt2vDjjz8mSkgBBAYG8tZbb7FmzRo8PDycEOm9K1++fJQqVQo/Pz9nhyIiIplASSkRkSykXLlyPPzwwyxdupQTJ0447IuMjGTWrFkUKFCAxo0b33acmzdvMmnSJMLCwsiVKxceHh4ULlyYZ555hmPHjjn07dmzp/2PqiNHjiSq9xJv1KhR2Gw2Ro0axdGjR+nduzchISG4ubnZZz/cqb5QdHQ048aNo3bt2uTMmRMPDw8KFSpEy5YtmTFjhkPfS5cuMXz4cMqVK4ePjw8eHh4EBwdTq1YtRowYwY0bN1JyS/n777/tY3/wwQfkypXrtv3z5s1LyZIlE7XPnDmTBg0a2O9noUKFePLJJ9m3b1+S48TXCTt8+DCLFi0iLCwMPz8/cubMSYsWLfjzzz/tfWfMmEGNGjXIkSMH/v7+tG3bloMHDyYa89b6ONHR0fz3v/+lWLFieHp6EhwcTO/evRN9buItX76c5557jooVKxIQEICHhwcFChSgU6dOyc66u9v3fP/+/Tz55JMULlwYDw8PsmfPTqFChXjssceYMmVKkudcsmQJLVq0IE+ePLi7uxMcHEynTp3YvHlzkv3DwsKw2WysWrWKbdu20bZtW/v1lSlThvfffz/NS56MMfYEce/evenVqxfZsmVjyZIlyd7neL/88gsdOnSgQIECeHh4EBgYSNWqVRk5ciTnzp2z95s6dSo2m42ePXty/vx5Bg4cSNGiRfHw8HCog3Tz5k0mTpxIzZo18fPzw9PTk+LFi/P8888nG0tq7//3339Pw4YNyZ07N25ubuTOnZsyZcrQt29fduzYkYY7eHtXrlzhiy++oG3bthQvXhwfHx98fHwoV64cr7zyChcvXkzyuJMnT/LCCy9QokQJPD098fb2JiQkhAYNGvDee++l+PxnzpyhZs2a2Gw2OnXqxPXr1+94zIwZM/j7779xd3fns88+I1u22/9f6apVq+Ll5WXfPnLkCO+88w7169enYMGCeHh44O/vT+3atfnf//5HXFycw/Hx34Px7lSPa9++fTz11FMULVoUT09P/Pz8qFu3Ll9//XWyMZ47d47nn3/eHk+hQoUYOHAgFy9evG19p7R8Jm/9vTJlyhRq1KiBn5+fw7XcqabUli1b6NKliz3eXLly0aRJExYuXJhk//T6vIiISAYxIiLidIUKFTKA+e2338ynn35qAPPGG2849Pnyyy8NYF555RVz6NAhAxgXF5dEY12+fNmEhYUZwGTPnt08+uijpn379qZkyZIGMLlz5zZbt2619//iiy9Mu3btDGB8fHxMjx49HF7xRo4caQDzxBNPmFy5cpmgoCDTrl0707ZtW/PSSy8ZY4w9rkKFCiWK6+jRo6ZMmTIGMN7e3qZRo0amc+fOpk6dOsbPz8/hmKioKFO2bFkDmMDAQNOyZUvTuXNnExYWZoKCggxgLly4kKJ7+9FHHxnA+Pv7m5s3b6bomFvFxcWZ7t27G8C4urqa+vXrm86dO5sSJUrYr2XRokWJjot/T4cOHWpsNpupVauW6dixo/04f39/c+DAATN48GD7uO3btzchISEGMMHBweb8+fMOY65cudIApkaNGuaRRx4x3t7epnnz5qZDhw4mX758BjBBQUFm3759ieIpWrSocXd3N5UqVTKtWrUybdu2tb8frq6u5ocffkh0zN2853/++afx9fU1gClZsqRp27at6dChg6lRo4bJnj27qVChQqLzDR8+3AD2+xUeHm4qVqxo/6x/+eWXiY559NFH7ffZ3d3dlC5d2nTu3Nk8+uijxsXFxQDmhRdeuM07nLwVK1YYwAQEBJiYmBhjjDGNGjUygHnzzTeTPe65554zgAFMxYoVTefOnU2zZs1MkSJFDGBWrlxp7ztlyhQDmMcee8wULlzY5MyZ07Rq1cp06NDBdOnSxRhjzLVr10zDhg0NYDw9PU2zZs1Mp06d7J+VgIAAs2XLFocYUnv/R48ebf8s1K1b14SHh5vmzZubsmXLGpvNZj788MM03UNjEr4XpkyZ4tD+22+/2b/Ha9eubTp16mQaN25scufObQBTrFgxc/bsWYdjTp48aYKDgw1gChYsaFq3bm06depk6tSpY3LlymX8/Pwc+sd/zzz66KMO7Xv37jVFixY1gBkyZIiJi4tL0bU8/vjjBjAtW7ZM7W0wxhjz+uuvG8AULlzYNGjQwP5ZdXd3N4Bp27atQyxz5841PXr0sH+e/v3z+cyZM/a+s2bNMp6engYwpUqVMo8//ripX7++8fHxMYDp1atXonj++ecf+33IlSuXadu2rWnTpo3JmTOnKVmypGnTpk2S711aPpPGGPt1DBgwwGTLls3Url3bhIeHm+rVq5vDhw8bY4z9ev99TmOMGTdunMmWLZv9e6t9+/amdu3a9vs3evRoh/6p/byIiEjmU1JKRCQLuDUpdfHiRePl5WWKFSvm0KdWrVrGZrOZgwcP3jYp9cQTTxjAtGjRwpw6dcph34cffmgAU7x4cYcEze2SSfHiExSA6dq1q7l27VqiPsmNExsbax5++GEDmMaNG5vTp0877L969ar5+eef7dvTpk0zgGnWrJk9GXDrWKtWrTLXr19PNtZbdevWzQCmfv36Ker/b5999pn9j6w//vjD3h4XF2e/J/7+/omuKf499fDwMMuXL7e337x503To0MEApmzZsiZ37txm27Zt9v1RUVGmZs2aSSYm4//Ajv+D/ciRI/Z9V69etScXH3nkkUTXMXfu3ERJrvh2V1dXkzt3bhMdHe2w727e8169eiV5DcYYEx0dbX799VeHtkWLFtn/wF26dKnDvkmTJhnAuLm5mZ07dzrsi09KAWbixIkO+1asWGFsNptxcXExx44dSxTHncR/Lw0cONDe9u233xrAFC1aNMlExscff2xP/v7yyy+J9m/YsMEcPXrUvh2flAJMgwYNzKVLlxId85///Md+zkOHDtnbY2JiTO/eve1Jjlu/J1Jz/69du2a8vLxM9uzZzV9//ZWo/+HDh82ePXuSuEMpk1xS6tixY2b58uUmNjbWoT0qKsqeCH722Wcd9sUnz/r165fo/sfExDh8rxmTdFJq9erVJleuXMbFxSXRZ+ZO4pMur732WqqOi7dx40bz559/Jmo/ceKEqVChggHMrFmzEu2P/4wkZ8eOHcbDw8N4enqa2bNnO+w7fPiwKVeunAHMtGnTHPbFJ9nCwsIcPnsXLlwwtWvXtp/33+9dWj6Tt16Hr6+vWb9+fZLXklxSavHixcZms5mAgIBEPz927NhhChQoYACzatUqe3tqPy8iIpL5lJQSEckCbk1KGWNMly5dHP7P9V9//WX/w8EYk2xSavfu3cZms5ng4GBz+fLlJM/VvHlzA5iffvrJ3paapFSuXLnMxYsXk+yT3Djz5s0zgMmXL5+5cuXKbe+FMcaMHTvWAOaDDz64Y987adq0qQFM586d03R8/CyCjz/+ONG+uLg4U758+SRnzsS/p4MHD0503NatW+1/nE2YMCHR/tmzZxvA1KtXz6H91qTUvHnzEh136tQp4+3tbQCzdu3aFF9jeHi4ARwSg8bc3Xse/zm7dVbe7TRo0MAAZtCgQUnub9GihQFM3759Hdrjk1Jt27ZN8rj493/69OkpiiPehQsX7LNObk0iXLt2zeTKlSvRjCdjjLlx44YJDAw0QKLEQHLik1Jubm7m4MGDifZfvXrVZM+e3QBm/vz5ifZHRUWZvHnzGsB888039vbU3P/Tp08bwJQvXz5FMadWckmp24mKijKurq4mMDDQof3ZZ581gJkzZ06Kxvl3UmrGjBnGw8PDZM+e3SxcuDDF8cSL/0ykNpmVEkuWLDGA6dChQ6J9d0pKderUyQDmvffeS3L/xo0bDWCqVKlibzt8+LCx2WwmW7ZsSSYd//zzT2Oz2RK9d2n9TN56HbdL6iWXlKpevboBkpzVaYw1Uwww7dq1s7el9vMiIiKZTzWlRESyoH8XPI//904FzhcuXIgxhmbNmpEjR44k+8TXqVm3bl2aYmvYsGGqC9AuXrwYgCeeeCJFj3+vWrUqAGPHjmX69OmcP38+9YGmg+PHj9trO/Xo0SPRfpvNRq9evQBYuXJlkmM0b948UVvx4sVTtP+ff/5Jckx/f39atWqVqD1Pnjw0bdoUsOpP/ds///zDF198wUsvvWR/ElrPnj3ZtWsXAHv37k3yfGl5z6tVqwbAM888w5IlS7h27VqyfW/evMnatWsBkn06W+/evYHk73PLli2TbI8vWn+nGlD/9vXXX3Pt2jWqVq1K2bJl7e0eHh488cQTQOKC51u2bOHMmTMEBATw+OOPp+p8lSpVokiRIonaN2/eTGRkJLly5UryGr29vencuTPgeG9Sc/8DAwMJDQ1lx44dvPTSS+zevTtVsd+tdevW8c4779C/f3969epFz549efbZZ3F3d+fMmTNcuHDB3jf+uoYOHcqcOXOIjIxM8XneeustunTpQu7cufntt99o1qxZul9LSly/fp2ffvqJESNG8PTTT9uv+X//+x+Q/PdhcuLi4li0aBEAnTp1SrLPww8/TPbs2fnjjz/sn4XffvsNYwyVK1emVKlSiY4pW7Ys5cuXT9Se1s/krdq3b5+yi/t/Z8+eZePGjXh5eSX7vZ7U77a7+byIiEjmcHV2ACIikli9evUoXLgwP/zwA+PGjWP69On4+vre8f/I//3334D1x/KdnhB25syZNMWWXBHz2zly5AhAkn/4JCUsLIz//Oc/vPvuu/To0QObzUbx4sWpVasWrVu3pmXLlncsMBwvMDAQgNOnT6c67vhERu7cufH19U2yT/xT55JLehQsWDBR262JuaT2xycUk0skxBdRT0p80frjx487tI8ePZo333zztgXiL1++nOz5Umvw4MGsWbOG5cuX07RpU9zc3KhQoQJ169alc+fO9sQjWIWW4681qSeZQdruM2B/326XlEnK7Z50+eSTTzJ+/Hhmz57N+PHj7Qm7+M95yZIlk31/kpPcPY6/3uTuCyR9b1Jz/wGmT59O+/bt+eCDD+wPBKhevTqNGjWiW7duBAQEpOp6UuL06dO0a9eONWvW3Lbf5cuXyZkzJwDdunVj2bJlfPPNN7Rr1w4XFxfKlClD7dq1ad++PfXr109yjLVr1/Lrr7/i6enJ6tWr0/y0yMDAQI4dO5amnycAv//+O506deLo0aPJ9knu+zA5586dsx8TEhKSov758+e3/4y43fd3aGgo27dvd2hL62fy3+OmxqFDhzDGcPXq1Ts+zfDW321p/byIiEjmUVJKRCQLin8a18iRI+nRowcRERH069fP4SlOSYl/clPFihWpUKHCbftWr149TbHdKYb08vbbb/P000/z008/sWbNGtauXcuUKVOYMmUKVatWZeXKlfj4+NxxnCpVqvDVV1+xdetWYmNjcXFxyYToE9wpeZbS5FpqmVueODdnzhxGjRpF9uzZGT9+PPXr1yc4OBgvLy9sNhv//e9/GTNmTLJPqUvLe+7t7c2yZcvYtGkTixcvZt26daxbt47NmzfzwQcf8OyzzzJhwoQ0X9+/ped93Lp1K9u2bQPg888/T/LJZdmyZePq1at8++23PP3003d9zvT+vkrt/a9Tpw6HDx/m559/5tdff2XdunUsWbKERYsWMXLkSObOnUuDBg3SNcY+ffqwZs0aatSowejRo6lQoQI5c+bEzc0NgODgYE6ePOnwucyWLRtff/01//3vf/n5559Zu3Yta9eu5bPPPuOzzz6jZcuWzJ07N9H3+UMPPYSbmxubN2/mueeeY/bs2Wm651WqVOHYsWPJPrHydqKjo2nTpg2nTp2iV69ePPPMMxQrVgxfX19cXFzYt28fJUuWTPXTIm99Yl9SMzr/7d9JndslUFObXE2p1N77+GvMnj077dq1S/Fxaf28iIhI5lFSSkQki+rZsyejR4/mp59+Au68dA8S/it5rVq1GD9+fIbGlxrxs1j++uuvVB0XGhrKc889x3PPPQfApk2b6Nq1K5s2bWLs2LGMHj36jmO0aNGCQYMGcfHiRebPn5+qZVX58+cHEmYiJDVbKn52WnzfzPDvx8Anta9AgQL2tlmzZgHw5ptv0q9fv0TH7N+/P13ju1XVqlXts3Ju3rzJvHnz6N69O59++int27enXr165M6dGw8PD65fv87ff/+d5JKhzLzPt84y/OOPP+7YNz4pFf8537dvH8aYdPmDPv56Dx06lGyf292blNz/eF5eXrRv394+I/PMmTMMHz6czz//nCeffNI+Eyw9REVFsXDhQrJly8bChQvx9/dPtD8iIiLZ48uUKUOZMmUYPHgwxhh++eUXnnjiCX766SemT59uX1Ybz9/fn/nz59OiRQsWLVpEs2bNWLBgQYqWE9+qdevWzJs3jyVLlnDq1Cny5s2b4mNXr17NqVOnqFy5sn1J9q3S+n0YEBCAl5cXV69e5b333kvxrLb4z0tKfp4kdVxaP5NpEf+7zWazMXny5FQnoVP7eRERkcyjmlIiIllUwYIFad26Nblz5+aRRx5J0cym+Bop8+fPT9VyJXd3d8D6ozUjxNc5+vbbb4mKikrzOFWrVuXZZ58FsM9kuZOiRYsSHh4OwEsvvXTH+lSnT5+213QpUKCAfRnK1KlTE/U1xtjbb/3jPqNdvHjRnqy81ZkzZ+z1u+LrqwD2ay5UqFCiY06fPs2yZcsyJtB/cXV1pX379jRp0gRIeA9dXV2pXbs2kPR9hoS6ahl9n69evcqMGTMAWLRoEcZ6KEyi14ULF/Dw8GDz5s3s2LEDsOr2BAQEcObMGebNm5cu8cTXAjp//jzz589PMt6ZM2cCd743yd3/5AQGBjJ27FgAjh496lDb6W5dunSJ2NhYfH19EyWkwKrpldIZQzabjQYNGthrfSV3Xb6+vixevJjGjRvz66+/0rBhw1RfU5cuXQgNDSUmJoZnnnnGYZZSUrZs2cLVq1eBhO/D5JaaJjUjL1787LGkfka7uLjQqFEjICEBnRJ16tTBZrOxZcsW9u3bl2j/7t27Ey3dg/T9TKZUcHAw5cuX58qVK/afcWmV0s+LiIhkDiWlRESysDlz5nD27FnWr1+fov6VKlWiXbt2HDt2jLZt2yb5X7mjoqL45ptvOHXqlL0tMDAQd3d3IiIiMqSoeKtWrahUqRL//PMPHTp04Ny5cw77r127Zi/UCzB37lxWr16d6A++Gzdu2P8gSSrBkpxPPvmEYsWKcejQIWrXrp1kDZuYmBgmT55MpUqV2LNnj7395ZdfBuD11193+APNGMMbb7zBtm3b8Pf3p2/fvimOJz289NJLDnWjrl+/Tv/+/YmKiqJatWrUqlXLvi++2Pfnn39OTEyMvf3SpUv06NGDS5cupXt8n376aZIFmyMiIti8eTPg+B6+9NJLAHz22WesWLHC4ZipU6cyf/583NzceOGFF9I91lvNnj2bixcvki9fPvsf+knx9/e3F1yOT5i5urryyiuvANCvXz9Wr16d6LhNmzYlqvd1O56envTv3x+w7tGts5Vu3LjBCy+8QEREBIULF3aoOZea+3/kyBEmTZqUZC2j+ORnzpw5k62rlhZ58+YlZ86cXLx4ka+++sph3++//86wYcOSPG769Ols2bIlUfuVK1fsxf1v97PB29ubn376ibZt27JhwwbCwsIcfhbeiZubG7NmzcLT05O5c+fSpk2bJGcMnT9/nldffZVatWpx/fp1IOH7cMWKFYmKyX/++ed89913yZ43fuZj/EMJ/m3kyJG4u7szePBgpk2blmSybOfOncyZM8e+HRoaSsuWLYmLi+OZZ57hypUr9n2XLl3imWeeSTIxmNbP5N164403AOjVq1eSSXljDBs2bGDp0qX2trv9vIiISCbI1Gf9iYhIkuIfmf7bb7+lqP+hQ4cMYFxcXBLtu3z5smnQoIEBjLu7u6latarp2LGj6dChg6latapxd3c3QKJHgLdv394AJiQkxISHh5vevXub3r172/ePHDnSAGbkyJF3jKtQoUKJ9h0+fNiULFnSAMbb29s0btzYhIeHm7p16xo/Pz+HY1544QUDmICAANOoUSPTpUsX06pVK5MnTx4DmPz585tjx46l6F7FO3XqlAkLC7M/krxw4cKmdevWJjw83NSvX9/+iHNfX1+zYcMG+3FxcXGmW7duBjCurq6mQYMGJjw83H4tXl5eST5aPv49PXToUJLxxMeRmvsY/3j7GjVqmOrVqxtvb2/TokUL07FjRxMcHGwAkydPHvPXX385HPf3338bf39/+71r166dadWqlfHz8zP58uUzTz75ZJLv7d285xUqVLDf55YtW5ouXbqYxo0bGy8vLwOY+vXrmxs3bjgcM3z4cAMYm81mateubZ544glTuXJl+2f9yy+/THT+Rx991ABm5cqVScaXkmu4VfxnZPDgwXfsO3/+fAOY3Llzm+vXrxtjrM/L008/bX9/K1WqZDp37myaN29uihQpkijWKVOmGMD06NEj2fNcu3bN/j3t5eVlmjdvbjp16mQKFixoP//mzZsdjknN/f/jjz8MYNzc3Ow/Lzp27GgqVapkfz8mTZqUovuXlPjvhSlTpji0f/jhh/b7VL16dRMeHm5q1aplbDab6datW5LfQ61btzaACQ4ONs2bNzddunQxzZs3N35+fgYwZcuWNZcvX7b3j/+eefTRRx3OffPmTfv3dYkSJczRo0dTdU0bN260x2ez2UzlypVN+/btTceOHU316tWNi4uLAUyRIkVMdHR0ovjd3d1N48aNTefOnU2pUqWMzWYzr7zySrI/P19++WX7z8SOHTvafz6fPXvW3mfWrFnG29vbAKZAgQKmcePGpkuXLqZZs2amQIECBjCdOnVyGPfEiRMmNDTU/jlq27atefzxx02uXLlM8eLFTatWrQxgvvnmG4fj0vKZNOb2P/fi9ejRI8nPizHGfPTRR8bV1dUAplixYuaxxx4zTzzxhGnUqJH998N//vOfRPc7pZ8XERHJfEpKiYhkAemZlDLGmNjYWDNjxgzTvHlzkzdvXuPm5mZy585typYta3r16mXmzp1rYmJiHI45d+6ceeqpp0zBggWNm5tboj8e7jYpZYwxV65cMe+8846pWrWqyZEjh/Hw8DCFChUyrVq1MjNnzrT3++OPP8zQoUNN7dq1Tf78+Y27u7sJDAw0VapUMW+99ZbDH2KptWjRItO9e3dTrFgxkz17duPm5maCgoJMo0aNzLhx48y5c+eSPG7GjBkmLCzM+Pv7Gzc3NxMSEmJ69uyZKAEULyOTUo8++qiJjIw0gwcPNoULFzbu7u4mb968pmfPnsn+cX3o0CHTpUsXU7BgQft9f/rpp01ERESy7+3dvOcLFiwwzzzzjKlUqZIJDAw07u7upkCBAiYsLMxMmzYt0ecv3qJFi0zz5s1N7ty5jaurqwkKCjIdOnRwSBTeKj2TUgcOHDA2m80AZufOnXfsf+PGDRMYGGgA89133yW6jtatW9u//wIDA021atXM6NGjHT5jKUlKxZ/r008/NY888ojJkSOHcXd3N0WLFjXPPfecOX78eKL+qbn/ly9fNuPGjTOPP/64KV68uMmePbvx8fExJUqUMN27d08yuZAaySWljDFm3rx5pmbNmsbf399kz57dPPzww+bTTz81cXFxSX4PrV692gwcONBUq1bNBAUFGXd3dxMUFGRq1KhhPvnkExMZGekwfnJJKWOsBOIzzzxj//zu378/Vdd1/fp1M2nSJNOyZUuTP39+4+HhYTw9PU3hwoVN+/btzbfffpvocx4TE2PeffddU65cOePt7W1y5cplGjdubJYuXXrbn59Xr141Q4YMMcWKFbP/h4Wkfr4cOnTIvPjii6Zs2bLGx8fHeHp6mkKFCpmwsDDz9ttvmwMHDiQa+/Tp06Z///6mQIECxt3d3YSEhJj+/fubc+fOmfr16xvALFmyJNFxqf1MGnP3SSljjPnzzz9Nv379TPHixY2np6fx9vY2RYoUMU2aNDEff/yxOXHihL1vaj8vIiKS+WzGpPIRHyIiIuIUq1atol69ejz66KP2pSciIhnh4sWLFClShEuXLnHq1KkUF1AXERFJDdWUEhERERF5QG3cuDFR25kzZ+jRowcXLlygRYsWSkiJiEiGcXV2ACIiIiIi4hzVq1enQIEClC5dmty5c3PixAn++OMPIiMjKViwIOPHj3d2iCIich9TUkpERERE5AE1fPhwVqxYwfbt27lw4QLu7u4ULVqUFi1aMGjQIHLnzu3sEEVE5D6mmlIiIiIiIiIiIpLpVFNKREREREREREQynZJSIiIiIiIiIiKS6ZSUEhERERERERGRTKeklIiIiIiIiIiIZDolpUREREREREREJNMpKSUiIiIiIiIiIplOSSkREREREREREcl0SkqJiIiIiIiIiEimU1JKREREREREREQynZJSIiIiIiIiIiKS6ZSUEhERERERERGRTKeklIiIiIiIiIiIZDolpUREREREREREJNMpKSUiIiIiIiIiIplOSSkREREREREREcl0SkqJiIiIiIiIiEimc3V2AFlRTEwMS5cuJTQ0FBcXF2eHIyIiIiIiIuJ0cXFxnD59mtq1a+Pm5ubscO5pxhiuXLlCjhw5sNlszg7HaZSUSsLSpUtp2bKls8MQERERERERyXJ++eUX6tWr5+ww7mlXrlzBz8+PS5cu4evr6+xwnEZJqSSEhoYC8NNPP1G0aFHnBiMiIiIiIiKSBURERFC/fn2KFCni7FDkPqGkVBLil+wVLVqU0qVLOzkaEREREREREefLkSMHgMrcSLpRoXMREREREREREcl0SkqJiIiIiIiIiEimU1JKREREREREREQynZJSIiIiIiIiIiKS6ZSUEhERERERERGRTKeklIiIiIiIiIiIZDolpUREREREREREJNMpKSUiIiIiIiIiIplOSSkREREREREREcl0SkqJiIiIiIiIiEimU1JKREREREREREQynZJSIiIiIiIiIiKS6ZSUEhERERERERGRTKeklIiIiIiIiIiIZDolpUREREREREREJNMpKSUiIiIiIiIiIplOSSkREREREREREcl0SkqJiIiIiIiIiEimU1JKREREREREREQynZJSIiIiIiIiIiKS6ZSUEhERERERERGRTOfq7AAkfdhszo5AnGKU3vgHkRnl7AjEKYxxdgQiIiIiIulKM6VERERERERERCTTKSklIiIiIiIiIiKZTkkpERERERERERHJdKopJSIiIiKSxdhGq27kg8iMVP1AEXmwaKaUiIiIiIiIiIhkOiWlREREREREREQk0ykpJSIiIiIiIiIimU5JKRERERERERERyXRKSomIiIiIiIiISKbT0/dEREREsjCbHsL2YBrl7ABEREQynmZKiYiIiIiIiIhIplNSSkREREREREREMp2SUiIiIiIiIiIikumUlBIRERERERERkUynpJSIiIiIiIiIiGQ6JaVERERERERERCTTKSklIiIiIiIiIuluzG9jqPpFVXKMyUGed/PQZmYb9p7de8fjvt/1PaXGl8LzDU/KfVaOhfsXOuw3xjBi5QjyvZ8Prze9aDi9IfvP7c+oy5AMpKSUiIiIiIiIiKS7X4/8Sv+q/fm99+8s67aMG3E3aPx1Y6JiopI9Zt2xdYTPDqd3pd788dQftCnZhjYz27Dz9E57n7Frx/Lxho+Z+NhENvTZgI+7D02+bsK1m9cy47IkHSkpJSIiIiIiIiLpbnHXxfSs2JOH8jxEhaAKTG09laOXjrLl5JZkj/low0c0LdaUwbUGUzqwNK/Xf53K+SozfuN4wJolNW7DOIbXHU7rUq0pn7c809tM558r/zDvr3mZdGWSXrJEUmrCBAgNBU9PqF4dNm5M2XEzZ4LNBm3aOLYbAyNGQL584OUFDRvCfs3kExEREREREXGaS9cvAZDLK1eyfdYfW0/DIg0d2poUbcL64+sBOHTxEBGREQ59/Dz9qF6gOuuPrc+AqCUjOT0p9d13MGgQjBwJW7dChQrQpAmcPn374w4fhpdfhjp1Eu8bOxY+/hgmToQNG8DHxxrzmmbyiYiIiIiIiNyVK1eucPnyZfvr+vXrdzwmzsQxcPFAaoXUomyessn2i4iMIK9PXoe2vNnzEhEZYd8PJO7jk5eIqIjUXoo4mauzA/jgA+jbF3r1srYnToSff4bJk2Ho0KSPiY2FLl1g9Gj47Te4eDFhnzEwbhwMHw6tW1tt06dD3rwwbx507pyBFyMiIiIicp/zdvEmwDMAGzZnh3Lfuab/ii7JcHNzw8XFxdlh2JUpU8Zhe+TIkYwaNeq2x/T/uT87T+9kzZNrMjAyudc4NSkVEwNbtsCwYQlt2bJZy+3W32bW3WuvQZ480Lu3lZS61aFDEBFhjRHPz89aFrh+fdJJqevXrztkdiMjI9N4RSIiIiIi9ycbNnoV60WrQq1wd3FXUioDHDp0yNkhSBbm7+9PUFAQNpvzv/d2795N/vz57dseHh637T9g4QAW7F/A6p6rKeBb4LZ9g7IHcSrqlEPbqchTBGUPsu8HOBV1inw58iX0iTpFxbwVU3MZkgU4NSl19qw16ymv46w78uaFv/5K+pg1a+DLL2HbtqT3R0QkjPHvMSOSmck3ZswYRo8eneK4RUREREQeNL2K9SK8eDj+ufzBDZSTSn+F8xR2dgiSBRljiI6O5vT/17jJly/fHY7IeDly5MDX1/eO/YwxPLfoOeb+NZdVPVZROOedP+M1Qmqw4tAKBj4y0N627O9l1ChQA4DC/oUJyh7Eir9XUDGoIgCXr19mw/ENPPPwM2m6HnEepy/fS40rV6BbN/jiCwgISL9xhw0bxqBBg+zbe/fupVq1aul3AhERERGRe5iPqw+tCrWyElLezo7m/uXp6ensECSL8vLyAuD06dPkyZMnSy3lu53+C/sz488Z/Nj5R3J45LDXg/Lz8MPLzbqm7nO7kz9HfsY0HAPAC9Vf4NGpj/L+uvd5rMRjzNw5k83/bObzlp8DYLPZGFh9IG/89gbFcxensH9hXl35KsE5gmlTqo1TrlPSzqlJqYAAcHGBU44z8zh1CoKCEvc/eNAqcN6yZUJbXJz1r6sr7N2bcNypU9bT924ds2LFpOPw8PBwmG6YPXv2VF+LiIiIiMj9KrdHbtxd3K0ZUiLiFN7eVkb4xo0b90xS6rPNnwEQNi3MoX1K6yn0rNgTgKOXjpLNlvAMtpohNZnRdgbDVw7nv7/8l+K5ijOv8zyH4uhDag0h6kYU/X7qx8VrF6ldsDaLuy7G01WJ3XuNU5NS7u5QpQqsWAFt2lhtcXHW9oABifuXKgV//unYNny4NYPqo48gJATc3KzE1IoVCUmoy5etp/A9o5l8IiIiIiKpZvv//2nJnojzZIVaUqllRpo79lnVc1Witg4PdaDDQx2SPcZms/Favdd4rd5rdxOeZAHZ7twlYw0aZC3HmzYN9uyxEkdRUQlP4+vePaEQuqcnlC3r+PL3hxw5rK/d3cFmg4ED4Y03YP58K4nVvTsEByckvkRERERERCTj9OzZkzZO+gOsW7duvPXWWxl6jvS8vrNnz5InTx6OHz+eLuPJfWz1amvpWHCwlfyYNy9h340b8J//QLly4ONj9eneHf75x3GM8+ehSxfw9bUSKr17gxMf9ub0pFSnTvDeezBihDWzads2WLw4oVD50aNw8mTqxhwyBJ57Dvr1g6pVrfu7eLGV1BIRERERkQfDqIGjqJq/aqLXc12ec3ZoWd6oUaOw2Wy3fWVF27dvZ+HChTz//PP2trCwMAYOHOi8oO4gICCA7t27M3LkSGeHIlldVBRUqAATJiTeFx0NW7fCq69a/86ZY9U4atXKsV+XLrBrFyxbBgsWWImufv0yJ/4kZIlC5wMGJL1cD2DVqtsfO3Vq4jabDV57zXqJiIiIiEjGqZr/4Uw716YTm1N9TI16NRjxwQiHNnd39/QKKUk3Ym7g5n7vFOCKjY3FZrORLVvCnIWXX36Zp59+2r5dtWpV+vXrR9++fZ0RYop98skndOjQ4Z6rE9yrVy+qVKnCu+++S65cuZwdjmRVzZpZr6T4+VmJpluNHw/VqlmzfQoWtJanLV4MmzbBw///s/uTT6B5c2u2UHBwxsafBKfPlBIREREREcko7u7uBOQJcHj5+ic8yr5q/qrMmzGPwb0HU7tobdrWasuvS391GOPAXwd4vuvz1C1elyYVmjDiuRFcPH/Rvv+p9k8x9pWxvD/ifRqWbchzT1gzsX5d+itta7WlVpFaPN3+aRbMWkDV/FW5cukKV6OvElYyjBULVjica968efj4+HDlypUkrycsLIwBAwYwYMAA/Pz8CAgI4NVXX8WYhNo9169f5+WXXyZ//vz4+PhQvXp1Vt3yX/unTp2Kv78/8+fPp0yZMnh4eHD06FGH82TPnp2goCD7y8XFhRw5cti3z5w5Q/369fHy8iJ37tz069ePyNssAdq0aROBgYG88847AFy8eJE+ffoQGBiIr68v9evXZ/v27fb+o0aNomLFinz11VeEhobi5+dH586dk70vYCXXfvjhB1re+mSsJFy4cIHu3buTM2dOvL29adasGfv370907luNGzeO0NDQZMeMi4tjzJgxFC5cGC8vLypUqMAPP/zgcM4uXboQGBiIl5cXxYsXZ8qUKfb9Dz30EMHBwcydO/e2sYukyqVL1qwdf39re/166+uHb/mPCQ0bQrZsViFuJ1BSSkREREREHmhffPAFDVs25Nvl31KzQU1GDBjBpQuXALhy6QrPdnyWkg+VZPqi6Xz8zcecP3ueYU8Ncxjj5+9/xs3djUnzJjH07aGcOHqCof2G8mjTR/lm6Te07daWz975zN7fy9uLRq0b8dN3PzmMM2XKFNq3b0+OHDmSjXfatGm4urqyceNGPvroIz744AMmTZpk3z9gwADWr1/PzJkz2bFjBx06dKBp06YOiZfo6GjeeecdJk2axK5du8iTJ0+K71dUVBRNmjQhZ86cbNq0ie+//57ly5czIJnlL7/88guNGjXizTff5D//+Q8AHTp04PTp0yxatIgtW7ZQuXJlGjRowPnz5+3HHTx4kHnz5rFgwQIWLFjAr7/+yttvv51sXDt27ODSpUs8/PDtZ+/17NmTzZs3M3/+fNavX48xhubNm3Pjxo0U34N/GzNmDNOnT2fixIns2rWLF198ka5du/Lrr1aC89VXX2X37t0sWrSIPXv28NlnnxEQEOAwRrVq1fjtt9/SHIPcmy5fvuzwun79evoMfO2aVWMqPNyqHwUQEQH//l53dYVcuax9TpAllu+JiIiIiIhkhDXL11C3eF2Htl7P9aLX873s2y06tqBJmyYA9B/an+++/I5d23ZRs15NZk2ZRcmyJek/rL+9/6vvv0qLqi04cvAIhYoWAiCkcAjPD0+oY/TJW59QqGghXnj1BQBCi4Vy8K+DTP54sr1Pm/A29G7dm7OnzhKQN4DTp0+zcOFCli9ffttrCgkJ4cMPP8Rms1GyZEn+/PNPPvzwQ/r27cvRo0eZMmUKR48eJfj/l+K8/PLLLF68mClTptgLgN+4cYNPP/2UChUqpPqezpgxg2vXrjF9+nR8fHwAGD9+PC1btuSdd94hb3yBYGDu3Ll0796dSZMm0alTJwDWrFnDxo0bOX36NB4eHgC89957zJs3jx9++IF+/1/fJi4ujqlTp9oTdN26dWPFihW8+eabScZ15MgRXFxcbptg279/P/Pnz2ft2rXUrFkTgG+++YaQkBDmzZtHhw7JP/EtOdevX+ett95i+fLl1KhRA4AiRYqwZs0a/ve///Hoo49y9OhRKlWqZE+YJTXrKjg4mD/++CPV55d7W0hIiMP2yJEjGTVq1N0NeuMGdOwIxsBnn925vxMpKSUiIiIiIvetKjWrMHTMUIe2W5fvARQvXdz+tZe3Fz45fLhw9gIA+3fvZ/O6zYkSWwDHjxy3J6VKlS/lsO/owaOUqVDGoa1MJcfthyo9RJESRVjw/QJ6DujJ119/TaFChahbN/G5bvXII484FBqvUaMG77//PrGxsfz555/ExsZSokQJh2OuX79O7ty57dvu7u6UL1/+tudJzp49e6hQoYI9IQVQq1Yt4uLi2Lt3rz0ptWHDBhYsWMAPP/zg8KS67du3ExkZ6RAPwNWrVzl48KB9OzQ01GHGWL58+Th9+nSycV29ehUPD4/bFmHfs2cPrq6uVK9e3d6WO3duSpYsyZ49e+588Uk4cOAA0dHRNGrUyKE9JiaGSpUqAfDMM8/Qrl07tm7dSuPGjWnTpo09KRbPy8uL6OjoNMUg965jx47h65vwMyk+UZtm8QmpI0fgl18SZkkBBAXBv7+Hbt60nsgXFHR3500jJaVEREREROS+5eXtRUjhkNv2cXVz/LPIZrMRFxcHWMvc6jSqw3P/TfzEvoC8CcuvvLy80hRf6yda8/3U7+k5oCdTpkyhV69ed/Vku8jISFxcXNiyZQsuLi4O+24t/u3l5ZXhT9ArWrQouXPnZvLkyTz22GO4ubnZY8yXL59Dnat4/vG1b8DeP96t70tSAgICiI6OJiYm5q6K2WfLls2hRhdw26V98bW0fv75Z/Lnz++wLz7B0KxZM44cOcLChQtZtmwZDRo0oH///rz33nv2vufPnycwMDDNccu9ydfX1yEpdVfiE1L798PKlfCvxC81asDFi7BlC1SpYrX98gvExcEtidrMpJpSIiIiIiIiyShVthR/7/2bfCH5CCkc4vDy8k4+EVWwaEH27HCcebN72+5E/Zq1bUbEiQhmfjmT3bt306NHjzvGtOFfBYl///13ihcvjouLC5UqVSI2NpbTp09TrFgxh1dQOs2EKF26NNu3bycqKsretnbtWrJly0bJkiXtbQEBAfzyyy8cOHCAjh072hM7lStXJiIiAldX10Qx/rvOUmrEFyffvTvxfb419ps3bzrcw3PnzrF3717KlLFmsgUGBhIREeGQmNq2bVuyY95aLP7f13Pr0qzAwEB69OjB119/zbhx4/j8888dxtm5c6d9ZpVIkiIjYds26wVw6JD19dGjVkKqfXvYvBm++QZiY606UREREBNj9S9dGpo2hb59YeNGWLsWBgyAzp2d8uQ9UFJKRERERETuYzExMZw9fdbhdeuT8+6kQ88OXL54meHPDmfXtl0cP3yc9avWM/rF0cTGxiZ7XNuubTl84DCfvPkJRw4eYdn8ZSyYtQDAYYaSr78vYc3C+PiNj2ncuDEFChS4Y0xHjx5l0KBB7N27l2+//ZZPPvmEF16waleVKFGCLl260L17d+bMmcOhQ4fYuHEjY8aM4eeff07xdd9Oly5d8PT0pEePHuzcuZOVK1fy3HPP0a1bN4d6UgB58uThl19+4a+//iI8PJybN2/SsGFDatSoQZs2bVi6dCmHDx9m3bp1vPLKK2zevDnNcQUGBlK5cmXWrFmTbJ/ixYvTunVr+vbty5o1a9i+fTtdu3Ylf/78tG7dGrCecHjmzBnGjh3LwYMHmTBhAosWLUp2zBw5cvDyyy/z4osvMm3aNA4ePMjWrVv55JNPmDZtGgAjRozgxx9/5MCBA+zatYsFCxZQunRp+xjR0dFs2bKFxo0bp/n65QGweTNUqmS9AAYNsr4eMQJOnID58+H4cahYEfLlS3itW5cwxjffQKlS0KABNG8OtWvDvxKkmUlJKRERERERuW+tX7meZpWaObz6tOmT4uMDgwKZNG8SsXGxPPfEc3Ru0JkPRn5ADt8cZMuW/J9T+Qvm5+3P32blwpU80egJZk+fzZPPPwmAm7vjsrTWnVtzI+YGTz75ZIpi6t69O1evXqVatWr079+fF154wV4cHKwn+HXv3p2XXnqJkiVL0qZNGzZt2kTBggVTfN234+3tzZIlSzh//jxVq1alffv2NGjQgPHjxyfZPygoiF9++YU///yTLl26EBcXx8KFC6lbty69evWiRIkSdO7cmSNHjiRKaqVWnz59+Oabbxza4uLicHVNWKI5ZcoUqlSpQosWLahRowbGGBYuXGhfLli6dGk+/fRTJkyYQIUKFdi4cSMvv/zybc/7+uuv8+qrrzJmzBhKly5N06ZN+fnnnylcuDBg1fAaNmwY5cuXp27duri4uDBz5kz78T/++CMFCxakTp06d3X9cp8LC7OKl//7NXUqhIYmvc8Y67h4uXLBjBlw5QpcugSTJ8MtS3szm838e7GssGfPHsqUKcPu3bsdstdZWQYvB5esapTe+AeRGeXsCMQp9Ov6gaXf8Q+oLPY7vpBPISbWmkhA/oBEVWmr5n840+LYdCLts2iygskfTWb2V7P5ebPjjKWFPyzkg1EfcDri9B1rIYWFhVGxYkXGjRuXgZHeu65evUrJkiX57rvv7E/CK1WqFH369LljYsmZHnnkEZ5//nmeeOKJZPtcu3aNQ4cOUbhwYTw9PTMxugTHjx8nJCSEY8eOpWhWnyTv8uXL+Pn5cenSpfSrKXUPUqFzERERERFJs3s9UZSRvp/6PWUqlsEvpx87Nu3gq4lf0bFnR/v+a1evcfbUWaZOmErbrm3vqji3WLy8vJg+fTpnz57l9OnTLFq0iL1799KgQQNnh5ass2fP0rZtW8LDw50dikimU1JKREREREQkAxw7dIzJH0/m8sXLBAUH0aVfF3o+19O+f/qn05n88WQqVa/k0C53J+z/lypVrlyZCxcu8PHHH2fpAuIBAQEMGTLE2WGIOIWSUiIiIiIiIhlg0OhBDBo9KNn9/V7qR7+X+iW7PymrVq26y6geHFu3bnV2CCJyByp0LiIiIiIiIiIimU5JKRERERERERERyXRKSomIiIiIiIiISKZTUkpERERERERERDKdklIiIiIiIiIiIpLplJQSEREREREREZFMp6SUiIiIiIiI3LPOnTtHnjx5OHz48B37nj17ljx58nD8+PGMD0xE7khJKRERERERua/t2LyD6iHVGdhtoLNDyRQ2m4158+al+fiwsDAGDhyYbvFktDfffJPWrVsTGhp6x74BAQF0796dkSNHZnxgInJHrs4OQERERERE7l1Vv6iaaefa1HdTmo6bP3M+HXt1ZP7M+ZyJOENgUGA6R5bAGENsbCyurvpTKzNER0fz5ZdfsmTJkhQf06tXL6pUqcK7775Lrly5MjA6EbkTzZQSEREREZH7VnRUNMvmL6Nd93bUalCLBbMW2PcN7z+cYU8Pc+h/88ZNGpZtyM/f/wxAXFwcUz6ZQutHWlO7aG2eaPgEKxassPffsm4LVfNXZe0va+nWtBs1C9dk+8btHD98nJd6vUSTCk2oW7wu3Zt3Z8PqDQ7nOnvqLAO7DaR20dq0fqQ1M2bMIDQ0lHHjxtn7XLx4kT59+hAYGIivry/169dn+/btab4f586dIzw8nPz58+Pt7U25cuX49ttv7ft79uzJr7/+ykcffYTNZsNms9mXxe3cuZNmzZqRPXt28ubNS7du3Th79qz92LCwMJ5//nmGDBlCrly5CAoKYtSoUQ7nv3jxIk899RR58+bF09OTsmXLsmDBAqKiovD19eWHH35w6D9v3jx8fHy4cuVKktezcOFCPDw8eOSRR+xtFy5coEuXLgQGBuLl5UXx4sWZMmWKff9DDz1EcHAwc+fOTettFJF0oqSUiIiIiIjct5b/tJxCxQoRWiyUZm2bMf+7+RhjAGj6eFN+W/Yb0VHR9v7rV63n2tVrhDULA2DqJ1NZ+MNChr49lJm/zCS8bzgjnh/BlvVbHM4z4a0JDPjvAL5f9T3FShcjOiqaWvVrMeG7CXy95GtqhNXgpV4vEXEiwn7MyBdGcubUGSZ+P5F3vniHzz//nNOnTzuM26FDB06fPs2iRYvYsmULlStXpkGDBpw/fz5N9+PatWtUqVKFn3/+mZ07d9KvXz+6devGxo0bAfjoo4+oUaMGffv25eTJk5w8eZKQkBAuXrxI/fr1qVSpEps3b2bx4sWcOnWKjh07Oow/bdo0fHx82LBhA2PHjuW1115j2bJlgJXga9asGWvXruXrr79m9+7dvP3227i4uODj40Pnzp0dkkcAU6ZMoX379uTIkSPJ6/ntt9+oUqWKQ9urr77K7t27WbRoEXv27OGzzz4jICDAoU+1atX47bff0nQPRST9aE6piIiIiIjct3789keatW0GQI16NYgcFMnW9VupUrMKj4Q9gpe3F6sWraJ5++YALJm3hLqN6+KT3YeY6zFM+WQKE2ZOoPzD5QEoUKgA2zdtZ+7Xc6lSIyEZ8tTgp6het7p92y+nHyUeKmHffmbIM6xavIrVS1fTsVdHDh84zMbfNjJt4TTKVCgDwKRJkyhevLj9mDVr1rBx40ZOnz6Nh4cHAO+99x7z5s3jhx9+oF+/fqm+H/nz5+fll1+2bz/33HMsWbKEWbNmUa1aNfz8/HB3d8fb25ugoCB7v/Hjx1OpUiXeeuste9vkyZMJCQlh3759lChhXWv58uXt9ZqKFy/O+PHjWbFiBY0aNWL58uVs3LiRPXv22PsXKVLEPl6fPn2oWbMmJ0+eJF++fJw+fZqFCxeyfPnyZK/nyJEjBAcHO7QdPXqUSpUq8fDDDwMkWWsqODiYP/74I6W3TUQyiJJSIiIiIiJyXzp84DC7tu3i3S/fBcDV1ZVGrRrx47c/UqVmFVxdXWnYsiGL5i6iefvmXI2+yq9LfuXNT98E4NjhY1y7eo0B4QMcxr1x4wYly5Z0aCtdvrTDdnRUNJ+//zlrV6zl7OmzxN6M5fq16/aZUkcOHsHF1YVS5UrZjylWrBg5c+a0b2/fvp3IyEhy587tMPbVq1c5ePBgmu5JbGwsb731FrNmzeLEiRPExMRw/fp1vL29b3vc9u3bWblyJdmzZ0+07+DBgw5JqVvFJ5cAtm3bRoECBex9/61atWo89NBDTJs2jaFDh/L1119TqFAh6tatm2xcV69exdPT06HtmWeeoV27dmzdupXGjRvTpk0batas6dDHy8uL6OhoRMS5lJQSEREREZH70vyZ84m9GUvzys3tbcYY3NzdGPLmELL7Zqfp4015qv1TnD97ng2rN+Dh6UHNelYC42rUVQA+nP4heYLyOIzt5u7msO3l7eWw/dFrH7Hhtw288OoLhISG4OHpwX/6/YcbMTdSHH9kZCT58uVj1apVifb5+/uneJxbvfvuu3z00UeMGzeOcuXK4ePjw8CBA4mJibljLC1btuSdd95JtC9fvnz2r93cHO+LzWYjLi4OsBJBd9KnTx8mTJjA0KFDmTJlCr169cJmsyXbPyAggAsXLji0NWvWjCNHjrBw4UKWLVtGgwYN6N+/P++99569z/nz5wkMzLiC9yKSMkpKiYiIiIjIfefmzZv8/MPPDBwxkOqPVnfYN7j3YJbMW0K77u2oULUCeYPzsmz+MtatXEfDFg1xdbP+TCpcojDuHu6cOnHKYaleSmzfvJ0WHVpQr1k9wJo5dfL4Sfv+QkULEXszlr0799pnWR04cMAhwVK5cmUiIiJwdXVNcglaWqxdu5bWrVvTtWtXwKrztG/fPsqUKWPv4+7uTmxsrMNxlStXZvbs2YSGhqb5yYLly5fn+PHjDsv9/q1r164MGTKEjz/+mN27d9OjR4/bjlmpUiW+/vrrRO2BgYH06NGDHj16UKdOHQYPHuyQlNq5cydhYWFpug4RST8qdC4iIiIiIvedNcvXcOXSFVqHt6ZYqWIOr/rN6/PjzB/tfZu2acrsr2azYfUGmrZtam/3ye5D16e68sGoD1gwawHHDx/nrz//4rvJ3zk8xS8pIYVDWLloJXt37mXfrn0M7z8cE2fs+0OLhVKtTjXeGvIWu/7Yxd6de+nXrx9eXl72mUENGzakRo0atGnThqVLl3L48GHWrVvHK6+8wubNm297/kOHDrFt2zaHV1RUFMWLF2fZsmWsW7eOPXv28NRTT3Hq1CmHY0NDQ9mwYQOHDx/m7NmzxMXF0b9/f86fP094eDibNm3i4MGDLFmyhF69eiVKYCXn0UcfpW7durRr145ly5Zx6NAhFi1axOLFi+19cubMSdu2bRk8eDCNGzemQIECtx2zSZMm7Nq1yyGZN2LECH788UcOHDjArl27WLBgAaVLJyyvjI6OZsuWLTRu3DhFcYtIxlFSSkRERERE7js/fvsj1WpXI7tv4hpI9ZvXZ8/2PezfvR+Apm2bcmjfIfIE5aFC1QoOfZ8e8jS9B/Zm6vipdAjrwPNdnmfNijUEFwxONO6tXhz5Ir5+vvRu3ZtBPQfxSNgjlCznWIdq9EejyRWYi37t+jG492D69u1Ljhw57DWSbDYbCxcupG7duvTq1YsSJUrQuXNnjhw5Qt68eW97/kGDBlGpUiWH1x9//MHw4cOpXLkyTZo0ISwsjKCgINq0aeNw7Msvv4yLiwtlypQhMDCQo0ePEhwczNq1a4mNjaVx48aUK1eOgQMH4u/vT7ZsKf+zcvbs2VStWpXw8HDKlCnDkCFDEiW1evfuTUxMDE8++eQdxytXrhyVK1dm1qxZ9jZ3d3eGDRtG+fLlqVu3Li4uLsycOdO+/8cff6RgwYLUqVMnxXGLSMawmfjnoYrdnj17KFOmDLt373bIqGdlt1lmLfezUXrjH0RmlLMjEKfQr+sHln7HP6Cy2O/4Qj6FmFhrIgH5A1QAJAMFxQUREhLC8uXLadCggbPDcZqvvvqKF198kX/++Qd3d/c79v/5558ZPHgwO3fuTFGC7JFHHuH555/niSeeSI9wM821a9c4dOgQhQsXTlTcPbMcP36ckJAQjh07dsdZbHJ7ly9fxs/Pj0uXLuHr6+vscJxGv1JEREREREScYNOaTURHR1OsVDHOnjrLwHcHEhoaetunzd3PoqOjOXnyJG+//TZPPfVUihJSAI899hj79+/nxIkThISE3Lbv2bNnadu2LeHh4ekRsojcJS3fExERERERcYKbN2/y6duf0qleJ4b0GUJgYCCrVq1K9AS7B8XYsWMpVaoUQUFBDBs2LFXHDhw48I4JKbCe1jdkyJDbPtFPRDKPZkqJiIiIiIg4QY2wGtQIq2Hffjj4YSdG43yjRo1i1KhRzg5DRDKRZkqJiIiIiIiIiEimU1JKREREREREREQynZJSIiIiIiIiIiKS6ZSUEhERERERERGRTKeklIiIiIiIiIiIZDolpUREREREREREJNMpKSUiIiIiIiJ37fDhw9hsNrZt25ZuY4aFhTFw4MB0G09EshYlpURERERE5L62Y/MOqodUZ2C3gc4OJVPYbDb7y8/Pj1q1avHLL784O6w0mTNnDq+//rp9OzQ0lHHjxjkvIBFJV67ODkBERERERO5dD+evmmnn2nxiU5qOmz9zPh17dWT+zPmciThDYFBgOkeWwBhDbGwsrq7O/VNrypQpNG3alLNnz/LKK6/QokULdu7cSZEiRVI9VkxMDO7u7hkQ5Z3lypXLKecVkcyhmVIiIiIiInLfio6KZtn8ZbTr3o5aDWqxYNYC+77h/Ycz7OlhDv1v3rhJw7IN+fn7nwGIi4tjyidTaP1Ia2oXrc0TDZ9gxYIV9v5b1m2hav6qrP1lLd2adqNm4Zps37id44eP81Kvl2hSoQl1i9ele/PubFi9weFcZ0+dZWC3gdQuWpvWj7RmxowZiWYCXbx4kT59+hAYGIivry/169dn+/btd7xuf39/goKCKFu2LJ999hlXr15l2bJlAOzcuZNmzZqRPXt28ubNS7du3Th79qz92LCwMAYMGMDAgQMJCAigSZMmgDUD67PPPqNZs2Z4eXlRpEgRfvjhh9vGcbtzrVq1Cnd3d3777Td7/7Fjx5InTx5OnTpljyV++V5YWBhHjhzhxRdftM8Ei4qKwtfXN1Ec8+bNw8fHhytXrtzxXomI8ygpJSIiIiIi963lPy2nULFChBYLpVnbZsz/bj7GGACaPt6U35b9RnRUtL3/+lXruXb1GmHNwgCY+slUFv6wkKFvD2XmLzMJ7xvOiOdHsGX9FofzTHhrAgP+O4DvV31PsdLFiI6Kplb9Wkz4bgJfL/maGmE1eKnXS0SciLAfM/KFkZw5dYaJ30/knS/e4fPPP+f06dMO43bo0IHTp0+zaNEitmzZQuXKlWnQoAHnz59P8T3w8vICrBlPFy9epH79+lSqVInNmzezePFiTp06RceOHR2OmTZtGu7u7qxdu5aJEyfa21999VXatWvH9u3b6dKlC507d2bPnj1JnvdO54pPOHXr1o1Lly7xxx9/8OqrrzJp0iTy5s2baLw5c+ZQoEABXnvtNU6ePMnJkyfx8fGhc+fOTJkyxaHvlClTaN++PTly5EjxfRKRzKfleyIiIiIict/68dsfada2GQA16tUgclAkW9dvpUrNKjwS9ghe3l6sWrSK5u2bA7Bk3hLqNq6LT3YfYq7HMOWTKUyYOYHyD5cHoEChAmzftJ25X8+lSo0q9vM8Nfgpqtetbt/2y+lHiYdK2LefGfIMqxavYvXS1XTs1ZHDBw6z8beNTFs4jTIVygAwadIkihcvbj9mzZo1bNy4kdOnT+Ph4QHAe++9x7x58/jhhx/o16/fHa8/Ojqa4cOH4+LiwqOPPsr48eOpVKkSb731lr3P5MmTCQkJYd++fZQoYcVcvHhxxo4dm2i8Dh060KdPHwBef/11li1bxieffMKnn36aqG9KzvXGG2+wbNky+vXrx86dO+nRowetWrVK8lpy5cqFi4sLOXLkICgoyN7ep08fatasycmTJ8mXLx+nT59m4cKFLF++/I73R0ScS0kpERERERG5Lx0+cJhd23bx7pfvAuDq6kqjVo348dsfqVKzCq6urjRs2ZBFcxfRvH1zrkZf5dclv/Lmp28CcOzwMa5dvcaA8AEO4964cYOSZUs6tJUuX9phOzoqms/f/5y1K9Zy9vRZYm/Gcv3adftMqSMHj+Di6kKpcqXsxxQrVoycOXPat7dv305kZCS5c+d2GPvq1ascPHjwttceHh6Oi4sLV69eJTAwkC+//JLy5cvz+uuvs3LlSrJnz57omIMHD9qTUlWqVEm0H6BGjRqJtpN72t727dvveC53d3e++eYbypcvT6FChfjwww9ve11JqVatGg899BDTpk1j6NChfP311xQqVIi6deumeixJX6uPrObdde+y5Z8tnIw8ydxOc2lTqk2y/XvO68m07dMStZcJLMOuZ3cBMGrVKEb/Otphf8ncJflrwF/pGrtkDiWlRERERETkvjR/5nxib8bSvHJze5sxBjd3N4a8OYTsvtlp+nhTnmr/FOfPnmfD6g14eHpQs15NAK5GXQXgw+kfkicoj8PYbu5uDtte3l4O2x+99hEbftvAC6++QEhoCB6eHvyn33+4EXMjxfFHRkaSL18+Vq1alWifv7//bY/98MMPadiwIX5+fgQGJhR2j4yMpGXLlrzzzjuJjsmXL5/9ax8fnxTHmZyUnmvdunUAnD9/nvPnz6fp3H369GHChAkMHTqUKVOm0KtXL2w2W9qDl3QRFRNFhbwVeLLik7Sd1faO/T9q+hFvN3zbvn0z7iYVJlagQ5kODv0eCnyI5d0TZsK5ZlNq416VJd65CRPg3XchIgIqVIBPPoFq1ZLuO2cOvPUWHDgAN25A8eLw0kvQrVtCn549Ydq/kqtNmsDixRl2CSIiIiIikoXcvHmTn3/4mYEjBlL90eoO+wb3HsySeUto170dFapWIG9wXpbNX8a6leto2KIhrm7Wn0mFSxTG3cOdUydOOSzVS4ntm7fTokML6jWrB1gzp04eP2nfX6hoIWJvxrJ35177LKsDBw5w4cIFe5/KlSsTERGBq6sroaGhqTp/UFAQxYoVS9ReuXJlZs+eTWhoaJqeEPj777/TvXt3h+1KlSol2Tcl5zp48CAvvvgiX3zxBd999x09evRg+fLlZMuWdPljd3d3YmNjE7V37dqVIUOG8PHHH7N792569OiR6muT9NeseDOaFW+W4v5+nn744WffnvfXPC5cvUCvir0c+rlmcyUoe9C/D5d7kNMLnX/3HQwaBCNHwtatVlKqSRP4V30/u1y54JVXYP162LEDevWyXkuWOPZr2hROnkx4ffttxl+LiIiIiIhkDWuWr+HKpSu0Dm9NsVLFHF71m9fnx5k/2vs2bdOU2V/NZsPqDTRt29Te7pPdh65PdeWDUR+wYNYCjh8+zl9//sV3k79zeIpfUkIKh7By0Ur27tzLvl37GN5/OCbO2PeHFgulWp1qvDXkLXb9sYu9O/fSr18/vLy87DN8GjZsSI0aNWjTpg1Lly7l8OHDrFu3jldeeYXNmzen6b7079+f8+fPEx4ezqZNmzh48CBLliyhV69eSSZ7/u37779n8uTJ7Nu3j5EjR7Jx40YGDBiQZN87nSs2NpauXbvSpEkTevXqxZQpU9ixYwfvv/9+sucPDQ1l9erVnDhxwuGJgTlz5qRt27YMHjyYxo0bU6BAgdTfHEmxK1eucPnyZfvr+vXrGXKeL//4koZFGlLIv5BD+/7z+wl+P5giHxWhy5wuHL10NEPOLxnP6UmpDz6Avn2txFKZMjBxInh7w+TJSfcPC4PHH4fSpaFoUXjhBShfHtascezn4QFBQQmvW5Zmi4iIiIjIfe7Hb3+kWu1qZPdNXM+ofvP67Nm+h/279wPQtG1TDu07RJ6gPFSoWsGh79NDnqb3wN5MHT+VDmEdeL7L86xZsYbggsG3Pf+LI1/E18+X3q17M6jnIB4Je4SS5RzrUI3+aDS5AnPRr10/BvceTN++fcmRIweenp4A2Gw2Fi5cSN26denVqxclSpSgc+fOHDlyJMmn06VEcHAwa9euJTY2lsaNG1OuXDkGDhyIv79/srOTHGIePZqZM2dSvnx5pk+fzrfffkuZMmXSdK4333yTI0eO8L///Q+wlvR9/vnnDB8+nO3btyc55muvvcbhw4cpWrSow7JEgN69exMTE8OTTz6ZyrsiqVWmTBn8/PzsrzFjxqT7Of658g+L9i+iT+U+Du3V81dnauupLO66mM8e+4xDFw5RZ0odrly/ku4xSMazmfjnoTpBTIyVgPrhB2jTJqG9Rw+4eBF+/DG5Iy3GwC+/QKtWMG8eNGpktffsaW27u1vJqPr14Y034F/1Ae2uX7/ukNndu3cv1apVY/fu3ZQuXTrpg7IYLZd+QI3SG/8gMqOcHYE4hfN+XYuT6Xf8AyqL/Y4v5FOIibUmEpA/IIsUALk/BcUFERISwvLly2nQoIGzw0nEZrMxd+5c2tz6x1sW8tVXX/Hiiy/yzz//4O7u7uxw0t21a9c4dOgQhQsXticuM9vx48cJCQlh9+7d5M+f397u4eFhf0JkcmyjbXcsdH6rMb+N4f317/PPS//g7pL8+3nx2kUKjSvEB40/oHfl3ikaOyu4fPkyfn5+XLp0CV9fX2eH4zRO/ZVy9izExsK/k/x588Jftymcf+kS5M8P16+Diwt8+mlCQgqspXtt20LhwnDwIPz3v9CsmbXkz8Ul8Xhjxoxh9OjRiXeIiIiIiIhkkE1rNhEdHU2xUsU4e+osA98dSGhoqJ4al0rR0dGcPHmSt99+m6eeeuq+TEhlNTly5MjQRIoxhsnbJtOtfLfbJqQA/D39KZG7BAfOH8iweCTjOH35XlrkyAHbtsGmTfDmm1ZNqlsfSNG5szV7qlw5awbWggVW3yQeWgHAsGHDuHTpkv21cePGjL8IERERERF5oN28eZNP3/6UTvU6MaTPEAIDA1m1ahVubm53Pljsxo4dS6lSpQgKCmLYsGHODkfSwa9HfuXA+QMpmvkUGRPJwfMHyZcj3x37Stbj1JlSAQHWzKVTpxzbT52y6kAlJ1s2iH+QRMWKsGcPjBlj1ZtKSpEi1rkOHICkZsH+e6ph9uyJ152LiIiIiIikpxphNagRVsO+/XDww06M5s6cWPnltkaNGsWoUaOcHYYkITIm0mEG06ELh9gWsY1cXrko6FeQYcuHceLKCaY/Pt3huC//+JLq+atTNk/ZRGO+vPRlWpZoSSH/Qvxz5R9GrhqJSzYXwsuGZ/j1SPpzalLK3R2qVIEVKxJqSsXFWdvJPMAhSXFx1lK+5Bw/DufOQT4lTkVEREREREQyxeZ/NlNvWj379qClgwDoUaEHU9tM5WTkyURPzrt07RKzd8/mo6YfJTnm8cvHCZ8dzrmr5wj0DqR2wdr83vt3An0Ck+wvWZvTyxQOGmQVNn/4YahWDcaNg6go62l8AN27W/Wj4ov5jxlj9S1a1EpELVwIX30Fn31m7Y+MhNGjoV07a7bVwYMwZIg1s6pJE6dcooiIiIiIiMgDJyw0DDMy+Rl2U9tMTdTm5+lH9CvRyR4zs/3M9AhNsginJ6U6dYIzZ2DECIiIsJbjLV6cUPz86FFruV68qCh49llr9pOXF5QqBV9/bY0D1nLAHTtg2jTrCX7BwdC4Mbz+OtzhYQAiIiIiIpIE8///I2uu3hJ5IGTV5ZMid8PpSSmwluolt1zv38XJ33jDeiXHywuWLEm30EREREREHnjnrp8jJjYGbgCqwS3iFNHR1uwhFcKX+0mWSEqJiIiIiEjWFXUzivlH5hPuHo4//lZiyubsqO4/165dc3YIkgUZY4iOjub06dP4+/vj4uLi7JBE0o2SUiIiIiIickdTDkwBoFWhVri7uGNTVirdHYo65OwQJAvz9/cn6HaPqRe5BykpJSIiIiIid2QwTD4wmZmHZhLgGaCkVAb4a8Bfzg5Bsig3NzfNkJL7kpJSIiIiIiKSYtGx0RyNOnrnjpJqnp6ezg5BRCRTZbtzFxERERERERERkfSlpJSIiIiIiIiIiGQ6JaVERERERERERCTTKSklIiIiIiIiIiKZTkkpERERERERERHJdEpKiYiIiIiIiIhIplNSSkREREREREREMp2SUiIiIiIiIiIikumUlBIRERERERERkUynpJSIiIiIiIiIiGQ6JaVERERERERERCTTKSklIiIiIiIiIpLVrV4NLVtCcDDYbDBvnuN+Y2DECMiXD7y8oGFD2L/fsc/589ClC/j6gr8/9O4NkZGZdQWJKCklIiIiIiIiIpLVRUVBhQowYULS+8eOhY8/hokTYcMG8PGBJk3g2rWEPl26wK5dsGwZLFhgJbr69cuc+JPg6rQzi4iIiIiIiIhIyjRrZr2SYgyMGwfDh0Pr1lbb9OmQN681o6pzZ9izBxYvhk2b4OGHrT6ffALNm8N771kzsDKZZkqJiIiIiIiIiDjB5cuXHV7Xr19P20CHDkFEhLVkL56fH1SvDuvXW9vr11tL9uITUmD1z5bNmlnlBEpKiYiIiIiIiIg4QUhICH5+fvbXmDFj0jZQRIT1b968ju158ybsi4iAPHkc97u6Qq5cCX0ymZbviYiIiIiIiIg4wbFjx/D19bVve3h4ODGazKeklIiIiIiIiIiIE/j6+jokpdIsKMj699Qp6+l78U6dgooVE/qcPu143M2b1hP54o/PZFq+JyIiIiIiIiJyLytc2EosrViR0Hb5slUrqkYNa7tGDbh4EbZsSejzyy8QF2fVnnICzZQSEREREREREcnqIiPhwIGE7UOHYNs2qyZUwYIwcCC88QYUL24lqV591XqiXps2Vv/SpaFpU+jbFyZOhBs3YMAA68l8TnjyHigpJSIiIiIiIiKS9W3eDPXqJWwPGmT926MHTJ0KQ4ZAVBT062fNiKpdGxYvBk/PhGO++cZKRDVoYD11r107+PjjzLwKB0pKiYiIiIiIiIhkdWFhYEzy+202eO0165WcXLlgxox0Dy2tVFNKREREREREREQynZJSIiIiIiIiIiKS6ZSUEhERERERERGRTKeklIiIiIiIiIiIZDolpUREREREREREJNMpKSUiIiIiIiIiIplOSSkREREREREREcl0SkqJiIiIiIiIiEimU1JKREREREREREQynZJSIiIiIiIiIiKS6ZSUEhERERERERGRTKeklIiIiIiIiIiIZDolpUREREREREREJNMpKSUiIiIiIiIiIplOSSkREREREREREcl0SkqJiIiIiIiIiEimU1JKREREREREREQynZJSIiIiIiIiIiKS6ZSUEhERERERERGRTKeklIiIiIiIiIiku9VHVtPy25YEvx+MbbSNeX/Nu23/VYdXYRttS/SKiIxw6Ddh4wRCx4Xi+YYn1SdVZ+OJjRl4FZKRXJ0dgIiIiIiIiIjcf6JioqiQtwJPVnyStrPapvi4vQP24uvha9/O45PH/vV3O79j0NJBTHxsItULVGfc7+No8nUT9g7Y69BP7g1ZYqbUhAkQGgqenlC9Omy8TZJzzhx4+GHw9wcfH6hYEb76yrGPMTBiBOTLB15e0LAh7N+fgRcgIiIiIiIiIg6aFW/GG/Xf4PHSj6fquDw+eQjKHmR/ZbMlpC4++P0D+lbuS69KvSgTWIaJLSbi7ebN5D8mp3f4kgmcnpT67jsYNAhGjoStW6FCBWjSBE6fTrp/rlzwyiuwfj3s2AG9elmvJUsS+owdCx9/DBMnwoYNVvKqSRO4di1zrklERERERETkfnXlyhUuX75sf12/fj1dx684sSL53s9Ho68asfboWnt7TGwMW/7ZQsMiDe1t2WzZaFikIeuPr0/XGCRzOD0p9cEH0LevlVgqU8ZKJHl7w+RkkpxhYfD441C6NBQtCi+8AOXLw5o11n5jYNw4GD4cWre29k2fDv/8A/PmZdJFiYiIiIiIiNynypQpg5+fn/01ZsyYdBk3X/Z8THxsIrM7zmZ2x9mE+IYQNi2MrSe3AnA2+iyxJpa8PnkdjsvrkzdR3Sm5Nzi1plRMDGzZAsOGJbRly2Ytt1ufgiSnMfDLL7B3L7zzjtV26BBERFhjxPPzs5YFrl8PnTsnHuf69esOmd3IyMg0XpGIiIiIiIjI/W337t3kz5/fvu3h4ZEu45YMKEnJgJL27ZohNTl44SAf/v4hXz3+1W2OlHuVU5NSZ89CbCzkdUxykjcv/PVX8sddugT588P16+DiAp9+Co0aWfsiIhLG+PeYEckkTseMGcPo0aPTdhEiIiIiIiIiD5AcOXLg6+t7547poFpwNdYcs5ZGBXgH4GJz4VTUKYc+p6JOEZQ9KFPikfTl9OV7aZEjB2zbBps2wZtvWjWpVq1K+3jDhg3j0qVL9tfG21VaFxEREREREZFMse3UNvJlzweAu4s7VYKrsOLvFfb9cSaOFX+voEaBGs4KUe6CU2dKBQRYM51OOSY5OXUKgm6T5MyWDYoVs76uWBH27IExY6x6U/HHnTplPX3v1jErVkx6PA8PD4fphtmzZ0/tpYiIiIiIiIjILSJjIjlw/oB9+9CFQ2yL2EYur1wU9CvIsOXDOHHlBNMfnw7AuN/HUdi/MA/leYhrN68xaeskfjn0C0u7LrWPMeiRQfSY14OHgx+mWv5qjPt9HFE3ouhVsVemX5/cPacmpdzdoUoVWLEC2rSx2uLirO0BA1I+TlyctZQPoHBhKzG1YkVCEuryZespfM88k57Ri4iIiIiIiEhyNv+zmXrT6tm3By0dBECPCj2Y2mYqJyNPcvTSUfv+mNgYXlr6EieunMDbzZvyecuzvNty6hVOGKNT2U6ciT7DiFUjiIiMoGJQRRZ3WUze7P+q4SP3BKcmpcBaetejBzz8MFSrZj05LyrKehofQPfuVv2o+GL+Y8ZYfYsWtRJRCxfCV1/BZ59Z+202GDgQ3ngDihe3klSvvgrBwQmJLxERERERERHJWGGhYZiRJtn9U9tMddgeUmsIQ2oNueO4A6oNYEC1VMxkkSzL6UmpTp3gzBkYMcIqRF6xIixenFCo/OhRa7levKgoePZZOH4cvLygVCn4+mtrnHhDhlj9+vWDixehdm1rTE/PzLwyERERERERERFJjs0Yk3za8l8uXoS5c+G33+DIEYiOhsBAqFQJmjSBmjUzMNJMtGfPHsqUKcPu3bspXbq0s8NJEZvN2RGIU4zSG/8gMqOcHYE4Rcp/Xct9Rr/jH1D6Hf9Aut2MEpGs4Pjx44SEhHDs2DEKFCjg7HDuaZcvX8bPz49Lly5l2pMMs6IUPX3vn3+gTx+rcPgbb8DVq9aMpgYNoEABWLkSGjWCMmXgu+8yOGIREREREREREbnnpWj5XqVKVt2nLVusxFNSrl6FefOsmlDHjsHLL6dfkCIiIiIiIiIicn9JUVJq927Infv2fby8IDzcep07lx6hiYiIiIiIiIjI/SpFy/fulJC62/4iIiIiIiIiIvJgSVFS6lbTpsHPPydsDxkC/v5WkfMjR9IxMhERERERERERuW+lOin11lvWUj2A9ethwgQYOxYCAuDFF9M7PBERERERERERuR+lqKbUrY4dg2LFrK/nzYN27aBfP6hVC8LC0jc4ERERERERERG5P6V6plT27AmFzJcuhUaNrK89Pa0n8ImIiIiIiIiIiNxJqmdKNWoEffpApUqwbx80b26179oFoaHpHJ2IiIiIiIiIiNyXUj1TasIEqFEDzpyB2bMTnrS3ZQuEh6d3eCIiIiIiIiIicj9K9Uwpf38YPz5x++jR6RCNiIiIiIiIiIg8EFI9Uwrgt9+ga1eoWRNOnLDavvoK1qxJz9BEREREREREROR+leqk1OzZ0KQJeHnB1q1w/brVfukSvPVWeocnIiIiIiIiIiL3o1Qnpd54AyZOhC++ADe3hPZatawklYiIiIiIiIiIyJ2kOim1dy/UrZu43c8PLl5Mh4hEREREREREROS+l+qkVFAQHDiQuH3NGihSJD1CEhERERERERGR+12qk1J9+8ILL8CGDWCzwT//wDffwMsvwzPPZESIIiIiIiIiIiJyv3FN7QFDh0JcHDRoANHR1lI+Dw8rKfXccxkRooiIiIiIiIjI3bl47SJz98zlt6O/ceTSEaJvRBPoHUiloEo0KdaEmiE1nR3iAyfVSSmbDV55BQYPtpbxRUZCmTKQPXtGhCciIiIiIiIiknb/XPmHEStH8M2f3xCcI5hq+atRMW9FvNy8OH/1PCsPr+S99e9RyK8QIx8dSaeynZwd8gMj1UmpeO7uVjJKRERERERERCSrqvS/SvSo0IMt/bZQJjDpRMbVG1eZ99c8xm0Yx7HLx3i55suZHOWDKUVJqbZtUz7gnDlpDUVEREREREREJH3tfnY3ub1z37aPl5sX4eXCCS8Xzrnoc5kU2T0mKgp8fNJ1yBQlpfz80vWcIiIiIiIiIiKZ4k4Jqbvt/8DImxc6doQnn4TatdNlyBQlpaZMSZdziYiIiIiIiIg4zbRt0wjwDuCxEo8BMGTZED7f8jllAsvwbbtvKeRfyMkRZmFffw1Tp0L9+hAaaiWnuneH4OA0D5kt3YITEREREREREcnC3lrzFl5uXgCsP7aeCZsmMLbRWAK8A3hxyYtOji6La9MG5s2DEyfg6adhxgwoVAhatLBqOd28meohU13ovHBh6wl8yfn771THICIiIiIiIiKS4Y5dOkaxXMUAmPfXPNqVbke/Kv2oFVKLsGlhTo3tnhEYCIMGWa9PPoHBg2HhQggIsJJVQ4eCt3eKhkp1UmrgQMftGzfgjz9g8WIrDhERERERERGRrCi7e3bORZ+joF9Blv69lEGPDALA09WTqzeuOjm6e8SpUzBtmrWU78gRaN8eeveG48fhnXfg999h6dIUDZXqpNQLLyTdPmECbN6c2tFERERERERERDJHo6KN6PNTHyoFVWLfuX00L94cgF1ndhHqH+rc4LK6OXOsouNLlkCZMvDss9C1K/j7J/SpWRNKl07xkOlWU6pZM5g9O71GExERERERERFJXxOaT6BGgRqciT7D7I6z7U/a2/LPFsLLhjs5uiyuVy+rqPnatbBtGwwY4JiQAmv/K6+keMhUz5RKzg8/QK5c6TWaiIiIiIiIiEj68vf0Z3zz8YnaR9cb7YRo7jEnT965VpSXF4wcmeIhU52UqlTJsdC5MRARAWfOwKefpnY0EREREREREZGMc/TSUQr6FUxx/xOXT5DfN38GRnSPypHDSkzlyePYfu6c1RYbm+ohU52UatPGcTtbNqvwelgYlCqV6vOLiIiIiIiIiGSYql9UpU3JNvSp3Ieq+asm2efStUvM2jWLjzZ8RL8q/Xi++vOZHOU9wJik269fB3f3NA2Z6qRUKmZhiYiIiIiIiIg41e5nd/Pmb2/S6KtGeLp6UiW4CsHZg/F09eTCtQvsPrObXWd2UTlfZcY2Gmsvfi7/7+OPrX9tNpg0CbJnT9gXGwurV6d5llKaakrFxsK8ebBnj7X90EPQqhW4uKQpBhERERERERGRDJHbOzcfNPmAN+u/yc/7f2bN0TUcuXSEqzeuEuAdQJdyXWhSrAll85R1dqhZ04cfWv8aAxMnOiZ/3N0hNNRqT4NUJ6UOHIDmzeHECShZ0mobMwZCQuDnn6Fo0TTFISIiIiIiIiKSYbzcvGhfpj3ty7R3diipFxsLo0bB119bhb2Dg6FnTxg+PKHwtzHW8rYvvoCLF6FWLfjsMyhe/O7OfeiQ9W+9ejBnDuTMeXfj3SJbag94/nkr8XTsGGzdar2OHoXCha19IiIiIiIiIiKSjt55x0owjR9vLVt75x0YOxY++SShz9ix1lK7iRNhwwbw8YEmTeDatfSJYeXKdE1IQRpmSv36K/z+O+TKldCWOze8/baVhBMRERERERERkXS0bh20bg2PPWZth4bCt9/Cxo3WtjEwbpw1c6p1a6tt+nTIm9eqv9S5c9rOO2gQvP66leAaNOj2fT/4INXDpzop5eEBV64kbo+MTHOxdRERERERERGRB87ly5cdtj08PPDw8EjcsWZN+Pxz2LcPSpSA7dthzZqERNChQ9ayvoYNE47x84Pq1WH9+rQnpf74A27cSPg6OfFLCFMp1UmpFi2gXz/48kuoVs1q27ABnn7aKnYuIiIiIiIiIiJ3FhIS4rA9cuRIRo0albjj0KFw+bL1lDsXF6vG1JtvQpcu1v6ICOvfvHkdj8ubN2FfWqxcmfTX6STVSamPP4YePaBGDXBzs9pu3rQSUh99lN7hiYiIiIiIiIikj6iYKHzcfZwdht2xY8fw9fW1byc5Swpg1iz45huYMQMeegi2bYOBA62C5z16ZEqsnDkDgYFJ7/vzTyhXLtVDprrQub8//Pgj7N0LP/xgvfbuhblzrZlhIiIiIiIiIiJZUd738vLkj0+y5ugaZ4cCgK+vr8Mr2aTU4MHWbKnOna3kT7du8OKLMGaMtT8oyPr31CnH406dSth3t8qVg59/Ttz+3nsJS+lSKdVJqXjFi0PLltarWLG0jiIiIiIiIiIikjm+bvs156+ep/60+pT4pARvr3mbf6784+yw7iw6GrL9K4Xj4gJxcdbXhQtbyacVKxL2X75s1VuqUSN9Yhg0CNq1g2eegatX4cQJaNDAeurfjBlpGjLFy/fuVGQdwNXVugcNGkCFCmmKR0REREREREQkQ7Qp1YY2pdpwJuoMX+34iqnbpvLqyldpUrQJT1Z6klYlW+GaLdWVjjJey5ZWDamCBa3le3/8YRU5f/JJa7/NZi3ne+MNaxZR4cLw6qvW8r42bdInhiFDoFEja5ZW+fJw/rxVSH3HjjTPxkrxnb5dkfV4cXFw+rQ1q+yTT+DZZ9MUk4iIiIiIiIhIhgn0CWRQjUEMqjGITzZ8wuBlg1m4fyEB3gE8/fDTDK09FG83b2eHmeCTT6wk07PPWomX4GB46ikYMSKhz5AhEBVlPZ3u4kWoXRsWLwZPz/SLo1gxKFsWZs+2tjt1uqvlgSlOSqWmyPq0afDaa0pKiYiIiIiIiEjWcyryFNO2T2PqtqkcuXSE9mXa07tSb45fPs47a9/h9+O/s7TbUmeHmSBHDhg3znolx2azkjGvvZYxMaxdC127Qq5c1uyotWvhuedg4UKYOBFy5kz1kBkyJ615c+spfSIiIiIiIiIiWcWcPXOYsm0KSw4soUxgGZ6t+ixdy3fF39Pf3qdmSE1KTyjtvCCzqvr1reLqr78Obm5QujTUq2clqsqVg+PHUz1kigqdv/22VVMrJTZsgI0bYcuWVMciIiIiIiIiIveJ1UdW0/LblgS/H4xttI15f827bf85e+bQ6KtGBL4biO8YX2p8WYMlB5Y49Bm1ahS20TaHV6nxpVIcU68fexGcPZi1T65l29PbGFBtgENCCiA4RzCv1HklxWM+MJYutRJEbm4JbUWLWjOmnnoqTUOmKCm1ezcUKmQtx1u0CM6cSdh386Y1a+vTT6FmTWs5YY4cqQtiwgQIDbWWOVavbiW1kvPFF1CnjjUrLGdOaNgwcf+ePa1Za7e+mjZNXUwiIiIiIiIiknZRMVFUyFuBCc0npKj/6iOraVSkEQufWMiWfluoF1qPlt+25I+TjkWuHwp8iJMvnbS/1jy5JsUxnXzpJP9r+T+q5q+abB8vNy9Gho1M8ZgPjEcftf49cACWLLGewAdW0uXVV9M0ZIqW702fDtu3w/jx8MQT1lMFXVzAwyNhBlWlStCnj5UQSk0Nre++s57sN3GilZAaNw6aNIG9eyFPnsT9V62C8HArAebpCe+8A40bw65dkD9/Qr+mTWHKlIRtD4+UxyQiIiIiIiIid6dZ8WY0K94sxf3HNR3nsP1Wg7f4ce+P/LTvJyrlq2Rvd83mSlD2tBXXXnV4FS42F5oUa+LQvuTAEuJMXKrifeCcOwcdO1pFx2022L8fihSB3r2tOlPvvZfqIVM0UwqgQgVrltK5c9bSvO+/t7aXLIFTp2DzZnj66dQXdf/gA+jbF3r1gjJlrOSUtzdMnpx0/2++sWZsVawIpUrBpEnWU/9WrHDs5+FhFYCPf6Wh3paIiIiIiIiI/MuVK1e4fPmy/XX9+vUMOU+ciePK9Svk8srl0L7//H6C3w+myEdF6DKnC0cvHU3xmEOXDyXWxCZqNxiGrhh61zHf11580Vq6d/SolbiJ16mTtawuDVKclLIfkM1KCLVuDZ07W8vnAgLSdG5iYqwEV8OGjuM3bAjr16dsjOhouHHDSsrdatUqa6ZVyZLwzDNWMk1ERERERERE7k6ZMmXw8/Ozv8aMGZMh53lv3XtExkTS8aGO9rbq+asztfVUFnddzGePfcahC4eoM6UOV65fSdGY+8/vp0xgmUTtpQJKceD8gXSL/b60dKm1XK1AAcf24sXhyJE0DZkhT99LqbNnITYW8uZ1bM+bF/76K2Vj/Oc/EBzsmNhq2hTatoXCheHgQfjvf6FZMyvR5eKSeIzr1687ZHYjIyPTcDUiIiIiIiIi97/du3eT/5b6OR4ZUC9nxp8zGP3raH7s/CN5fBJq+9y6vK583vJUL1CdQuMKMWvXLHpX7n3Hcf08/Pj7wt+E+oc6tB84fwAfN590i/++FBXlOEMq3vnzaa6ZlOqZUlnJ22/DzJkwd67jssHOnaFVK+uJhG3awIIFsGmTNXsqKWPGjHHI8larVi0zwhcRERERERG55+TIkQNfX1/7K72TUjN3zqTP/D7Maj+LhkUa3ravv6c/JXKXSPEsp9YlWzNw8UAOnj9obztw/gAvLX2JViVb3VXc9706dayi4/FsNque0tixUK9emoZ0alIqIMCauXTqlGP7qVNWHajbee89Kym1dCmUL3/7vkWKWOc6kMxndNiwYVy6dMn+2ni7x/+JiIiIiIiISIb49s9v6fVjL75t9y2PlXjsjv0jYyI5eP4g+XLkS9H4YxuNxcfdh1ITSlH4o8IU/qgwpSeUJrdXbt5rnPpC3Q+UsWPh88+tpWgxMTBkCJQtC6tXW8v60sCpy/fc3aFKFatIeZs2Vlt80fIBA5I/buxYePNNq8j6ww/f+TzHj1s1pfIl8xn18PBwyOxmz5495RchIiIiIiIiIolExkQ6zGA6dOEQ2yK2kcsrFwX9CjJs+TBOXDnB9Met2Tcz/pxBj3k9+KjpR1QvUJ2IyAgAvFy98PP0A+DlpS/TskRLCvkX4p8r/zBy1UhcsrkQXjY8RTH5efqx7sl1LPt7GdsjtuPl5kX5vOWpW6huOl/9fahsWdi3D8aPhxw5IDLSqp3Uv3/yCZc7cGpSCmDQIOjRw0ouVasG48ZZyxR79bL2d+8O+fNDfN20d96BESNgxgwIDYUI6zNK9uzWKzISRo+Gdu2s2VYHD1rJu2LFoEmTpCIQERERERERkfS2+Z/N1JuWsKxr0NJBAPSo0IOpbaZyMvKkw5PzPt/yOTfjbtJ/YX/6L+xvb4/vD3D88nHCZ4dz7uo5Ar0DqV2wNr/3/p1An8AUx2Wz2WhctDGNiza+yyt8APn5wSuvpNtwqU5KRUVZy+ZWrIDTp62ZTbf6++/UjdepE5w5YyWaIiKsJ/stXpxQ/PzoUeuJfPE++8yaJda+veM4I0fCqFHWcsAdO2DaNLh40SqC3rgxvP56mutuiYiIiIiIiEgqhYWGYUaaZPfHJ5rireq56o5jzmw/8y6jghV/r2DFoRWcjjpNnHFMakxuPfmux7+v7NiR8r53qq2UhFQnpfr0gV9/hW7drNlZNluqz5nIgAHJL9f7d3Hyw4dvP5aXl7WsT0RERERERETkVqNXjea11a/xcPDD5MueD1t6JDXuZxUrWokfk3xyEbD6xMamevhUJ6UWLYKff4ZatVJ9LhERERERERERp5m4ZSJTW0+lW4Vuzg7l3nDoUIYOn+qkVM6ckCtXRoQiIiIiIiIiIpJxYmJjqBlS09lh3DsKFcrQ4bPduYuj11+36j9FR2dEOCIiIiIiIiIiGaNPpT7M+HOGs8O4d+3da9VfatDAeg0YYLWlUapnSr3/vvVEu7x5raffubk57t+6Nc2xiIiIiIiIiIhkmGs3r/H51s9Zfmg55fOUx83FManxQZMPnBTZPWD2bOjcGR5+GGrUsNp+/x3KloWZM6Fdu1QPmeqkVJs2qT6HiIiIiIiIiIjT7Ti9g4pBFQHYeWanwz4bKnp+W0OGwLBh8Nprju0jR1r7MiMpNXJkqs8hIiIiIiIiIuJ0K3usdHYI966TJ6F798TtXbvCu++machU15QCuHgRJk2yEmTnz1ttW7fCiRNpikFEREREREREJNMcOH+AJQeWcPXGVQCMMU6O6B4QFga//Za4fc0aqFMnTUOmeqbUjh3QsCH4+cHhw9C3r/U0vjlz4OhRmD49TXGIiIiIiIiIiGSoc9Hn6PhDR1YeWonNZmP/c/spkrMIvef3JqdnTt5v8r6zQ8y6WrWC//wHtmyBRx6x2n7/Hb7/HkaPhvnzHfumQKqTUoMGQc+eMHYs5MiR0N68OTzxRGpHExERERERERHJHC8ueRG3bG4cffEopSeUtrd3eqgTg5YO4n2UlErWs89a/376qfVKah+AzQaxsSkaMtVJqU2b4H//S9yePz9ERKR2NBERERERERGRzLH04FKWdF1CAd8CDu3FcxfnyMUjTorqHhEXl+5DprqmlIcHXL6cuH3fPggMTI+QRERERERERETSX9SNKLzdvBO1n796Hg9XDydEdI+4cQMaNID9+9N12FQnpVq1sp7+d+OGtW2zWbWk/vOfND39T0REREREREQkU9QpWIfp2xOKYduwEWfiGLt2LPVC6zkxsizOzc0qMp7OUp2Uev99iIyEPHng6lV49FEoVsyqL/Xmm+ken4iIiIiIiIhIuhjbaCyfb/2cZt80IyY2hiHLh1D207KsPrKadxq+4+zwsrauXeHLL9N1yFTXlPLzg2XLYO1a2L7dSlBVrmw9kU9PUBQRERERERGRrKpsnrLsG7CP8RvHk8M9B5ExkbQt3Zb+VfuTL0c+Z4eXtd28CZMnw/LlUKUK+Pg47v/gg1QPmeqk1LvvwuDBUKuW9YoXG2slzb79NtUxiIiIiIiIiIhkuKOXjhLiG8IrdV9Jcl9Bv4JOiOoesXOnNSsJrMLit7LZ0jRkmpJSuXJB794JbbGx0LmzFZ+IiIiIiIiISFZU+KPCnHzpJHl88ji0n4s+R+GPChM7ItZJkd0DVq5M9yFTnZT6+Wdo3Nhaxte+vTV7q2NH+OuvDIlPRERERERERCRdGGOwkXhWT2RMJJ6unk6I6B504AAcPAh164KXl1XLKbNmSlWtCrNnQ5s24O5u1bg6cMBKSOXNm6YYREREREREREQyzKAlgwCw2Wy8uvJVvN287fti42LZcGIDFYMqOim6e8S5c9aspJUrrSTU/v1QpIi1lC5nTuvJeKmU6qQUQP36MH06tGsHpUvDr79CQEBaRhIRERERERERyVh/RPwBWDOl/jz9J+4u7vZ97i7uVMhbgZdrvuys8O4NL74Ibm5w9KiVDIrXqRMMGpRxSam2bZNuDwwEf3/o1y+hbc6cVMcgIiIiIiIiIpJhVvaw6g31+rEXHzX9CF8PXydHdA9auhSWLIECBRzbixeHI0fSNGSKklJ+fkm3N2mSpnOKiIiIiIiIiGS6Ka2nODuEe1dUFHh7J24/fx48PNI0ZIqSUlP0nomIiIiIiIjIfWDzP5uZtWsWRy8dJSY2xmHfnE5a/pWsOnWsWk6vv25t22wQFwdjx0K9emkaMk01pQDOnIG9e62vS5a0lvKJiIiIiIiIiGRVM3fOpPvc7jQp1oSlB5fSuGhj9p3bx6nIUzxe+nFnh5e1jR0LDRrA5s0QEwNDhsCuXdZMqbVr0zRkttQeEBUFTz4J+fJZT/+rWxeCg61i69HRaYpBRERERERERCTDvfXbW3zY5EN+Cv8Jdxd3Pmr6EX/1/4uOD3WkoG9BZ4eXtZUtC/v2Qe3a0Lq1lSBq2xb++AOKFk3TkKlOSg0aZD1t76ef4OJF6/Xjj1bbSy+lKQYRERERERERkQx38MJBHivxGGA9dS8qJgqbzcaLj7zI51s/d3J0Wdjhw/DFF/DNN1ZCatYsWLgQ3njDmrWURqlevjd7NvzwA4SFJbQ1bw5eXtCxI3z2WZpjERERERERERHJMDk9c3Ll+hUA8ufIz87TOymXtxwXr10k+oaWfyVp5Upo0QKuXrW2XV1h8mTo2vWuh071TKnoaMibN3F7njxaviciIiIiIiIiWVfdQnVZ9vcyADqU6cALi1+g7/y+hM8Op0HhBk6OLot69VVo1AhOnIBz56BvX6ueVDpI9UypGjVg5Eir4Lqnp9V29SqMHm3tExERERERERHJisY3H8+1m9cAeKXuK7i5uLHu2DralW7H8LrDnRxdFrVzJ6xbl7BM79134X//sxJUuXPf1dApTkq5uMDJkzBuHDRtCgUKQIUK1r7t260E1ZIldxWLiIiIiIiIiEiGyeWVy/51Nls2htYeCkD0jWi2RWyjZkhNZ4WWdV2+DAEBCdve3lYNp0uXMi8pZYz1b7lysH+/Vdvqr7+stvBw6NLFiklERERERERE5F6y/9x+6kypQ+yIWGeHkjUtWQJ+fgnbcXGwYoU1iypeq1apHjbVy/fASor17ZuWI0VERERERERE5J7So0fitqeeSvjaZoPY1Cf0UpWUmjQJsme/fZ/nn091DCIiIiIiIiIikhXFxWXY0KlKSk2caNWWSo7NpqSUiIiIiIiIiIjcWaqSUps3Q548GRWKiIiIiIiIiEj6m793/m33H7pwKJMikVulOClls2VkGCIiIiIiIiIiGaPNzDZ37GNT4iPTpfrpeyIiIiIiIiIi95K4kRlXF0nSLltKO44ceeci5yIiIiIiIiIiIimRqqSUt3dGhiIiIiIiIiIiIlnWxYswaRIMGwbnz1ttW7fCiRNpGi5Vhc5FREREREREROQBtGMHNGwIfn5w+DD07Qu5csGcOXD0KEyfnuohUzxTSkREREREREREHlCDBkHPnrB/P3h6JrQ3bw6rV6dpyFQlpYyxkl/XrqXpXCIiIiIiIiIici/atAmeeipxe/78EBGRpiFTnZQqVgyOHUvTuUREREREREREnOritYtM2jqJYcuHcf6qVRdp68mtnLictrpIDwwPD7h8OXH7vn0QGJimIVOVlMqWDYoXh3Pn0nQuERERERERERGn2XFqByU+KcE7a9/hvfXvcfHaRQDm7JnDsBXDnBtcVteqFbz2Gty4YW3bbNZyuv/8B9q1S9OQqa4p9fbbMHgw7NyZpvOJiIiIiIiIiDjFoCWD6FmxJ/uf24+na0JdpObFm7P6SNrqIj0w3n8fIiMhTx64ehUefdRaTpcjB7z5ZpqGTPXT97p3h+hoqFAB3N3By8txf/wTAUVEREREREREspJN/2zify3+l6g9f478RESmrS7SA8PPD5YtgzVrrCfxRUZC5crWE/nSKNVJqXHj0nwuERERERERERGn8XDx4PL1xHWR9p3bR6BP2uoiPXBq17Ze6SDVSakePdLlvCIiIiIiIiJyH1t9ZDXvrnuXLf9s4WTkSeZ2mkubUm1ue8yqw6sYtGQQu87sIsQ3hOF1h9OzYk+HPhM2TuDdde8SERlBhaAKfNLsE6rlr5aimFqVbMVrq19jVvtZANiwcfTSUf6z/D+0K522ukgPjI8/TrrdZgNPT2spX9264OKS4iFTXVMK4OBBGD4cwsPh9GmrbdEi2LUrLaPBhAkQGmpdQ/XqsHFj8n2/+ALq1IGcOa1Xw4aJ+xsDI0ZAvnzW8sKGDWH//rTFJiIiIiIiIiKpFxUTRYW8FZjwf+3deVxV1f7/8fcRBWTUHECUUtNQckBxwrqp6VdscMhKLXPKrGvpTXEoSzHTwizNunrjm0NqWpq39Fa3TOOrlYZaDqWhVpohKTgUIJCgwO+P9RM8Cco5wj4or+fjsR+w115nnbXtcM9jv+/an33n/BL1/+WPX3TXO3epS/0u2v3Ybo3pMEaPfPiIPvv5s4I+q/auUtT6KE3tNFU7H9uplgEtFbk8Usczj5foPWZ3n62MnAzVfqW2/jz7pzot6aRGrzeSr4evXrjdubpIFcarr0rPPCONGSNNm2a2MWOkSZOkKVOkrl2lkBDpyJESD+lwKPXFF1Lz5tK2bdIHH5hbCCXpu++kqVMdHU1atUqKijKv3bnT1KqKjCwMu/5q0yYThm3cKMXHS8HBUvfu0m8XPLlx1iwT4MXGmnl6e5sxz5xxfH4AAAAAAMBxdzS+QzNun6F7mt5Tov6x38aqQbUGmh05W01rNdWodqN0X+h9enXrqwV95mydoxGtR2hYq2EKrRWq2Ltj5VXFS4t3LS7Re/h7+mvDoA366IGP9Podr2tUu1H6ZOAn+mLoF/J293bqPCuMF1+U2rY1q35OnTLbjz+a1UWvvWaexBcYKI0dW+IhHQ6lnn5amjHD1LZydy9sv/12aetWR0eT5syRRoyQhg2TQkNNkOTlJS0u5vO0YoX0+ONSWJjUpIm0cKGUlyfFxZnj+fmm7tXkyVLv3lKLFtKyZdLRo9LatY7PDwAAAAAAlL34pHh1a2hfNDvyxkjFJ8VLknJyc7Tj6A67PpVsldStYbeCPiV16/W36vG2j2viLRMvek8UY/Jks1rqxhsL2xo1kl55xayWqlfPrBLasqXEQzpcU2rPHumddy5ur11bOnnSsbFycqQdO8zcz6tUydxuF1/Cz1NWlnT2rHTddWb/l1+k5GT74u/+/ia4i4+XBgxwbI4AAAAAAKDQ6dOnlZ5eWCzcw8NDHh4eVzxuckayArwD7NoCfAKUnp2uP8/+qT/O/KHc/NyL+3gHaP/J/SV6j9e3FV0XySabPCt7qtF1jXTbDbfJrVLJ6yJVGMeOSefOXdx+7pwJYiQpKEg6fbrEQzocSlWrZubRoIF9+65dUt26jo118qSUmysF2H+eFBAg7S/Z50lPPWXO+XwIdf7foagxk4t5umN2drays7ML9jPO35MIAAAAAADshIaG2u1PnTpVzz33nGsm46BXt76qE5knlHU2S9WrVpck/fHnH/Kq4iUfdx8dzzyuhtUbauOQjQr2D3bxbP/it99MCPLpp2aFTqNG0ltvSW3amOP5+aY20oIFUmqqdMst0htvSI0bl877d+kiPfaYuWWtVSvTtmuXNHKkuX1OMiuZ/hoYXYLDt+8NGGD+DZKTTYH1vDyzMmv8eGnwYEdHuzIzZ0orV0pr1pgi6c6KiYmRv79/wdauXcmq9gMAAAAAUNEkJCQoLS2tYJt04e1PVyDQJ1ApmSl2bSkZKfLz8FPVKlVV06um3GxuF/fJTFGgT2CJ3uPF219U27pt9dPon3Rq4imdmnhKP47+Ue3rtddrPV5T4thEBfoEauxnJa+LZIk//jAhU5UqJpRKSJBmzzZPgDuvrAtsL1pkblMLD5c8PMzWpo1pW7TI9PHxMfMqIYdDqRdfNLWcgoNNkfPQUPPEv44dze2FjqhZ0zwpMMX+86SUFFMb61JeecWEUuvXm7pR551/nSNjTpo0ye4PavulHv8HAAAAAEAF5uvrKz8/v4KtNG7dk6SIehGK+yXOrm3DoQ2KqBchSXJ3c1d4ULjiDhX2ycvPU9yhuII+lzN542S9GvmqbryusC5So+sa6ZX/eUWT4iapnl89zfqfWdpypOR1kSzx0ksmiHnrLaldO7MaqXv3wvpOVhTYDgw0BcYTEqTVq82WkGCCmfO3q3XpYuZVQg6HUu7uZiXYwYPSxx9Ly5ebW+3eftsETI6OFR5eWKRcKixaHnGJz9OsWdL06dK6dYWr1M5r0MD8O104Znq6CQmLG9PDw8PuD8rHx8exEwEAAAAAAHYycjK0O3m3difvliT98scv2p28W4lpiZKkSZ9P0uA1hbdc/b3N33Xoj0OauGGi9p/cr3998y+998N7GtuhcNVSVIcoLdi5QEt3L9W+E/s08uORyjybqWFhw0o0p2Onj+lc3sV1kc7lnVNyhqn5E+QbpNPZJa+LZIkPPzQByP33m6LerVqZcOa8yxXYLk1Nmki9epktJOSKhnK4ptR5119vQjrJ3MbnrKgoacgQ82/brp0J9jIzzdP4JHNLYN26UkyM2X/pJSk62hRbr1+/sE6Uj4/ZbDZpzBjzhMDGjU1INWWKqTvVp4/z8wQAAAAAACX37dFv1WVpl4L9qPVRkqQhLYdoSZ8lOpZxrCCgkqQG1Rvovw/+V2M/G6vXtr2men71tLDXQkU2iizo079Zf53IOqHoTdFKzkhWWGCY1g1cpwCfvxSWLkaXBl302MePaWHPhWpVx9RF2nVsl0b+d6Rub2DqIu1J2aMG1UteF+lKXFgwXrpE0fhDh0x9qKgo6ZlnpG++kf7xD7PaZ8gQ5wpsOyMpyQRkiYnm6XUXmjPH4eGcCqUWLTJPAfzpJ7PfuLEJgh55xPGx+veXTpwwQVNyshQWZlZAnf93TEw0T+Q77403zHnfd5/9OFOnSufrqk2caIKtRx81tb1uvdWMeSV1pwAAAAAAQMl1rt9Z+VPziz2+pM+SIl+z67Fdlxx3VLtRGtVulFNzWtRrkQatGaTwN8NVxa2KJLNKqmuDrlrUy9RF8nH30ezuJa+LdCWCg+2LqRdbND4vz6zmefFFs9+qlbR3r6kfNWRI2U9UMrek9eolNWxobplr1kw6fNjcOti6tVNDOhxKRUeb8Gv06MLb4eLjpbFjTYD0/POOT2LUKLMVZdMm+/3Dhy8/ns1m5uHMXAAAAAAAwLUp0CdQGwZt0P6T+/XjqR8lSSE1QhRSs/A2tC4NuhT38lJ35MgR+fn5FewXW5+rTh1T1PtCTZtK779vfr+wwHadOoV9UlLM6p/SMGmSecrdtGmSr69579q1pYEDpR49nBrS4VDqjTfMbYsPPFDY1quXqaE1ejRBEAAAAAAAKN+a1GyiJjWbuHoaBbWtL+uWW6QDB+zbfvxRuuEG8/uFBbbPh1DnC2yPHFk6k923T3r3XfN75crSn3+aOkrPP2+KqzvxPg6HUmfPXlxcXDIFy89dXCsMAAAAAACg3EhKT9KHBz5UYlqicnLt6yLNiXS8LpIlxo6VOnY0t+/16ydt3y69+abZJGsKbHt7F9aRqlPHPAHv5pvN/smTTg3pcCg1aJBZLfXX+lVvvmlWbAEAAAAAAJRHcYfi1GtlLzWs3lD7T+5Xs9rNdDj1sPLz89W6jnN1kSzRtq20Zo25he75503oNHeufRBT1gW2O3SQNm82tw3eeac0bpy0Z4/0wQfmmBOcLnS+fn3he27bZupJDR5sCsGf50ThdQAAAAAAgDIxKW6SxkeM17Qu0+Qb46v3+72v2t61NfCDgepxo3N1kSxz991mK05ZF9ieM0fKyDC/T5tmfl+1yqzMcjIAcjiU2ru3sKj6wYPmZ82aZtu7t7CfzebUfAAAAAAAAMrEvpP79O69pi5S5UqV9efZP+Xj7qPnOz+v3it7a2TbUqq/dK3JzZWSkkxBccncyhcbe8XDOhxKbdx4xe8JAAAAAABgOe8q3gV1pOr41NHBPw7q5tqmLtLJLOfqIlUIbm5S9+6m2Hm1aqU2rFO37wEAAAAAAFxtOtTroM2Jm9W0VlPd2fhOjVs/TntS9uiD/R+oQz3n6iJVGM2aSYcOmXpWpYRQCgAAAAAAVAhzIucoI8fURZrWeZoycjK06odValyjseZ0pzD2Jc2YIY0fL02fLoWHm1v4LuTn5/CQhFIAAAAAAOCal5uXq6T0JLUIMHWRvN29FXv3lddFqjDuvNP87NXLvpB4fr7Zz811eEhCKQAAAAAAcM1zq+Sm7m93174n9qmaZzVXT+fqUwZFxh0OpTIzL16hBQAAAAAAUN41q91Mh/44pAbVS68uUoXRqVOpD1nJ0RcEBEgPPyxt3lzqcwEAAAAAACgzM26fofEbxuvjHz/WsdPHlJ6dbrfhMr76SnroIaljR+m330zb2287HRI5vFJq+XJpyRLp9tul+vVNQDV4sBQU5NT7AwAAAAAAWOLOFaYuUq93e8l2QV2k/Px82Ww25UY7Xhepwnj/fWnQIGngQGnnTik727SnpUkvvih98onDQzocSvXpY7YTJ0wYtmSJNGWKFBlpAqpevaTKVKoCAAAAAADlzMYhpV8XqcKYMUOKjTUrk1auLGy/5RZzzAlOx0e1aklRUWb75z+lCRNMKFazpvT3v0tPPy15eTk7OgAAAAAAQOnqVL/06yJVGAcOSLfddnG7v7+UmurUkA7XlDovJUWaNUsKDTUB1H33SXFx0uzZ0gcfmNVUAAAAAAAA5clXv36lhz54SB0XddRv6aYu0tvfva3NiRTPvqTAQOnnny9u37xZatjQqSEdDqU++EDq2VMKDpbeeUd6/HFT22r5cqlLF3N74X/+I23a5NR8AAAAAAAAysT7Ce8rcnmkqlauqp3Hdio719RFSstO04tfveji2ZVzI0ZITz4pbdsm2WzS0aPSihXS+PHSyJFODenw7XvDhkkDBkhbtkht2xbdJyhIevZZp+YDAAAAAABQJmZ8NUOxd8dqcMvBWvlDYV2kW4Jv0YwvnauLVGE8/bSUlyd17SplZZlb+Tw8TCg1erRTQzocSh07dvlaUVWrSlOnOjUfAAAAAACAMnHg5AHddsPFdZH8Pf2VeibV+gldTWw2swJpwgRzG19Ghqnp5OPj9JAOh1Lnzknp6UXPzcNDcnd3ei4AAAAAAABlJtAnUD///rPqV6tv1745cbMaVneuLlKFsXy51LevWakUGloqQzpcU6paNal69Yu3atXMCqkbbjCrpPLySmV+AAAAAAAApWJE6xF6ct2T2pa0TTbZdPT0Ua34foXGrx+vkW2cq4tUYYwdK9WuLT34oPTJJ1Ju7hUP6fBKqSVLzGqtoUOldu1M2/bt0tKl0uTJ0okT0iuvmFVTzzxzxfMDAAAAAAAoFU/f+rTy8vPUdVlXZZ3N0m1v3SaPyh4aHzFeo9s7Vxepwjh2TFq3Tnr3XalfP7Ni6v77pYEDpY4dnRrS4VBq6VJp9mzz/uf17Ck1by797/9KcXHS9ddLL7xAKAUAAAAAAMoPm82mZ297VhNumaCff/9ZGTkZCq0VKh935+siVRiVK0t33222rCxpzRrpnXekLl2kevWkgwcdHtLh2/e+/lpq1eri9latpPh48/utt0qJiQ7PBQAAAAAAoMws/365ss5myd3NXaG1QtWubjsCKWd4eUmRkdIdd0iNG0uHDzs1jMOhVHCwtGjRxe2LFpljknTqlKkzBQAAAAAAUF6M/Wysar9cWw++/6A++ekT5eZdeV2kCiUrS1qxQrrzTqluXWnuXOmee6QffnBqOIdv33vlFXPL4KefSm3bmrZvv5X275f+/W+z/803Uv/+Ts0HAAAAAACgTBwbd0zrfl6nd/e+q36r+8mripfuD71fA1sMVMdg5+oiVRgDBkgff2xWSfXrJ02ZIkVEXNGQDodSvXpJBw6Y+lEHDpi2O+6Q1q6V6tc3+yMpWA8AAAAAAMqZypUq6+6b7tbdN92trLNZWrNvjd7Z+466LO2ien71dPAfjtdFqjDc3KT33jO37bm52R/bu1dq1szhIR0Kpc6elXr0kGJjpZgYh98LAAAAAACgXPCq4qXIRpH648wf+jX1V+07uc/VUyrfVqyw3z992jyJb+FCaccOKdfxWyEdCqWqVJG+/97h9wAAAAAAACgXzq+QWrFnheJ+iVOwX7AeaPaA/t3i366e2tXhyy9NYfH335eCgqS+faX5850ayuHb9x56yLz3zJlOvR8AAAAAAIBLDPj3AH3848fyquKlfjf305Tbpigi+MrqIlUIycnSkiUmEEpPNzWlsrNNLafQUKeHdTiUOndOWrxY+vxzKTxc8va2Pz5njtNzAQAAAAAAKDNuldz03v3vKfLGSLlVsq+LtPf4XjWr7XhdpGtez55mddRdd5mn7fXoYWpKxcZe8dAOh1J790qtW5vff/zR/pjNdsXzAQAAAAAAKBMr+trXRTqdfVrv7n1XC3cu1I5jO5Qb7XhdpGvep59K//iHeapd48alOrTDodTGjaX6/gAAAAAAAJb68tcvtWjXIr2f8L6CfIPUt2lfzb/TubpI17zNm81te+HhUtOm0qBB0oABpTK0w6HUeT//LB08KN12m1S1qpSfz0opAAAAAABQPiVnJGvJ7iVatGuR0rPT1S+0n7Jzs7V2wFqF1nK+LtI1r0MHs82dK61aZWo6RUVJeXnShg1ScLDk6+vU0JUcfcGpU1LXrtJNN0l33ikdO2bahw+Xxo1zag4AAAAAAABlpue7PRUyL0Tfp3yvuZFzdTTqqP555z9dPa2ri7e39PDDZuXUnj0mBJo5U6pdW+rVy6khHQ6lxo6VqlSREhMlL6/C9v79pXXrnJoDAAAAAABAmfn0p081vNVwTes8TXfddNdFRc7hoJAQadYsKSlJevddp4dxOJRav1566SWpXj379saNpV9/dXoeAAAAAAAAZWLzw5t1Ovu0wt8MV/uF7TVv+zydzDrp6mld/dzcpD59pA8/dOrlDodSmZn2K6TO+/13ycPDqTkAAAAAAACUmQ71OmhBrwU6Nu6YHgt/TCv3rlTQ7CDl5edpw8ENOp192tVTrJAcDqX+9jdp2bLCfZvN1LaaNUvq0qU0pwYAAAAAAFB6vN299XCrh7X54c3aM3KPxkWM08wtM1X7ldrq9a5zdZHgPIefvjdrlil0/u23Uk6ONHGi9MMPZqXUli1lMUUAAAAAAIDSFVIzRLP+Z5Ziusboox8/0uJdi109pQrH4ZVSzZpJP/4o3Xqr1Lu3uZ2vb19p1y7pxhvLYooAAAAAAABlw62Sm/o06aMPH3CuLhKc5/BKKUny95eefba0pwIAAAAAAICKwqlQKjVV2r5dOn7c1JO60ODBpTArAAAAAKhobDZXzwCukJ/v6hkALuNwKPXRR9LAgVJGhuTnZ/+/mzYboRQAAAAAAAAuz+GaUuPGSQ8/bEKp1FTpjz8Kt99/L4MZAgAAAAAA4JrjcCj122/SP/4heXmVxXQAAAAAAABQETh8+15kpPTtt1LDhmUxHQAAAAAAcC2Zv32+Xv76ZSVnJKtlYEv9845/ql3ddkX27byks7749YuL2u9sfKf+++B/JUlD1w7V0u+W2h2PvDFS6x5aV/qTR5lyOJS66y5pwgQpIUFq3lyqUsX+eK9ejo03f7708stScrLUsqX0z39K7Yr+bOqHH6ToaGnHDunXX6VXX5XGjLHv89xz0rRp9m0hIdL+/Y7NCwAAAAAAXJlVe1cpan2UYu+KVft67TV361xFLo/UgVEHVNu79kX9P+j/gXJycwr2T2WdUsvYlro/9H67fj0a9dBbvd8q2Pdw8yi7k0CZcTiUGjHC/Hz++YuP2WxSbm7Jx1q1SoqKkmJjpfbtpblzzUqsAwek2hd/NpWVZVZo3X+/NHZs8ePefLP0+eeF+5WdesYgAAAAAAC4EnO2ztGI1iM0rNUwSVLs3bH670//1eJdi/X0rU9f1P+6qtfZ7a/cu1JeVbwuCqU83DwU6BNYdhOHJRyuKZWXV/zmSCAlSXPmmJBr2DApNNSEU15e0uLFRfdv29asqhowQPK4RAhaubIUGFi41azp2LwAAAAAAMCVycnN0Y6jO9StYbeCtkq2SurWsJvik+JLNMaiXYs0oNkAebt727VvOrxJtV+urZB5IRr58UidyjpVqnOHNRwOpUpLTo65Da9b4WdTlSqZ/fiSfTaL9dNPUlCQWVU1cKCUmHhl4wEAAAAAAOP06dNKT08v2LKzs4vsdzLrpHLzcxXgHWDXHuAdoOSM5Mu+z/bftmvv8b16pPUjdu09GvXQsnuWKW5wnF7q9pK++PUL3bHiDuXmObhSBi5X4lDqzjultLTC/ZkzpdTUwv1Tp8xqp5I6edKsrAqw/2wqIMDUl3JW+/bSkiXSunXSG29Iv/wi/e1v0unTxb8mOzvb7g8qIyPD+QkAAAAAAHANCw0Nlb+/f8EWExNTJu+zaOciNa/d/KKi6AOaDVCvkF5qHtBcfZr00ccPfqxvjn6jTYc3lck8UHZKXG3ps8+kC8PPF1+U+vWTqlUz++fOmVpQrnbHHYW/t2hhQqobbpDee08aPrzo18TExGjaX6ujAwAAAACAiyQkJKhu3boF+x7F1Nep6VVTbjY3pWSm2LWnZKZcth5UZk6mVv6wUs93LqKg9V80rN5QNb1q6ufff1bXhl1LcAYoL0q8Uio//9L7jqpZU3Jzk1LsP5tKSTF1oEpLtWrSTTdJP/9cfJ9JkyYpLS2tYNu+fXvpTQAAAAAAgGuIr6+v/Pz8CrbiQil3N3eFB4Ur7lBcQVtefp7iDsUpol7EJd9jdcJqZZ/L1kMtHrrsfJLSk3Qq65Tq+NZx7ETgci6rKeXuLoWHS3GFn03l5Zn9iEt/Nh2SkSEdPCjVucRn08PDw+4PysfHp/QmAAAAAABABRXVIUoLdi7Q0t1Lte/EPo38eKQyz2ZqWJh5Gt/gNYM16fNJF71u0a5F6tOkj2p41bBrz8jJ0IT1E7Q1aasOpx5W3KE49V7ZW42ua6TIGyMtOSeUnhLfvmezme2vbVciKkoaMkRq00Zq106aO1fKzDRP45OkwYOlunWl87en5uRICQmFv//2m7R7t+TjIzVqZNrHj5d69jS37B09Kk2dalZkPfDAlc0VAAAAAAA4pn+z/jqRdULRm6KVnJGssMAwrRu4TgE+psB0YlqiKtns18scOHlAmxM3a/1D6y8az83mpu+Pf6+l3y1V6plUBfkGqfuN3TW9y3R5VC56xRbKrxKHUvn50tCh0vlVeWfOSH//u+T9/5/KWEyx/Uvq3186cUKKjjbFzcPCTIHy88XPExPNE/nOO3pUatWqcP+VV8zWqZO0aZNpS0oyAdSpU1KtWtKtt0pbt5rfAQAAAACAtUa1G6VR7UYVeWzT0E0XtYXUDFH+1KJrBlWtUlWfPfRZaU4PLlTiUGrIEPv9h4q4rXPwYMcnMGqU2YpyPmg6r379y9eyWrnS8TkAAAAAAADAWiUOpd56qyynAQAAAAAAgIrEZYXOAQAAAAAAUHERSgEAAAAAAMByhFIAAAAAAACwHKEUAAAAAAAALEcoBQAAAAAAAMsRSgEAAAAAAMByhFIAAAAAAACwHKEUAAAAAAAALEcoBQAAAAAAAMsRSgEAAAAAAMByhFIAAAAAAABXk5kzJZtNGjOmsO3MGemJJ6QaNSQfH+nee6WUFJdNsSQIpQAAAAAAAK4W33wj/e//Si1a2LePHSt99JG0erX0xRfS0aNS376umWMJEUoBAAAAAABcDTIypIEDpQULpOrVC9vT0qRFi6Q5c6Tbb5fCw6W33pK+/lrautV1870MQikAAAAAAICrwRNPSHfdJXXrZt++Y4d09qx9e5Mm0vXXS/Hx1s7RAZVdPQEAAAAAAICKKD093W7fw8NDHh4eRXdeuVLaudPcvvdXycmSu7tUrZp9e0CAOVZOsVIKAAAAAADABYKDg+Xv71+wxcTEFN3xyBHpySelFSskT09rJ1mGWCkFAAAAAADgAkeOHJGfn1/BfrGrpHbskI4fl1q3LmzLzZW+/FKaN0/67DMpJ0dKTbVfLZWSIgUGlsncSwOhFAAAAAAAgAv4+fnZhVLF6tpV2rPHvm3YMFM36qmnpOBgqUoVKS5Ouvdec/zAASkxUYqIKP2JlxJCKQAAAAAAgPLM11dq1sy+zdtbqlGjsH34cCkqSrruOsnPTxo92gRSHTpYP98SIpQCAAAAAAC42r36qlSpklkplZ0tRUZK//qXq2d1SYRSAAAAAAAAV5tNm+z3PT2l+fPNdpXg6XsAAAAAAACwHKEUAAAAAAAALEcoBQAAAAAAAMsRSgEAAAAAAMByhFIAAAAAAACwHKEUAAAAAAAALEcoBQAAAAAAAMsRSgEAAAAAAMByhFIAAAAAAACwHKEUAAAAAAAALEcoBQAAAAAAAMsRSgEAAAAAAMByhFIAAAAAAACwHKEUAAAAAAAALEcoBQAAAAAAAMsRSgEAAAAAAMByhFIAAAAAAACwHKEUAAAAAAAALEcoBQAAAAAAAMsRSgEAAAAAAMByhFIAAAAAAACwHKEUAAAAAAAALEcoBQAAAAAAAMsRSgEAAAAAAMByhFIAAAAAAKDMzN8+X/Xn1pfnDE+1X9he23/bXmzfJbuXyDbNZrd5zvC065Ofn6/ojdGqM7uOqr5QVd2WddNPp34q69NAGXB5KDV/vlS/vuTpKbVvL20v/rOpH36Q7r3X9LfZpLlzr3xMAAAAAABQNlbtXaWo9VGa2mmqdj62Uy0DWipyeaSOZx4v9jV+Hn46Nu5YwfbrmF/tjs/aMkuvb3tdsXfFatsj2+Tt7q3I5ZE6c+5MWZ8OSplLQ6lVq6SoKGnqVGnnTqllSykyUjpezGczK0tq2FCaOVMKDCydMQEAAAAAQNmYs3WORrQeoWGthim0Vqhi746VVxUvLd61uNjX2GRToE9gwRbgE1BwLD8/X3O3zdXk2yard5PeahHQQsv6LNPR00e1dv9aC84IpcmlodScOdKIEdKwYVJoqBQbK3l5SYuL+Wy2bSu9/LI0YIDk4VE6YwIAAAAAgNKXk5ujHUd3qFvDbgVtlWyV1K1hN8UnxRf7uoycDN0w9wYFvxqs3it764fjPxQc+yX1FyVnJNuN6e/pr/b12iv+SPFjonxyWSiVkyPt2CF1K/wcqVIlsx/v5OeoLMYEAAAAAACFTp8+rfT09IItOzu7yH4ns04qNz9XAd4Bdu0B3gFKzkgu8jUhNUK0uPdi/WfAf7T8nuXKy89Tx8UdlZSeJEkFrytyzMyix0T55bJQ6uRJKTdXCrD/HCkgQEp28nPk7JjZ2dl2f1AZGRnOTQAAAAAAgGtcaGio/P39C7aYmJhSGzsiOEKDWw5WWGCYOtXvpA/6faBaXrX0v9/+b6m9B8qPyq6eQHkQExOjadOmuXoaAAAAAACUewkJCapbt27Bvkcx9XVqetWUm81NKZkpdu0pmSkK9CmmUPRfVHGrolZ1WunnP36WpILXpWSmqI5vHbsxwwLCHDkNlAMuWylVs6bk5ial2H82lZJSfBHzshpz0qRJSktLK9i287g+AAAAAACK5OvrKz8/v4KtuFDK3c1d4UHhijsUV9CWl5+nuENxiqgXUaL3ys3L1Z6UParjYwKoBtUaKNAn0G7M9Ox0bUvapojgko2J8sNloZS7uxQeLsUVfo6Ul2f2I5z8HDk7poeHh90flI+Pj3MTAAAAAAAABaI6RGnBzgVaunup9p3Yp5Efj1Tm2UwNCxsmSRq8ZrAmfT6poP/zXzyv9QfX69Afh7Tz2E49tOYh/Zr2qx5p/YgkyWazaUz7MZrx1Qx9eOBD7UnZo8FrBivIN0h9mvRxxSniCrj09r2oKGnIEKlNG6ldO2nuXCkz0zw5T5IGD5bq1pXO356akyMlJBT+/ttv0u7dko+P1KhRycYEAAAAAADW6N+sv05knVD0pmglZyQrLDBM6wauU4CPKQadmJaoSrbC9TJ//PmHRnw0QskZyaruWV3hQeH6+uGvFVortKDPxFsmKvNsph796FGlnknVrdffqnUPrZNnZU/Lzw9Xxpafn5/vygnMmye9/LIpRB4WJr3+utS+vTnWubNUv760ZInZP3xYatDg4jE6dZI2bSrZmCWxb98+hYaGKiEhQU2bNnXirKxns7l6BnCJ5/gPXxHlP+fqGcAlXPt1DRfiO76C4ju+QuI7voK6ir7jk5KSFBwcrCNHjqhevXquns5VLT09Xf7+/kpLS5Ofn5+rp+MyLi90PmqU2YpyYdAkmYCqJH+vlxoTAAAAAAAArueymlIAAAAAAACouAilAAAAAAAAYDlCKQAAAAAAAFiOUAoAAAAAAACWI5QCAAAAAACA5QilAAAAAAAAYDlCKQAAAAAAAFiOUAoAAAAAAACWI5QCAAAAAACA5QilAAAAAAAAYDlCKQAAAAAAAFiOUAoAAAAAAACWI5QCAAAAAACA5QilAAAAAAAAYDlCKQAAAAAAAFiOUAoAAAAAAACWI5QCAAAAAACA5QilAAAAAAAAYDlCKQAAAAAAAFiOUAoAAAAAAACWI5QCAAAAAACA5QilAAAAAAAAYDlCKQAAAAAAAFiOUAoAAAAAAACWI5QCAAAAAACA5QilAAAAAAAAYDlCKQAAAAAAAFiOUAoAAAAAAACWI5QCAAAAAACA5QilAAAAAAAAyrOYGKltW8nXV6pdW+rTRzpwwL7PmTPSE09INWpIPj7SvfdKKSkumW5JEUoBAAAAAACUZ198YQKnrVulDRuks2el7t2lzMzCPmPHSh99JK1ebfofPSr17eu6OZdAZVdPAAAAAAAAAJewbp39/pIlZsXUjh3SbbdJaWnSokXSO+9It99u+rz1ltS0qQmyOnSwfMolwUopAAAAAACAq0lamvl53XXm544dZvVUt26FfZo0ka6/XoqPt35+JcRKKQAAAAAAABdIT0+32/fw8JCHh8elX5SXJ40ZI91yi9SsmWlLTpbc3aVq1ez7BgSYY+UUK6UAAAAAAABcIDg4WP7+/gVbTEzM5V/0xBPS3r3SypVlP8EyxkopAAAAAAAAFzhy5Ij8/PwK9i+7SmrUKOnjj6Uvv5Tq1StsDwyUcnKk1FT71VIpKeZYOcVKKQAAAAAAABfw8/Oz24oNpfLzTSC1Zo30f/8nNWhgfzw8XKpSRYqLK2w7cEBKTJQiIsruBK4QK6UAAAAAAADKsyeeME/W+89/JF/fwjpR/v5S1arm5/DhUlSUKX7u5yeNHm0CqXL65D2JUAoAAAAAAKB8e+MN87NzZ/v2t96Shg41v7/6qlSpknTvvVJ2thQZKf3rX1bO0mGEUgAAAAAAAOVZfv7l+3h6SvPnm+0qQU0pAAAAAAAAWI5QCgAAAAAAAJYjlAIAAAAAAIDlCKUAAAAAAABgOUIpAAAAAAAAWI5QCgAAAAAAAJYjlAIAAAAAAIDlykUoNX++VL++5OkptW8vbd9+6f6rV0tNmpj+zZtLn3xif3zoUMlms9969Cir2QMAAAAAgOLM3z5f9efWl+cMT7Vf2F7bfyv+on/BjgX621t/U/WXqqv6S9XVbVm3i/oPXTtUtmk2u63Hci76r0YuD6VWrZKioqSpU6WdO6WWLaXISOn48aL7f/219MAD0vDh0q5dUp8+Ztu7175fjx7SsWOF27vvlvWZAAAAAACAC63au0pR66M0tdNU7Xxsp1oGtFTk8kgdzyz6on/Tr5v0QLMHtHHIRsUPj1ewf7C6v91dv6X/ZtevR6MeOjbuWMH27r1c9F+NXB5KzZkjjRghDRsmhYZKsbGSl5e0eHHR/V97zQROEyZITZtK06dLrVtL8+bZ9/PwkAIDC7fq1cv+XAAAAAAAQKE5W+doROsRGtZqmEJrhSr27lh5VfHS4l1FX/Sv6LtCj7d9XGGBYWpSs4kW9lyovPw8xf0SZ9fPw81DgT6BBVv1qlz0X41cGkrl5Eg7dkjduhW2Vapk9uPji35NfLx9f8msrPpr/02bpNq1pZAQaeRI6dSp4ueRnZ2t9PT0gi0jI8Op8wEAAAAA4Fp3+vRpu2vo7OzsIvvl5OZox9Ed6taw8CK+kq2SujXspvikYi76/yLrbJbO5p3VdVWvs2vfdHiTar9cWyHzQjTy45E6lXWJi36UWy4NpU6elHJzpYAA+/aAACk5uejXJCdfvn+PHtKyZVJcnPTSS9IXX0h33GHeqygxMTHy9/cv2Nq1a+f8SQEAAAAAcA0LDQ21u4aOiYkpst/JrJPKzc9VgLf9RXyAd4CSM4q56P+Lpz5/SkG+QXbBVo9GPbTsnmWKGxynl7q9pC9+/UJ3rLhDuXnFXPSj3Krs6gmUhQEDCn9v3lxq0UK68Uazeqpr14v7T5o0SVFRUQX7Bw4cIJgCAAAAAKAICQkJqlu3bsG+h4dHmbzPzM0ztXLvSm0aukmelT0L2gc0K7zobx7QXC0CWujG12/UpsOb1LVhERf9KLdculKqZk3JzU1KSbFvT0kxdaCKEhjoWH9JatjQvNfPPxd93MPDQ35+fgWbj49PyU8CAAAAAIAKxNfX1+4aurhQqqZXTbnZ3JSSaX8Rn5KZokCfS1zES3rl61c0c/NMrR+0Xi0CWlyyb8PqDVXTq6Z+/r2Yi36UWy4NpdzdpfBwc5vdeXl5Zj8ioujXRETY95ekDRuK7y9JSUmmplSdOlc+ZwAAAAAAcHnubu4KDwpX3KHCi/i8/DzFHYpTRL3iL+JnbZml6V9O17qH1qlNUJvLvk9SepJOZZ1SHV8u+q82Ln/6XlSUtGCBtHSptG+fKUqemWmexidJgwdLkyYV9n/ySWndOmn2bGn/fum556Rvv5VGjTLHMzLMk/m2bpUOHzYBVu/eUqNGpiA6AAAAAACwRlSHKC3YuUBLdy/VvhP7NPLjkco8m6lhYeaif/CawZr0eeFF/0ubX9KUjVO0uNdi1a9WX8kZyUrOSFZGjnkgWUZOhiasn6CtSVt1OPWw4g7FqffK3mp0XSNF3shF/9XG5TWl+veXTpyQoqNNsfKwMBM6nS9mnphonsh3XseO0jvvSJMnS888IzVuLK1dKzVrZo67uUnff29CrtRUKShI6t5dmj5dKqPbXAEAAAAAQBH6N+uvE1knFL0pWskZyQoLDNO6gesU4GMu+hPTElXJVnjR/8a3bygnN0f3rb7Pbpypnabquc7Pyc3mpu+Pf6+l3y1V6plUBfkGqfuN3TW9y3R5VOai/2pjy8/Pz3f1JMqbffv2KTQ0VAkJCWratKmrp1MiNpurZwCXeI7/8BVR/nOungFcgq/rCovv+AqK7/gKie/4Cuoq+o5PSkpScHCwjhw5onr16rl6Ole19PR0+fv7Ky0tTX5+fq6ejsu4/PY9AAAAAAAAVDyEUgAAAAAAALAcoRQAAAAAAAAsRygFAAAAAAAAyxFKAQAAAAAAwHKEUgAAAAAAALAcoRQAAAAAAAAsRygFAAAAAAAAyxFKAQAAAAAAwHKEUgAAAAAAALAcoRQAAAAAAAAsRygFAAAAAAAAyxFKAQAAAAAAwHKEUgAAAAAAALAcoRQAAAAAAAAsRygFAAAAAAAAyxFKAQAAAAAAwHKEUgAAAAAAALAcoRQAAAAAAAAsRygFAAAAAAAAyxFKAQAAAAAAwHKEUgAAAAAAALAcoRQAAAAAAAAsRygFAAAAAAAAyxFKAQAAAAAAwHKEUgAAAAAAALAcoRQAAAAAAAAsRygFAAAAAAAAyxFKAQAAAAAAwHKEUgAAAAAAALAcoRQAAAAAAAAsRygFAAAAAAAAyxFKAQAAAAAAwHKEUgAAAAAAALAcoRQAAAAAAAAsRygFAAAAAAAAyxFKAQAAAAAAwHKEUgAAAAAAALAcoRQAAAAAAAAsRygFAAAAAAAAyxFKAQAAAAAAwHKEUgAAAAAAALAcoRQAAAAAAAAsRygFAAAAAAAAyxFKAQAAAAAAwHKEUgAAAAAAALAcoRQAAAAAAAAsVy5Cqfnzpfr1JU9PqX17afv2S/dfvVpq0sT0b95c+uQT++P5+VJ0tFSnjlS1qtStm/TTT2U2fQAAAAAAUIz52+er/tz68pzhqfYL22v7b5e+6F/9w2o1mddEnjM81fyN5vrkJ/uL/vz8fEVvjFad2XVU9YWq6rasm346VUEu+h0NUMo5l4dSq1ZJUVHS1KnSzp1Sy5ZSZKR0/HjR/b/+WnrgAWn4cGnXLqlPH7Pt3VvYZ9Ys6fXXpdhYads2ydvbjHnmjBVnBAAAAAAAJGnV3lWKWh+lqZ2maudjO9UyoKUil0fqeGbRF/1fH/laD7z/gIa3Gq5dj+1Sn5A+6rOyj/YeL7zon7Vlll7f9rpi74rVtke2ydvdW5HLI3Xm3DV+0e9ogHIVcHkoNWeONGKENGyYFBpqgiQvL2nx4qL7v/aa1KOHNGGC1LSpNH261Lq1NG+eOZ6fL82dK02eLPXuLbVoIS1bJh09Kq1da9VZAQAAAACAOVvnaETrERrWaphCa4Uq9u5YeVXx0uJdRV/0v7btNfVo1EMTbpmgprWaavrt09W6TmvN224u+vPz8zV321xNvm2yejfprRYBLbSszzIdPX1Ua/evtfDMXMDRAOUqUNmVb56TI+3YIU2aVNhWqZK53S4+vujXxMebYPBCkZGFgdMvv0jJyWaM8/z9zaq2+HhpwICLx8zOzlZ2dnbBflpamiTp4MGDTpwVYKE0V08ArpDu6gnAJX7bt8/VUwBgJb7jKyS+4yumq+k7Pjk5WZKUmpoqPz+/gnYPDw95eHhc1D8nN0c7ju7QpFsLL/or2SqpW8Nuik8q+qI//ki8oiLsL/ojb4zU2gNrJUm/pP6i5IxkdWtYeNHv7+mv9vXaK/5IvAY0K+KivxxLT7f/yy/u39KpAOUq4NJQ6uRJKTdXCgiwbw8IkPbvL/o1yclF9///fxsFPy/V569iYmI0bdq0i9p79ux5mTMAXOxVV08AruDv6gnANUJDXT0DAFbiO75C4ju+groKv+ObN29utz916lQ999xzF/U7mXVSufm5CvC2v0AP8A7Q/pNFX/QnZyRf3N8nQMkZyQXHz4/x1zGTM4u56C/HgoOD7faL+7d0KkC5Crg0lCovJk2apKgLll/l5OToq6++UuPGjeXm5ubCmQEoSkZGhtq1a6ft27fLx8fH1dMBAAClhO94oHzLy8tTUlKS2rRpoypVqhS0F7myB5fk6+ur48ePy93dXTabraC9ov1bujSUqllTcnOTUlLs21NSpMDAol8TGHjp/ud/pqSYp+9d2CcsrOgxi1oed88995TsJABY7vwS15CQELtlwwAA4OrGdzxQ/t18880l7lvTq6bcbG5KybS/iE/JTFGgT9EX/YE+gRf3zyjsf/5nSmaK6vgWXvSnZKYoLCCsxHNzNZvNplq1apX8Bc4EKFcBlxY6d3eXwsOluLjCtrw8sx8RUfRrIiLs+0vShg2F/Rs0MP89LuyTnm6ewlfcmAAAAAAAoHS5u7krPChccYcKL9Dz8vMUdyhOEfWKvkCPCI5Q3C/2F/0bDm0o6N+gWgMF+gTajZmena5tSdsUEXwNX/Q7E6BcBVx++15UlDRkiNSmjdSunXlyXmamKSYvSYMHS3XrSjExZv/JJ6VOnaTZs6W77pJWrpS+/VZ6801z3GaTxoyRZsyQGjc2IdWUKVJQkNSnjwtOEAAAAACACiqqQ5SGrB2iNkFt1K5uO83dOleZZzM1LMxc9A9eM1h1fesqppu56H+y/ZPqtKSTZn89W3fddJdW7l2pb49+qzd7mot+m82mMe3HaMZXM9S4RmM1qNZAUzZOUZBvkPo06eOq07TG5QKUq5DLQ6n+/aUTJ6ToaFOIPCxMWreusHZXYqIpKH9ex47SO+9IkydLzzxjgqe1a6VmzQr7TJxo/rs8+qiUmirdeqsZ09PTwhMDUGY8PDw0derUCne/NQAA1zq+44FrT/9m/XUi64SiN0UrOSNZYYFhWjdwnQJ8zEV/YlqiKtkKL/o7BnfUO33f0eSNk/XM/z2jxtc11toBa9WsduFF/8RbJirzbKYe/ehRpZ5J1a3X36p1D62TZ+Vr/KL/cgHKVciWn5+f7+pJAAAAAAAAoGJxaU0pAAAAAAAAVEyEUgAAAAAAALAcoRQAAAAAAAAsRygF4KozdOhQ9bngcZqdO3fWmDFjXDYfAABgrcOHD8tms2n37t2ungoA4Aq4/Ol7AHClPvjgA1WpUsXV0wAAAGVg6NChSk1N1dq1a109FQBAKSOUAnDVu+6661w9BQAAAACAg7h9D0CZy8vL06xZs9SoUSN5eHjo+uuv1wsvvCBJ2rNnj26//XZVrVpVNWrU0KOPPqqMjIyC1+bm5ioqKkrVqlVTjRo1NHHiROXn59uN/9fb9+rXr68XX3xRDz/8sHx9fXX99dfrzTfftHvN119/rbCwMHl6eqpNmzZau3YttwEAAHCFOnfurNGjR2vMmDGqXr26AgICtGDBAmVmZmrYsGHy9fVVo0aN9Omnn0oy3/PDhw9XgwYNVLVqVYWEhOi1114rGO+5557T0qVL9Z///Ec2m002m02bNm0qOH7o0CF16dJFXl5eatmypeLj460+ZQDAFSCUAlDmJk2apJkzZ2rKlClKSEjQO++8o4CAAGVmZioyMlLVq1fXN998o9WrV+vzzz/XqFGjCl47e/ZsLVmyRIsXL9bmzZv1+++/a82aNZd9z9mzZ6tNmzbatWuXHn/8cY0cOVIHDhyQJKWnp6tnz55q3ry5du7cqenTp+upp54qs/MHAKAiWbp0qWrWrKnt27dr9OjRGjlypO6//3517NhRO3fuVPfu3TVo0CBlZWUpLy9P9erV0+rVq5WQkKDo6Gg988wzeu+99yRJ48ePV79+/dSjRw8dO3ZMx44dU8eOHQve69lnn9X48eO1e/du3XTTTXrggQd07tw5V506AMBBtvy/LjkAgFJ0+vRp1apVS/PmzdMjjzxid2zBggV66qmndOTIEXl7e0uSPvnkE/Xs2VNHjx5VQECAgoKCNHbsWE2YMEGSdO7cOTVo0EDh4eEFtSU6d+6ssLAwzZ07V5JZKfW3v/1Nb7/9tiQpPz9fgYGBmjZtmv7+978rNjZWkydPVlJSkjw9PSVJCxcu1IgRI7Rr1y6FhYWV/T8MAADXoM6dOys3N1dfffWVJLMSyt/fX3379tWyZcskScnJyapTp47i4+PVoUOHi8YYNWqUkpOT9e9//1tS0TWlDh8+rAYNGmjhwoUaPny4JCkhIUE333yz9u3bpyZNmpTxmQIASgMrpQCUqX379ik7O1tdu3Yt8ljLli0LAilJuuWWW5SXl6cDBw4oLS1Nx44dU/v27QuOV65cWW3atLns+7Zo0aLgd5vNpsDAQB0/flySdODAAbVo0aIgkJKkdu3aOXV+AADA3oXfwW5ubqpRo4aaN29e0BYQECBJBd/L8+fPV3h4uGrVqiUfHx+9+eabSkxMdPi96tSpYzcuAKD8I5QCUKaqVq3qkvf969P4bDab8vLyXDIXAAAqkqK+gy9ss9lskkzNyZUrV2r8+PEaPny41q9fr927d2vYsGHKyclx+L0uHBcAcHUglAJQpho3bqyqVasqLi7uomNNmzbVd999p8zMzIK2LVu2qFKlSgoJCZG/v7/q1Kmjbdu2FRw/d+6cduzYcUVzCgkJ0Z49e5SdnV3Q9s0331zRmAAAwHFbtmxRx44d9fjjj6tVq1Zq1KiRDh48aNfH3d1dubm5LpohAKAsEUoBKFOenp566qmnNHHiRC1btkwHDx7U1q1btWjRIg0cOFCenp4aMmSI9u7dq40bN2r06NEaNGhQwdL+J598UjNnztTatWu1f/9+Pf7440pNTb2iOT344IPKy8vTo48+qn379umzzz7TK6+8Iqnw/2UFAABlr3Hjxvr222/12Wef6ccff9SUKVMu+j+K6tevr++//14HDhzQyZMndfbsWRfNFgBQ2gilAJS5KVOmaNy4cYqOjlbTpk3Vv39/HT9+XF5eXvrss8/0+++/q23btrrvvvvUtWtXzZs3r+C148aN06BBgzRkyBBFRETI19dX99xzzxXNx8/PTx999JF2796tsLAwPfvss4qOjpYkuzpTAACgbD322GPq27ev+vfvr/bt2+vUqVN6/PHH7fqMGDFCISEhatOmjWrVqqUtW7a4aLYAgNLG0/cAQNKKFSs0bNgwpaWluawOFgAAAABUJJVdPQEAcIVly5apYcOGqlu3rr777js99dRT6tevH4EUAAAAAFiEUApAhZScnKzo6GglJyerTp06uv/++/XCCy+4eloAAAAAUGFw+x4AAAAAAAAsR6FzAAAAAAAAWI5QCgAAAAAAAJYjlAIAAAAAAIDlCKUAAAAAAABgOUIpAAAAAAAAWI5QCgAAAAAAAJYjlAIAAAAAAIDlCKUAAAAAAABgOUIpAAAAAAAAWO7/AZL3jPBulzUUAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1200x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def plot_metrics(metrics, categories):\n",
    "    num_categories = len(categories)\n",
    "\n",
    "    # Prepare the data for plotting\n",
    "    energy_per_token = []\n",
    "    avg_latencies = []\n",
    "    avg_perplexities = []\n",
    "\n",
    "    for category in categories:\n",
    "        if category in metrics:\n",
    "            energy_per_token.append(np.mean(metrics[category][\"energy_per_token\"]))\n",
    "            avg_latencies.append(np.mean(metrics[category][\"latencies\"]))\n",
    "            avg_perplexities.append(np.mean(metrics[category][\"perplexities\"]))\n",
    "        else:\n",
    "            energy_per_token.append(0)\n",
    "            avg_latencies.append(0)\n",
    "            avg_perplexities.append(0)\n",
    "\n",
    "    x = np.arange(num_categories)  # the label locations\n",
    "    width = 0.25  # the width of the bars\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Plot energy consumption\n",
    "    bars1 = ax1.bar(x - width, energy_per_token, width, label='Energy per Token (Joules)', color='b')\n",
    "    ax1.set_ylabel('Energy per Token (Joules)', color='b')\n",
    "    ax1.tick_params(axis='y', labelcolor='b')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(categories)\n",
    "    \n",
    "    # Create a second y-axis for latencies\n",
    "    ax2 = ax1.twinx()\n",
    "    bars2 = ax2.bar(x, avg_latencies, width, label='Average Latency (s)', color='g')\n",
    "    ax2.set_ylabel('Average Latency (s)', color='g')\n",
    "    ax2.tick_params(axis='y', labelcolor='g')\n",
    "\n",
    "    # Create a third y-axis for perplexities\n",
    "    ax3 = ax1.twinx()\n",
    "    bars3 = ax3.bar(x + width, avg_perplexities, width, label='Average Perplexity', color='r')\n",
    "    ax3.spines['right'].set_position(('outward', 60))  # move the third y-axis to the right\n",
    "    ax3.set_ylabel('Average Perplexity', color='r')\n",
    "    ax3.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "    # Adding titles and legend\n",
    "    fig.suptitle('Metrics Comparison Across Task Categories', fontsize=16)\n",
    "    fig.legend(loc='upper right', bbox_to_anchor=(0.85, 0.85))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the plotting function\n",
    "plot_metrics(metrics, categories)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MT_Bench mit quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    }
   ],
   "source": [
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",  # Specify the quantization type\n",
    "    bnb_4bit_use_double_quant=True,  # Use double quantization if needed\n",
    "    bnb_4bit_compute_dtype=torch.float16  # Specify computation dtype\n",
    ")\n",
    "device = \"cuda:0\" \n",
    "\n",
    "# Configure 4-bit quantization\n",
    "#quant_config = { \"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\" }\n",
    "# Load the model and tokenizer with quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\",\n",
    "                                             #\"tiiuae/falcon-mamba-7b\",\n",
    "                                             quantization_config=quant_config,\n",
    "                                             )\n",
    "tokenizer = AutoTokenizer.from_pretrained (\"facebook/opt-125m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ebergy per Token next to perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing category: knowledge\n",
      "Processing category: common-sense\n",
      "Processing category: coding\n",
      "Processing category: math\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch\n",
    "import subprocess\n",
    "import pynvml\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Specify the GPU device you want to use\n",
    "device = \"cuda:0\"  # Change this to your preferred GPU\n",
    "\n",
    "# Load model and tokenizer on the specified device\n",
    "#model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\").to(device)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")\n",
    "\n",
    "# Initialize NVML for power measurement\n",
    "def initialize_nvml():\n",
    "    pynvml.nvmlInit()\n",
    "\n",
    "def shutdown_nvml():\n",
    "    pynvml.nvmlShutdown()\n",
    "\n",
    "def get_gpu_handle(gpu_index=0):\n",
    "    return pynvml.nvmlDeviceGetHandleByIndex(gpu_index)\n",
    "\n",
    "def measure_power_consumption(handle, duration_sec=1.0, interval_sec=0.1):\n",
    "    power_readings = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while (time.time() - start_time) < duration_sec:\n",
    "        power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0  # Convert from mW to W\n",
    "        power_readings.append(power)\n",
    "        time.sleep(interval_sec)\n",
    "    \n",
    "    return sum(power_readings) / len(power_readings) if power_readings else 0\n",
    "\n",
    "def measure_energy_during_inference(handle, inference_function, *args, **kwargs):\n",
    "    power_start = measure_power_consumption(handle, duration_sec=0.5)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    result = inference_function(*args, **kwargs)  \n",
    "    end_time = time.time()\n",
    "    \n",
    "    power_end = measure_power_consumption(handle, duration_sec=0.5)\n",
    "    \n",
    "    avg_power = (power_start + power_end) / 2\n",
    "    elapsed_time = end_time - start_time  \n",
    "    energy_consumed = avg_power * elapsed_time  \n",
    "    \n",
    "    return energy_consumed, elapsed_time, result\n",
    "\n",
    "def calculate_perplexity(model, input_text, tokenizer):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)  # Ensure input is on the same device\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "        perplexity = torch.exp(loss)\n",
    "    return perplexity.item()\n",
    "\n",
    "def run_experiment_for_texts(texts, bootstrapping, handle):\n",
    "    latencies = []\n",
    "    energy_per_token = []\n",
    "    throughputs = []\n",
    "    generated_texts = []\n",
    "    perplexities = []\n",
    "\n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\").to(device)  # Ensure input is on the same device\n",
    "        text_latencies = []\n",
    "        text_energy_per_token = []\n",
    "        text_throughput = []\n",
    "        text_generated = []\n",
    "        text_perplexities = []\n",
    "\n",
    "        for _ in range(bootstrapping):\n",
    "            energy_consumed, latency, output = measure_energy_during_inference(handle, model.generate, inputs['input_ids'], max_new_tokens=200)\n",
    "            text_latencies.append(latency)\n",
    "\n",
    "            output_tokens = output.size(-1)\n",
    "            energy_token = energy_consumed / output_tokens if output_tokens > 0 else 0\n",
    "            text_energy_per_token.append(energy_token)\n",
    "\n",
    "            throughput = output_tokens / latency\n",
    "            text_throughput.append(throughput)\n",
    "\n",
    "            perplexity = calculate_perplexity(model, text, tokenizer)\n",
    "            text_perplexities.append(perplexity)\n",
    "\n",
    "            generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            filtered_generated_text = generated_text.replace(text, \"\").strip()\n",
    "            text_generated.append(filtered_generated_text)\n",
    "\n",
    "        latencies.append(text_latencies)\n",
    "        energy_per_token.append(text_energy_per_token)\n",
    "        throughputs.append(text_throughput)\n",
    "        generated_texts.append(text_generated)\n",
    "        perplexities.append(text_perplexities)\n",
    "\n",
    "    return latencies, energy_per_token, throughputs, generated_texts, perplexities\n",
    "\n",
    "def collect_metrics_for_categories(df, categories, bootstrapping):\n",
    "    category_metrics = {}\n",
    "    handle = get_gpu_handle(gpu_index=0)\n",
    "\n",
    "    for category in categories:\n",
    "        print(f\"Processing category: {category}\")\n",
    "        texts = filter_texts_by_category(df, category)\n",
    "        latencies, energy_per_token, throughputs, generated_texts, perplexities = run_experiment_for_texts(texts, bootstrapping, handle)\n",
    "\n",
    "        category_metrics[category] = {\n",
    "            \"latencies\": latencies,\n",
    "            \"energy_per_token\": energy_per_token,\n",
    "            \"throughput\": throughputs,\n",
    "            \"generated_texts\": generated_texts,\n",
    "            \"perplexities\": perplexities\n",
    "        }\n",
    "\n",
    "    shutdown_nvml()  \n",
    "    return category_metrics\n",
    "\n",
    "def filter_texts_by_category(df, category):\n",
    "    return df[df['category'] == category]['text'].values\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Example Usage\n",
    "file_path = \"../projects/question.jsonl\"\n",
    "bootstrapping = 2  \n",
    "df_mtconversation = load_dataset(file_path)\n",
    "\n",
    "categories = [ 'knowledge', 'common-sense', 'coding', 'math']\n",
    "\n",
    "initialize_nvml()\n",
    "\n",
    "metrics = collect_metrics_for_categories(df_mtconversation, categories, bootstrapping)\n",
    "\n",
    "# (Optionally, you can visualize the collected metrics here)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def plot_metrics(metrics, categories):\n",
    "    num_categories = len(categories)\n",
    "\n",
    "    # Prepare the data for plotting\n",
    "    energy_per_floost = []\n",
    "    energy_per_token = []\n",
    "    avg_latencies = []\n",
    "    avg_perplexities = []\n",
    "\n",
    "    for category in categories:\n",
    "        if category in metrics:\n",
    "            energy_per_token.append(np.mean(metrics[category][\"energy_per_token\"]))\n",
    "            avg_latencies.append(np.mean(metrics[category][\"latencies\"]))\n",
    "            avg_perplexities.append(np.mean(metrics[category][\"perplexities\"]))\n",
    "        else:\n",
    "            energy_per_token.append(0)\n",
    "            avg_latencies.append(0)\n",
    "            avg_perplexities.append(0)\n",
    "\n",
    "    x = np.arange(num_categories)  # the label locations\n",
    "    width = 0.25  # the width of the bars\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Plot energy consumption\n",
    "    bars1 = ax1.bar(x - width, energy_per_token, width, label='Energy per Token (Joules)', color='b')\n",
    "    ax1.set_ylabel('Energy per Token (Joules)', color='b')\n",
    "    ax1.tick_params(axis='y', labelcolor='b')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(categories)\n",
    "    \n",
    "    # Create a second y-axis for latencies\n",
    "    ax2 = ax1.twinx()\n",
    "    bars2 = ax2.bar(x, avg_latencies, width, label='Average Latency (s)', color='g')\n",
    "    ax2.set_ylabel('Average Latency (s)', color='g')\n",
    "    ax2.tick_params(axis='y', labelcolor='g')\n",
    "\n",
    "    # Create a third y-axis for perplexities\n",
    "    ax3 = ax1.twinx()\n",
    "    bars3 = ax3.bar(x + width, avg_perplexities, width, label='Average Perplexity', color='r')\n",
    "    ax3.spines['right'].set_position(('outward', 60))  # move the third y-axis to the right\n",
    "    ax3.set_ylabel('Average Perplexity', color='r')\n",
    "    ax3.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "    # Adding titles and legend\n",
    "    fig.suptitle('Metrics Comparison Across Task Categories', fontsize=16)\n",
    "    fig.legend(loc='upper right', bbox_to_anchor=(0.85, 0.85))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the plotting function\n",
    "plot_metrics(metrics, categories)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# energy_per_flops - 20.10.24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hier: That one is working (takes a lot of time)\n",
    "\n",
    "### Reasons might be: the duration_sec in measure_power_consumption (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"facebook/opt-125m\"\n",
    "bootstrapping = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julius/energy_per_token/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing category: knowledge\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        12.18%     188.590ms        17.98%     278.336ms      19.329us     122.295ms        52.99%     122.295ms       8.493us         14400     38220.595  \n",
      "                                               aten::mm         0.19%       2.937ms         0.27%       4.162ms      20.811us      44.887ms        19.45%      44.887ms     224.434us           200     17374.003  \n",
      "                                              aten::bmm         4.22%      65.364ms         5.80%      89.835ms      18.716us       8.359ms         3.62%       8.359ms       1.741us          4800       949.248  \n",
      "                                              aten::add         4.18%      64.634ms         6.80%     105.272ms      13.156us       9.761ms         4.23%       9.761ms       1.220us          8002         8.029  \n",
      "                                              aten::mul         1.70%      26.358ms         2.80%      43.322ms      14.426us       3.245ms         1.41%       3.245ms       1.081us          3003         2.099  \n",
      "                                            aten::empty         3.49%      54.038ms         3.49%      54.038ms       2.866us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.26%       4.028ms         4.81%      74.532ms      17.695us       0.000us         0.00%       1.503ms       0.357us          4212            --  \n",
      "                                         aten::_to_copy         0.59%       9.180ms         4.55%      70.504ms      25.135us       0.000us         0.00%       1.503ms       0.536us          2805            --  \n",
      "                                    aten::empty_strided         0.85%      13.217ms         0.85%      13.217ms       4.397us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.03%      15.967ms         3.81%      59.010ms      16.105us       2.420ms         1.05%       2.420ms       0.660us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.548s\n",
      "Self CUDA time total: 230.811ms\n",
      "\n",
      "output: tensor([[    2,  2264,    32,   103,   801,  8819,     9,   634,    10,   881,\n",
      "            12,  3698,  4136,  7304,  4411,    10, 31460,  7304,    15,   258,\n",
      "             5,  1737,     8,  1050,   474,   116, 50118, 50118,   133,  4136,\n",
      "          7304,    16,    10, 31460,  7304,     6,    53,    24,    16,    45,\n",
      "         31460,     4,    20,  4136,  7304,    16,    10, 31460,  7304,     6,\n",
      "            53,    24,    16,    45, 31460,     4,    20,  4136,  7304,    16,\n",
      "            10, 31460,  7304,     6,    53,    24,    16,    45, 31460,     4,\n",
      "            20,  4136,  7304,    16,    10, 31460,  7304,     6,    53,    24,\n",
      "            16,    45, 31460,     4,    20,  4136,  7304,    16,    10, 31460,\n",
      "          7304,     6,    53,    24,    16,    45, 31460,     4,    20,  4136,\n",
      "          7304,    16,    10, 31460,  7304,     6,    53,    24,    16,    45,\n",
      "         31460,     4,    20,  4136,  7304,    16,    10, 31460,  7304,     6,\n",
      "            53,    24,    16,    45, 31460,     4,    20,  4136,  7304,    16,\n",
      "            10, 31460,  7304,     6,    53,    24,    16,    45, 31460,     4,\n",
      "            20,  4136,  7304,    16,    10, 31460,  7304,     6,    53,    24,\n",
      "            16,    45, 31460,     4,    20,  4136,  7304,    16,    10, 31460,\n",
      "          7304,     6,    53,    24,    16,    45, 31460,     4,    20,  4136,\n",
      "          7304,    16,    10, 31460,  7304,     6,    53,    24,    16,    45,\n",
      "         31460,     4,    20,  4136,  7304,    16,    10, 31460,  7304,     6,\n",
      "            53,    24,    16,    45, 31460,     4,    20,  4136,  7304,    16,\n",
      "            10, 31460,  7304,     6,    53,    24,    16,    45, 31460,     4,\n",
      "            20,  4136,  7304,    16,    10, 31460,  7304,     6,    53,    24,\n",
      "            16,    45, 31460,     4,    20,  4136]], device='cuda:0')\n",
      "text_energy_per_token: [5.852261699971793]\n",
      "output_tokens: 226\n",
      "flop: 56553974230\n",
      "energy_consumed:  1322.6111441936253\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.06%     187.740ms        21.87%     255.629ms      17.752us     122.146ms        52.96%     122.146ms       8.482us         14400     38220.595  \n",
      "                                               aten::mm         0.25%       2.887ms         0.37%       4.296ms      21.481us      44.898ms        19.47%      44.898ms     224.491us           200     17374.003  \n",
      "                                              aten::bmm         5.02%      58.700ms         6.80%      79.435ms      16.549us       8.351ms         3.62%       8.351ms       1.740us          4800       949.248  \n",
      "                                              aten::add         5.50%      64.280ms         8.35%      97.576ms      12.194us       9.769ms         4.24%       9.769ms       1.221us          8002         8.029  \n",
      "                                              aten::mul         2.24%      26.183ms         3.34%      39.001ms      12.987us       3.247ms         1.41%       3.247ms       1.081us          3003         2.099  \n",
      "                                            aten::empty         5.09%      59.524ms         5.09%      59.524ms       3.156us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       4.122ms         5.95%      69.570ms      16.517us       0.000us         0.00%       1.505ms       0.357us          4212            --  \n",
      "                                         aten::_to_copy         0.77%       8.984ms         5.60%      65.448ms      23.333us       0.000us         0.00%       1.505ms       0.536us          2805            --  \n",
      "                                    aten::empty_strided         1.36%      15.949ms         1.36%      15.949ms       5.306us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.38%      16.131ms         4.54%      53.101ms      14.493us       2.422ms         1.05%       2.422ms       0.661us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.169s\n",
      "Self CUDA time total: 230.651ms\n",
      "\n",
      "output: tensor([[    2,  2264,    32,   103,   801,  8819,     9,   634,    10,   881,\n",
      "            12,  3698,  4136,  7304,  4411,    10, 31460,  7304,    15,   258,\n",
      "             5,  1737,     8,  1050,   474,   116, 50118, 50118,   133,  4136,\n",
      "          7304,    16,    10, 31460,  7304,     6,    53,    24,    16,    45,\n",
      "         31460,     4,    20,  4136,  7304,    16,    10, 31460,  7304,     6,\n",
      "            53,    24,    16,    45, 31460,     4,    20,  4136,  7304,    16,\n",
      "            10, 31460,  7304,     6,    53,    24,    16,    45, 31460,     4,\n",
      "            20,  4136,  7304,    16,    10, 31460,  7304,     6,    53,    24,\n",
      "            16,    45, 31460,     4,    20,  4136,  7304,    16,    10, 31460,\n",
      "          7304,     6,    53,    24,    16,    45, 31460,     4,    20,  4136,\n",
      "          7304,    16,    10, 31460,  7304,     6,    53,    24,    16,    45,\n",
      "         31460,     4,    20,  4136,  7304,    16,    10, 31460,  7304,     6,\n",
      "            53,    24,    16,    45, 31460,     4,    20,  4136,  7304,    16,\n",
      "            10, 31460,  7304,     6,    53,    24,    16,    45, 31460,     4,\n",
      "            20,  4136,  7304,    16,    10, 31460,  7304,     6,    53,    24,\n",
      "            16,    45, 31460,     4,    20,  4136,  7304,    16,    10, 31460,\n",
      "          7304,     6,    53,    24,    16,    45, 31460,     4,    20,  4136,\n",
      "          7304,    16,    10, 31460,  7304,     6,    53,    24,    16,    45,\n",
      "         31460,     4,    20,  4136,  7304,    16,    10, 31460,  7304,     6,\n",
      "            53,    24,    16,    45, 31460,     4,    20,  4136,  7304,    16,\n",
      "            10, 31460,  7304,     6,    53,    24,    16,    45, 31460,     4,\n",
      "            20,  4136,  7304,    16,    10, 31460,  7304,     6,    53,    24,\n",
      "            16,    45, 31460,     4,    20,  4136]], device='cuda:0')\n",
      "text_energy_per_token: [5.852261699971793, 8.884083323174032]\n",
      "output_tokens: 226\n",
      "flop: 56553974230\n",
      "energy_consumed:  2007.8028310373313\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.52%     178.723ms        21.24%     244.638ms      16.989us     122.129ms        53.02%     122.129ms       8.481us         14400     36521.902  \n",
      "                                               aten::mm         0.37%       4.224ms         0.48%       5.524ms      27.619us      44.924ms        19.50%      44.924ms     224.618us           200     16601.825  \n",
      "                                              aten::bmm         5.26%      60.565ms         7.37%      84.910ms      17.690us       8.140ms         3.53%       8.140ms       1.696us          4800       860.406  \n",
      "                                              aten::add         5.66%      65.230ms         8.66%      99.801ms      12.472us       9.754ms         4.23%       9.754ms       1.219us          8002         7.490  \n",
      "                                              aten::mul         2.26%      26.039ms         3.35%      38.618ms      12.860us       3.241ms         1.41%       3.241ms       1.079us          3003         2.005  \n",
      "                                            aten::empty         4.77%      54.988ms         4.77%      54.988ms       2.916us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.36%       4.110ms         5.97%      68.738ms      16.320us       0.000us         0.00%       1.497ms       0.355us          4212            --  \n",
      "                                         aten::_to_copy         0.78%       8.934ms         5.61%      64.628ms      23.040us       0.000us         0.00%       1.497ms       0.534us          2805            --  \n",
      "                                    aten::empty_strided         1.18%      13.592ms         1.18%      13.592ms       4.522us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.40%      16.106ms         4.62%      53.223ms      14.526us       2.412ms         1.05%       2.412ms       0.658us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.152s\n",
      "Self CUDA time total: 230.336ms\n",
      "\n",
      "output: tensor([[    2,  2264,  2433,    74,    47,  1701,    77, 15293,    41, 10510,\n",
      "             8,  6500,   285,  4264,   467,   116, 50118, 50118,   133,   412,\n",
      "             9,   764,  2659,    34,    10,   251,   750,     9,  1976,  4555,\n",
      "             6,  6500,     6,     8,  1522,   285,  4264,     4,   166,    32,\n",
      "          2021,     7,  1976,     5,   275,   678,  4264,  1735,    13,    70,\n",
      "            84,  1196,     4, 50118, 50118,  2264,  2433,    74,    47,  1701,\n",
      "            77, 15293,    41, 10510,     8,  6500,   285,  4264,   467,   116,\n",
      "         50118, 50118,   133,   412,     9,   764,  2659,    34,    10,   251,\n",
      "           750,     9,  1976,  4555,     6,  6500,     6,     8,  1522,   285,\n",
      "          4264,     4,   166,    32,  2021,     7,  1976,     5,   275,   678,\n",
      "          4264,  1735,    13,    70,    84,  1196,     4, 50118, 50118,  2264,\n",
      "          2433,    74,    47,  1701,    77, 15293,    41, 10510,     8,  6500,\n",
      "           285,  4264,   467,   116, 50118, 50118,   133,   412,     9,   764,\n",
      "          2659,    34,    10,   251,   750,     9,  1976,  4555,     6,  6500,\n",
      "             6,     8,  1522,   285,  4264,     4,   166,    32,  2021,     7,\n",
      "          1976,     5,   275,   678,  4264,  1735,    13,    70,    84,  1196,\n",
      "             4, 50118, 50118,  2264,  2433,    74,    47,  1701,    77, 15293,\n",
      "            41, 10510,     8,  6500,   285,  4264,   467,   116, 50118, 50118,\n",
      "           133,   412,     9,   764,  2659,    34,    10,   251,   750,     9,\n",
      "          1976,  4555,     6,  6500,     6,     8,  1522,   285,  4264,     4,\n",
      "           166,    32,  2021,     7,  1976,     5,   275,   678,  4264,  1735,\n",
      "            13,    70,    84,  1196,     4, 50118]], device='cuda:0')\n",
      "text_energy_per_token: [9.376887687194348]\n",
      "output_tokens: 216\n",
      "flop: 53993627700\n",
      "energy_consumed:  2025.4077404339791\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.65%     172.314ms        21.46%     236.327ms      16.412us     122.048ms        53.01%     122.048ms       8.476us         14400     36521.902  \n",
      "                                               aten::mm         0.38%       4.137ms         0.49%       5.352ms      26.758us      44.894ms        19.50%      44.894ms     224.472us           200     16601.825  \n",
      "                                              aten::bmm         5.26%      57.948ms         7.10%      78.217ms      16.295us       8.146ms         3.54%       8.146ms       1.697us          4800       860.406  \n",
      "                                              aten::add         5.70%      62.828ms         8.69%      95.709ms      11.961us       9.749ms         4.23%       9.749ms       1.218us          8002         7.490  \n",
      "                                              aten::mul         2.44%      26.857ms         3.57%      39.326ms      13.096us       3.238ms         1.41%       3.238ms       1.078us          3003         2.005  \n",
      "                                            aten::empty         4.83%      53.201ms         4.83%      53.201ms       2.821us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.880ms         6.15%      67.685ms      16.070us       0.000us         0.00%       1.500ms       0.356us          4212            --  \n",
      "                                         aten::_to_copy         0.75%       8.303ms         5.79%      63.805ms      22.747us       0.000us         0.00%       1.500ms       0.535us          2805            --  \n",
      "                                    aten::empty_strided         1.16%      12.786ms         1.16%      12.786ms       4.253us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.45%      15.938ms         4.87%      53.644ms      14.641us       2.416ms         1.05%       2.416ms       0.659us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.101s\n",
      "Self CUDA time total: 230.227ms\n",
      "\n",
      "output: tensor([[    2,  2264,  2433,    74,    47,  1701,    77, 15293,    41, 10510,\n",
      "             8,  6500,   285,  4264,   467,   116, 50118, 50118,   133,   412,\n",
      "             9,   764,  2659,    34,    10,   251,   750,     9,  1976,  4555,\n",
      "             6,  6500,     6,     8,  1522,   285,  4264,     4,   166,    32,\n",
      "          2021,     7,  1976,     5,   275,   678,  4264,  1735,    13,    70,\n",
      "            84,  1196,     4, 50118, 50118,  2264,  2433,    74,    47,  1701,\n",
      "            77, 15293,    41, 10510,     8,  6500,   285,  4264,   467,   116,\n",
      "         50118, 50118,   133,   412,     9,   764,  2659,    34,    10,   251,\n",
      "           750,     9,  1976,  4555,     6,  6500,     6,     8,  1522,   285,\n",
      "          4264,     4,   166,    32,  2021,     7,  1976,     5,   275,   678,\n",
      "          4264,  1735,    13,    70,    84,  1196,     4, 50118, 50118,  2264,\n",
      "          2433,    74,    47,  1701,    77, 15293,    41, 10510,     8,  6500,\n",
      "           285,  4264,   467,   116, 50118, 50118,   133,   412,     9,   764,\n",
      "          2659,    34,    10,   251,   750,     9,  1976,  4555,     6,  6500,\n",
      "             6,     8,  1522,   285,  4264,     4,   166,    32,  2021,     7,\n",
      "          1976,     5,   275,   678,  4264,  1735,    13,    70,    84,  1196,\n",
      "             4, 50118, 50118,  2264,  2433,    74,    47,  1701,    77, 15293,\n",
      "            41, 10510,     8,  6500,   285,  4264,   467,   116, 50118, 50118,\n",
      "           133,   412,     9,   764,  2659,    34,    10,   251,   750,     9,\n",
      "          1976,  4555,     6,  6500,     6,     8,  1522,   285,  4264,     4,\n",
      "           166,    32,  2021,     7,  1976,     5,   275,   678,  4264,  1735,\n",
      "            13,    70,    84,  1196,     4, 50118]], device='cuda:0')\n",
      "text_energy_per_token: [9.376887687194348, 9.412101735086353]\n",
      "output_tokens: 216\n",
      "flop: 53993627700\n",
      "energy_consumed:  2033.013974778652\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.05%     177.248ms        21.80%     240.763ms      16.720us     121.950ms        53.00%     121.950ms       8.469us         14400     36352.033  \n",
      "                                               aten::mm         0.26%       2.850ms         0.37%       4.083ms      20.417us      44.916ms        19.52%      44.916ms     224.579us           200     16524.607  \n",
      "                                              aten::bmm         5.13%      56.621ms         6.97%      77.041ms      16.050us       8.129ms         3.53%       8.129ms       1.694us          4800       851.927  \n",
      "                                              aten::add         5.75%      63.562ms         8.73%      96.461ms      12.055us       9.754ms         4.24%       9.754ms       1.219us          8002         7.437  \n",
      "                                              aten::mul         2.42%      26.784ms         3.56%      39.338ms      13.100us       3.240ms         1.41%       3.240ms       1.079us          3003         1.996  \n",
      "                                            aten::empty         5.16%      57.000ms         5.16%      57.000ms       3.023us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.859ms         6.33%      69.895ms      16.594us       0.000us         0.00%       1.506ms       0.358us          4212            --  \n",
      "                                         aten::_to_copy         0.74%       8.191ms         5.98%      66.036ms      23.542us       0.000us         0.00%       1.506ms       0.537us          2805            --  \n",
      "                                    aten::empty_strided         1.42%      15.739ms         1.42%      15.739ms       5.236us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.44%      15.927ms         4.92%      54.376ms      14.841us       2.420ms         1.05%       2.420ms       0.660us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.105s\n",
      "Self CUDA time total: 230.092ms\n",
      "\n",
      "output: tensor([[    2,  6179,    64,  3233, 16085,  2358,     8,  5775,  1986,     7,\n",
      "          5217,   776,  3872, 27366,   116, 50118, 50118,   133,  1948,    16,\n",
      "          2007,    35,     5,   168,    64,   304,  2358,     8,  5775,  1986,\n",
      "             7,  5217,   776,  3872, 27366,     4, 50118, 50118,   133,   168,\n",
      "            64,   304,  2358,     8,  5775,  1986,     7,  5217,   776,  3872,\n",
      "         27366,     4, 50118, 50118,   133,   168,    64,   304,  2358,     8,\n",
      "          5775,  1986,     7,  5217,   776,  3872, 27366,     4, 50118, 50118,\n",
      "           133,   168,    64,   304,  2358,     8,  5775,  1986,     7,  5217,\n",
      "           776,  3872, 27366,     4, 50118, 50118,   133,   168,    64,   304,\n",
      "          2358,     8,  5775,  1986,     7,  5217,   776,  3872, 27366,     4,\n",
      "         50118, 50118,   133,   168,    64,   304,  2358,     8,  5775,  1986,\n",
      "             7,  5217,   776,  3872, 27366,     4, 50118, 50118,   133,   168,\n",
      "            64,   304,  2358,     8,  5775,  1986,     7,  5217,   776,  3872,\n",
      "         27366,     4, 50118, 50118,   133,   168,    64,   304,  2358,     8,\n",
      "          5775,  1986,     7,  5217,   776,  3872, 27366,     4, 50118, 50118,\n",
      "           133,   168,    64,   304,  2358,     8,  5775,  1986,     7,  5217,\n",
      "           776,  3872, 27366,     4, 50118, 50118,   133,   168,    64,   304,\n",
      "          2358,     8,  5775,  1986,     7,  5217,   776,  3872, 27366,     4,\n",
      "         50118, 50118,   133,   168,    64,   304,  2358,     8,  5775,  1986,\n",
      "             7,  5217,   776,  3872, 27366,     4, 50118, 50118,   133,   168,\n",
      "            64,   304,  2358,     8,  5775,  1986,     7,  5217,   776,  3872,\n",
      "         27366,     4, 50118, 50118,   133]], device='cuda:0')\n",
      "text_energy_per_token: [9.501395126946028]\n",
      "output_tokens: 215\n",
      "flop: 53738000135\n",
      "energy_consumed:  2042.7999522933958\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.78%     171.828ms        21.59%     235.058ms      16.323us     122.186ms        53.04%     122.186ms       8.485us         14400     36352.033  \n",
      "                                               aten::mm         0.27%       2.906ms         0.38%       4.088ms      20.442us      44.930ms        19.50%      44.930ms     224.650us           200     16524.607  \n",
      "                                              aten::bmm         5.32%      57.945ms         7.18%      78.106ms      16.272us       8.133ms         3.53%       8.133ms       1.694us          4800       851.927  \n",
      "                                              aten::add         5.83%      63.417ms         8.83%      96.109ms      12.011us       9.751ms         4.23%       9.751ms       1.219us          8002         7.437  \n",
      "                                              aten::mul         2.45%      26.704ms         3.60%      39.220ms      13.060us       3.240ms         1.41%       3.240ms       1.079us          3003         1.996  \n",
      "                                            aten::empty         4.73%      51.533ms         4.73%      51.533ms       2.733us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.34%       3.744ms         6.37%      69.293ms      16.451us       0.000us         0.00%       1.494ms       0.355us          4212            --  \n",
      "                                         aten::_to_copy         0.75%       8.202ms         6.02%      65.549ms      23.369us       0.000us         0.00%       1.494ms       0.533us          2805            --  \n",
      "                                    aten::empty_strided         1.28%      13.932ms         1.28%      13.932ms       4.635us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.46%      15.930ms         5.00%      54.398ms      14.847us       2.409ms         1.05%       2.409ms       0.658us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.089s\n",
      "Self CUDA time total: 230.369ms\n",
      "\n",
      "output: tensor([[    2,  6179,    64,  3233, 16085,  2358,     8,  5775,  1986,     7,\n",
      "          5217,   776,  3872, 27366,   116, 50118, 50118,   133,  1948,    16,\n",
      "          2007,    35,     5,   168,    64,   304,  2358,     8,  5775,  1986,\n",
      "             7,  5217,   776,  3872, 27366,     4, 50118, 50118,   133,   168,\n",
      "            64,   304,  2358,     8,  5775,  1986,     7,  5217,   776,  3872,\n",
      "         27366,     4, 50118, 50118,   133,   168,    64,   304,  2358,     8,\n",
      "          5775,  1986,     7,  5217,   776,  3872, 27366,     4, 50118, 50118,\n",
      "           133,   168,    64,   304,  2358,     8,  5775,  1986,     7,  5217,\n",
      "           776,  3872, 27366,     4, 50118, 50118,   133,   168,    64,   304,\n",
      "          2358,     8,  5775,  1986,     7,  5217,   776,  3872, 27366,     4,\n",
      "         50118, 50118,   133,   168,    64,   304,  2358,     8,  5775,  1986,\n",
      "             7,  5217,   776,  3872, 27366,     4, 50118, 50118,   133,   168,\n",
      "            64,   304,  2358,     8,  5775,  1986,     7,  5217,   776,  3872,\n",
      "         27366,     4, 50118, 50118,   133,   168,    64,   304,  2358,     8,\n",
      "          5775,  1986,     7,  5217,   776,  3872, 27366,     4, 50118, 50118,\n",
      "           133,   168,    64,   304,  2358,     8,  5775,  1986,     7,  5217,\n",
      "           776,  3872, 27366,     4, 50118, 50118,   133,   168,    64,   304,\n",
      "          2358,     8,  5775,  1986,     7,  5217,   776,  3872, 27366,     4,\n",
      "         50118, 50118,   133,   168,    64,   304,  2358,     8,  5775,  1986,\n",
      "             7,  5217,   776,  3872, 27366,     4, 50118, 50118,   133,   168,\n",
      "            64,   304,  2358,     8,  5775,  1986,     7,  5217,   776,  3872,\n",
      "         27366,     4, 50118, 50118,   133]], device='cuda:0')\n",
      "text_energy_per_token: [9.501395126946028, 9.144692846501927]\n",
      "output_tokens: 215\n",
      "flop: 53738000135\n",
      "energy_consumed:  1966.1089619979143\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.76%     172.292ms        21.56%     235.682ms      16.367us     122.212ms        53.01%     122.212ms       8.487us         14400     37031.510  \n",
      "                                               aten::mm         0.38%       4.198ms         0.50%       5.429ms      27.144us      44.910ms        19.48%      44.910ms     224.548us           200     16833.479  \n",
      "                                              aten::bmm         5.41%      59.119ms         7.25%      79.276ms      16.516us       8.290ms         3.60%       8.290ms       1.727us          4800       886.284  \n",
      "                                              aten::add         6.03%      65.980ms         9.04%      98.802ms      12.347us       9.758ms         4.23%       9.758ms       1.219us          8002         7.648  \n",
      "                                              aten::mul         2.32%      25.370ms         3.46%      37.846ms      12.603us       3.241ms         1.41%       3.241ms       1.079us          3003         2.033  \n",
      "                                            aten::empty         4.81%      52.627ms         4.81%      52.627ms       2.791us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.773ms         6.22%      67.956ms      16.134us       0.000us         0.00%       1.500ms       0.356us          4212            --  \n",
      "                                         aten::_to_copy         0.74%       8.116ms         5.87%      64.183ms      22.882us       0.000us         0.00%       1.500ms       0.535us          2805            --  \n",
      "                                    aten::empty_strided         1.30%      14.230ms         1.30%      14.230ms       4.734us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.45%      15.905ms         4.82%      52.740ms      14.394us       2.415ms         1.05%       2.415ms       0.659us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.093s\n",
      "Self CUDA time total: 230.558ms\n",
      "\n",
      "output: tensor([[    2,  6179,   109,  2777,     8,  4106,  7926,  3327,     5,   169,\n",
      "            82,  8469,     8,  1026,  4158,    11, 30286, 17537,   116, 50118,\n",
      "         50118,   133,   892,     6,  1027,    11,     5,  8812,  3574, 32255,\n",
      "             6,   303,    14,  2777,     8,  4106,  7926,  3327,     5,   169,\n",
      "            82,  8469,     8,  1026,  4158,    11, 30286, 17537,     4, 50118,\n",
      "         50118,    17,    48, 46969,     8,  4106,  7926,  3327,     5,   169,\n",
      "            82,  8469,     8,  1026,  4158,    11, 30286, 17537,     6,    17,\n",
      "            46,   161,   892,  1029,    12, 11515,   925,     4,   871,   305,\n",
      "             4,   289,  3343,  2596,     6, 15221,     6,    10,  3097,     9,\n",
      "         16797,    23,     5,   589,     9,   886,     6,   764,  2659,     4,\n",
      "            44,    48, 46969,     8,  4106,  7926,  3327,     5,   169,    82,\n",
      "          8469,     8,  1026,  4158,    11, 30286, 17537,     4,    17,    46,\n",
      "         50118, 50118,   133,   892,     6,  1027,    11,     5,  8812,  3574,\n",
      "         32255,     6,   303,    14,  2777,     8,  4106,  7926,  3327,     5,\n",
      "           169,    82,  8469,     8,  1026,  4158,    11, 30286, 17537,     4,\n",
      "         50118, 50118,    17,    48, 46969,     8,  4106,  7926,  3327,     5,\n",
      "           169,    82,  8469,     8,  1026,  4158,    11, 30286, 17537,     6,\n",
      "            17,    46,   161,   892,  1029,    12, 11515,   925,     4,   871,\n",
      "           305,     4,   289,  3343,  2596,     6, 15221,     6,    10,  3097,\n",
      "             9, 16797,    23,     5,   589,     9,   886,     6,   764,  2659,\n",
      "             4,    44,    48, 46969,     8,  4106,  7926,  3327,     5,   169,\n",
      "            82,  8469,     8,  1026,  4158,    11, 30286, 17537,     4]],\n",
      "       device='cuda:0')\n",
      "text_energy_per_token: [9.60340019070547]\n",
      "output_tokens: 219\n",
      "flop: 54760954491\n",
      "energy_consumed:  2103.144641764498\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.15%     181.415ms        22.01%     247.259ms      17.171us     122.209ms        53.01%     122.209ms       8.487us         14400     37031.510  \n",
      "                                               aten::mm         0.25%       2.826ms         0.36%       4.071ms      20.353us      44.896ms        19.47%      44.896ms     224.478us           200     16833.479  \n",
      "                                              aten::bmm         5.10%      57.332ms         6.92%      77.712ms      16.190us       8.290ms         3.60%       8.290ms       1.727us          4800       886.284  \n",
      "                                              aten::add         5.61%      63.004ms         8.55%      96.027ms      12.000us       9.761ms         4.23%       9.761ms       1.220us          8002         7.648  \n",
      "                                              aten::mul         2.29%      25.697ms         3.42%      38.452ms      12.805us       3.234ms         1.40%       3.234ms       1.077us          3003         2.033  \n",
      "                                            aten::empty         4.94%      55.471ms         4.94%      55.471ms       2.941us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.938ms         6.28%      70.595ms      16.760us       0.000us         0.00%       1.498ms       0.356us          4212            --  \n",
      "                                         aten::_to_copy         0.77%       8.614ms         5.93%      66.657ms      23.764us       0.000us         0.00%       1.498ms       0.534us          2805            --  \n",
      "                                    aten::empty_strided         1.29%      14.510ms         1.29%      14.510ms       4.827us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.43%      16.054ms         4.87%      54.677ms      14.923us       2.415ms         1.05%       2.415ms       0.659us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.124s\n",
      "Self CUDA time total: 230.548ms\n",
      "\n",
      "output: tensor([[    2,  6179,   109,  2777,     8,  4106,  7926,  3327,     5,   169,\n",
      "            82,  8469,     8,  1026,  4158,    11, 30286, 17537,   116, 50118,\n",
      "         50118,   133,   892,     6,  1027,    11,     5,  8812,  3574, 32255,\n",
      "             6,   303,    14,  2777,     8,  4106,  7926,  3327,     5,   169,\n",
      "            82,  8469,     8,  1026,  4158,    11, 30286, 17537,     4, 50118,\n",
      "         50118,    17,    48, 46969,     8,  4106,  7926,  3327,     5,   169,\n",
      "            82,  8469,     8,  1026,  4158,    11, 30286, 17537,     6,    17,\n",
      "            46,   161,   892,  1029,    12, 11515,   925,     4,   871,   305,\n",
      "             4,   289,  3343,  2596,     6, 15221,     6,    10,  3097,     9,\n",
      "         16797,    23,     5,   589,     9,   886,     6,   764,  2659,     4,\n",
      "            44,    48, 46969,     8,  4106,  7926,  3327,     5,   169,    82,\n",
      "          8469,     8,  1026,  4158,    11, 30286, 17537,     4,    17,    46,\n",
      "         50118, 50118,   133,   892,     6,  1027,    11,     5,  8812,  3574,\n",
      "         32255,     6,   303,    14,  2777,     8,  4106,  7926,  3327,     5,\n",
      "           169,    82,  8469,     8,  1026,  4158,    11, 30286, 17537,     4,\n",
      "         50118, 50118,    17,    48, 46969,     8,  4106,  7926,  3327,     5,\n",
      "           169,    82,  8469,     8,  1026,  4158,    11, 30286, 17537,     6,\n",
      "            17,    46,   161,   892,  1029,    12, 11515,   925,     4,   871,\n",
      "           305,     4,   289,  3343,  2596,     6, 15221,     6,    10,  3097,\n",
      "             9, 16797,    23,     5,   589,     9,   886,     6,   764,  2659,\n",
      "             4,    44,    48, 46969,     8,  4106,  7926,  3327,     5,   169,\n",
      "            82,  8469,     8,  1026,  4158,    11, 30286, 17537,     4]],\n",
      "       device='cuda:0')\n",
      "text_energy_per_token: [9.60340019070547, 9.799414799980594]\n",
      "output_tokens: 219\n",
      "flop: 54760954491\n",
      "energy_consumed:  2146.07184119575\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.46%     176.369ms        21.17%     241.551ms      16.774us     122.167ms        52.98%     122.167ms       8.484us         14400     37371.249  \n",
      "                                               aten::mm         0.37%       4.214ms         0.48%       5.472ms      27.359us      44.923ms        19.48%      44.923ms     224.616us           200     16987.914  \n",
      "                                              aten::bmm         5.31%      60.553ms         7.11%      81.161ms      16.908us       8.307ms         3.60%       8.307ms       1.731us          4800       903.905  \n",
      "                                              aten::add         5.93%      67.646ms         8.84%     100.829ms      12.600us       9.765ms         4.23%       9.765ms       1.220us          8002         7.756  \n",
      "                                              aten::mul         2.27%      25.889ms         3.37%      38.490ms      12.817us       3.243ms         1.41%       3.243ms       1.080us          3003         2.052  \n",
      "                                            aten::empty         4.68%      53.356ms         4.68%      53.356ms       2.829us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.36%       4.068ms         6.22%      70.934ms      16.841us       0.000us         0.00%       1.506ms       0.358us          4212            --  \n",
      "                                         aten::_to_copy         0.77%       8.784ms         5.86%      66.866ms      23.838us       0.000us         0.00%       1.506ms       0.537us          2805            --  \n",
      "                                    aten::empty_strided         1.27%      14.438ms         1.27%      14.438ms       4.803us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.41%      16.045ms         4.80%      54.814ms      14.960us       2.421ms         1.05%       2.421ms       0.661us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.141s\n",
      "Self CUDA time total: 230.578ms\n",
      "\n",
      "output: tensor([[    2, 47066, 21700,    10,  5665,   147,  7350,  2316,   115,    28,\n",
      "           341,     7,  1477,     5,  1318,     8,  5838,     9,  3717,  2996,\n",
      "             4, 50118, 50118,  2264,    74,    47,   101,     7,   192,    11,\n",
      "             5,   499,   116, 50118, 50118,  2264,    74,    47,   101,     7,\n",
      "           192,    11,     5,   499,   116, 50118, 50118,  2264,    74,    47,\n",
      "           101,     7,   192,    11,     5,   499,   116, 50118, 50118,  2264,\n",
      "            74,    47,   101,     7,   192,    11,     5,   499,   116, 50118,\n",
      "         50118,  2264,    74,    47,   101,     7,   192,    11,     5,   499,\n",
      "           116, 50118, 50118,  2264,    74,    47,   101,     7,   192,    11,\n",
      "             5,   499,   116, 50118, 50118,  2264,    74,    47,   101,     7,\n",
      "           192,    11,     5,   499,   116, 50118, 50118,  2264,    74,    47,\n",
      "           101,     7,   192,    11,     5,   499,   116, 50118, 50118,  2264,\n",
      "            74,    47,   101,     7,   192,    11,     5,   499,   116, 50118,\n",
      "         50118,  2264,    74,    47,   101,     7,   192,    11,     5,   499,\n",
      "           116, 50118, 50118,  2264,    74,    47,   101,     7,   192,    11,\n",
      "             5,   499,   116, 50118, 50118,  2264,    74,    47,   101,     7,\n",
      "           192,    11,     5,   499,   116, 50118, 50118,  2264,    74,    47,\n",
      "           101,     7,   192,    11,     5,   499,   116, 50118, 50118,  2264,\n",
      "            74,    47,   101,     7,   192,    11,     5,   499,   116, 50118,\n",
      "         50118,  2264,    74,    47,   101,     7,   192,    11,     5,   499,\n",
      "           116, 50118, 50118,  2264,    74,    47,   101,     7,   192,    11,\n",
      "             5,   499,   116, 50118, 50118,  2264,    74,    47,   101,     7,\n",
      "           192]], device='cuda:0')\n",
      "text_energy_per_token: [9.032529919601888]\n",
      "output_tokens: 221\n",
      "flop: 55272875765\n",
      "energy_consumed:  1996.1891122320174\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.12%     180.986ms        21.88%     245.624ms      17.057us     122.112ms        52.98%     122.112ms       8.480us         14400     37371.249  \n",
      "                                               aten::mm         0.25%       2.834ms         0.37%       4.098ms      20.488us      44.903ms        19.48%      44.903ms     224.516us           200     16987.914  \n",
      "                                              aten::bmm         5.11%      57.314ms         6.92%      77.684ms      16.184us       8.305ms         3.60%       8.305ms       1.730us          4800       903.905  \n",
      "                                              aten::add         5.62%      63.088ms         8.56%      96.083ms      12.007us       9.757ms         4.23%       9.757ms       1.219us          8002         7.756  \n",
      "                                              aten::mul         2.29%      25.747ms         3.41%      38.279ms      12.747us       3.243ms         1.41%       3.243ms       1.080us          3003         2.052  \n",
      "                                            aten::empty         5.06%      56.747ms         5.06%      56.747ms       3.009us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.36%       3.998ms         6.18%      69.360ms      16.467us       0.000us         0.00%       1.504ms       0.357us          4212            --  \n",
      "                                         aten::_to_copy         0.77%       8.684ms         5.82%      65.361ms      23.302us       0.000us         0.00%       1.504ms       0.536us          2805            --  \n",
      "                                    aten::empty_strided         1.41%      15.837ms         1.41%      15.837ms       5.268us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.43%      16.026ms         4.73%      53.095ms      14.491us       2.420ms         1.05%       2.420ms       0.661us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.122s\n",
      "Self CUDA time total: 230.490ms\n",
      "\n",
      "output: tensor([[    2, 47066, 21700,    10,  5665,   147,  7350,  2316,   115,    28,\n",
      "           341,     7,  1477,     5,  1318,     8,  5838,     9,  3717,  2996,\n",
      "             4, 50118, 50118,  2264,    74,    47,   101,     7,   192,    11,\n",
      "             5,   499,   116, 50118, 50118,  2264,    74,    47,   101,     7,\n",
      "           192,    11,     5,   499,   116, 50118, 50118,  2264,    74,    47,\n",
      "           101,     7,   192,    11,     5,   499,   116, 50118, 50118,  2264,\n",
      "            74,    47,   101,     7,   192,    11,     5,   499,   116, 50118,\n",
      "         50118,  2264,    74,    47,   101,     7,   192,    11,     5,   499,\n",
      "           116, 50118, 50118,  2264,    74,    47,   101,     7,   192,    11,\n",
      "             5,   499,   116, 50118, 50118,  2264,    74,    47,   101,     7,\n",
      "           192,    11,     5,   499,   116, 50118, 50118,  2264,    74,    47,\n",
      "           101,     7,   192,    11,     5,   499,   116, 50118, 50118,  2264,\n",
      "            74,    47,   101,     7,   192,    11,     5,   499,   116, 50118,\n",
      "         50118,  2264,    74,    47,   101,     7,   192,    11,     5,   499,\n",
      "           116, 50118, 50118,  2264,    74,    47,   101,     7,   192,    11,\n",
      "             5,   499,   116, 50118, 50118,  2264,    74,    47,   101,     7,\n",
      "           192,    11,     5,   499,   116, 50118, 50118,  2264,    74,    47,\n",
      "           101,     7,   192,    11,     5,   499,   116, 50118, 50118,  2264,\n",
      "            74,    47,   101,     7,   192,    11,     5,   499,   116, 50118,\n",
      "         50118,  2264,    74,    47,   101,     7,   192,    11,     5,   499,\n",
      "           116, 50118, 50118,  2264,    74,    47,   101,     7,   192,    11,\n",
      "             5,   499,   116, 50118, 50118,  2264,    74,    47,   101,     7,\n",
      "           192]], device='cuda:0')\n",
      "text_energy_per_token: [9.032529919601888, 9.160851598062127]\n",
      "output_tokens: 221\n",
      "flop: 55272875765\n",
      "energy_consumed:  2024.5482031717302\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.92%     184.426ms        21.72%     251.588ms      17.471us     122.308ms        53.00%     122.308ms       8.494us         14400     38220.595  \n",
      "                                               aten::mm         0.25%       2.862ms         0.36%       4.138ms      20.689us      44.865ms        19.44%      44.865ms     224.323us           200     17374.003  \n",
      "                                              aten::bmm         5.01%      57.979ms         6.79%      78.618ms      16.379us       8.351ms         3.62%       8.351ms       1.740us          4800       949.248  \n",
      "                                              aten::add         5.51%      63.857ms         8.38%      97.079ms      12.132us       9.752ms         4.23%       9.752ms       1.219us          8002         8.029  \n",
      "                                              aten::mul         2.24%      25.972ms         3.34%      38.663ms      12.875us       3.241ms         1.40%       3.241ms       1.079us          3003         2.099  \n",
      "                                            aten::empty         5.10%      59.041ms         5.10%      59.041ms       3.131us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       4.084ms         6.19%      71.720ms      17.028us       0.000us         0.00%       1.506ms       0.357us          4212            --  \n",
      "                                         aten::_to_copy         0.77%       8.952ms         5.84%      67.636ms      24.113us       0.000us         0.00%       1.506ms       0.537us          2805            --  \n",
      "                                    aten::empty_strided         1.40%      16.190ms         1.40%      16.190ms       5.386us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.39%      16.109ms         4.74%      54.947ms      14.996us       2.419ms         1.05%       2.419ms       0.660us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.158s\n",
      "Self CUDA time total: 230.785ms\n",
      "\n",
      "output: tensor([[    2, 43043,  1851,     5,   609,     9, 10596,  5390,   634,  4307,\n",
      "          1729,  4454,    12, 36061,   466,   806,     6,     8,  2268,    63,\n",
      "           801,  2975,     8, 13557,  8819,     4, 50118, 50118,   133,  4307,\n",
      "          1729,  4454,    12, 36061,   466,   806,    16,    10,    92,   806,\n",
      "            14,    16,   145,  2226,    30,     5,   589,     9,   886,     6,\n",
      "         10817,     6,     8,    16,   145,   341,     7,  1045,    10,    92,\n",
      "         10596,  5390,   467,     4,    20,   806,    16,   145,   341,     7,\n",
      "          1045,    10,    92, 10596,  5390,   467,    14,    16,   145,   341,\n",
      "             7,  1045,    10,    92, 10596,  5390,   467,    14,    16,   145,\n",
      "           341,     7,  1045,    10,    92, 10596,  5390,   467,    14,    16,\n",
      "           145,   341,     7,  1045,    10,    92, 10596,  5390,   467,    14,\n",
      "            16,   145,   341,     7,  1045,    10,    92, 10596,  5390,   467,\n",
      "            14,    16,   145,   341,     7,  1045,    10,    92, 10596,  5390,\n",
      "           467,    14,    16,   145,   341,     7,  1045,    10,    92, 10596,\n",
      "          5390,   467,    14,    16,   145,   341,     7,  1045,    10,    92,\n",
      "         10596,  5390,   467,    14,    16,   145,   341,     7,  1045,    10,\n",
      "            92, 10596,  5390,   467,    14,    16,   145,   341,     7,  1045,\n",
      "            10,    92, 10596,  5390,   467,    14,    16,   145,   341,     7,\n",
      "          1045,    10,    92, 10596,  5390,   467,    14,    16,   145,   341,\n",
      "             7,  1045,    10,    92, 10596,  5390,   467,    14,    16,   145,\n",
      "           341,     7,  1045,    10,    92, 10596,  5390,   467,    14,    16,\n",
      "           145,   341,     7,  1045,    10,    92, 10596,  5390,   467,    14,\n",
      "            16,   145,   341,     7,  1045,    10]], device='cuda:0')\n",
      "text_energy_per_token: [9.88010567788065]\n",
      "output_tokens: 226\n",
      "flop: 56553974230\n",
      "energy_consumed:  2232.903883201027\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.05%     174.934ms        21.86%     238.281ms      16.547us     122.194ms        52.98%     122.194ms       8.486us         14400     38220.595  \n",
      "                                               aten::mm         0.26%       2.818ms         0.37%       4.065ms      20.323us      44.848ms        19.44%      44.848ms     224.242us           200     17374.003  \n",
      "                                              aten::bmm         5.24%      57.114ms         7.08%      77.192ms      16.082us       8.349ms         3.62%       8.349ms       1.739us          4800       949.248  \n",
      "                                              aten::add         5.69%      62.016ms         8.70%      94.786ms      11.845us       9.762ms         4.23%       9.762ms       1.220us          8002         8.029  \n",
      "                                              aten::mul         2.33%      25.394ms         3.49%      38.021ms      12.661us       3.241ms         1.41%       3.241ms       1.079us          3003         2.099  \n",
      "                                            aten::empty         4.98%      54.316ms         4.98%      54.316ms       2.880us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.779ms         6.35%      69.225ms      16.435us       0.000us         0.00%       1.510ms       0.358us          4212            --  \n",
      "                                         aten::_to_copy         0.75%       8.148ms         6.00%      65.446ms      23.332us       0.000us         0.00%       1.510ms       0.538us          2805            --  \n",
      "                                    aten::empty_strided         1.40%      15.265ms         1.40%      15.265ms       5.078us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.46%      15.937ms         4.98%      54.285ms      14.816us       2.426ms         1.05%       2.426ms       0.662us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.090s\n",
      "Self CUDA time total: 230.661ms\n",
      "\n",
      "output: tensor([[    2, 43043,  1851,     5,   609,     9, 10596,  5390,   634,  4307,\n",
      "          1729,  4454,    12, 36061,   466,   806,     6,     8,  2268,    63,\n",
      "           801,  2975,     8, 13557,  8819,     4, 50118, 50118,   133,  4307,\n",
      "          1729,  4454,    12, 36061,   466,   806,    16,    10,    92,   806,\n",
      "            14,    16,   145,  2226,    30,     5,   589,     9,   886,     6,\n",
      "         10817,     6,     8,    16,   145,   341,     7,  1045,    10,    92,\n",
      "         10596,  5390,   467,     4,    20,   806,    16,   145,   341,     7,\n",
      "          1045,    10,    92, 10596,  5390,   467,    14,    16,   145,   341,\n",
      "             7,  1045,    10,    92, 10596,  5390,   467,    14,    16,   145,\n",
      "           341,     7,  1045,    10,    92, 10596,  5390,   467,    14,    16,\n",
      "           145,   341,     7,  1045,    10,    92, 10596,  5390,   467,    14,\n",
      "            16,   145,   341,     7,  1045,    10,    92, 10596,  5390,   467,\n",
      "            14,    16,   145,   341,     7,  1045,    10,    92, 10596,  5390,\n",
      "           467,    14,    16,   145,   341,     7,  1045,    10,    92, 10596,\n",
      "          5390,   467,    14,    16,   145,   341,     7,  1045,    10,    92,\n",
      "         10596,  5390,   467,    14,    16,   145,   341,     7,  1045,    10,\n",
      "            92, 10596,  5390,   467,    14,    16,   145,   341,     7,  1045,\n",
      "            10,    92, 10596,  5390,   467,    14,    16,   145,   341,     7,\n",
      "          1045,    10,    92, 10596,  5390,   467,    14,    16,   145,   341,\n",
      "             7,  1045,    10,    92, 10596,  5390,   467,    14,    16,   145,\n",
      "           341,     7,  1045,    10,    92, 10596,  5390,   467,    14,    16,\n",
      "           145,   341,     7,  1045,    10,    92, 10596,  5390,   467,    14,\n",
      "            16,   145,   341,     7,  1045,    10]], device='cuda:0')\n",
      "text_energy_per_token: [9.88010567788065, 8.78710965975871]\n",
      "output_tokens: 226\n",
      "flop: 56553974230\n",
      "energy_consumed:  1985.8867831054688\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.76%     172.471ms        21.55%     235.806ms      16.375us     122.254ms        53.01%     122.254ms       8.490us         14400     37201.379  \n",
      "                                               aten::mm         0.38%       4.176ms         0.49%       5.409ms      27.046us      44.900ms        19.47%      44.900ms     224.501us           200     16910.696  \n",
      "                                              aten::bmm         5.40%      59.102ms         7.25%      79.374ms      16.536us       8.305ms         3.60%       8.305ms       1.730us          4800       895.058  \n",
      "                                              aten::add         6.04%      66.090ms         9.04%      98.938ms      12.364us       9.771ms         4.24%       9.771ms       1.221us          8002         7.702  \n",
      "                                              aten::mul         2.33%      25.448ms         3.47%      37.924ms      12.629us       3.246ms         1.41%       3.246ms       1.081us          3003         2.043  \n",
      "                                            aten::empty         4.82%      52.797ms         4.82%      52.797ms       2.800us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.785ms         6.17%      67.524ms      16.031us       0.000us         0.00%       1.487ms       0.353us          4212            --  \n",
      "                                         aten::_to_copy         0.74%       8.090ms         5.82%      63.739ms      22.723us       0.000us         0.00%       1.487ms       0.530us          2805            --  \n",
      "                                    aten::empty_strided         1.29%      14.070ms         1.29%      14.070ms       4.681us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.46%      15.970ms         4.80%      52.538ms      14.339us       2.401ms         1.04%       2.401ms       0.655us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.094s\n",
      "Self CUDA time total: 230.623ms\n",
      "\n",
      "output: tensor([[    2,  6179,   109, 32509,   173,     7,  1744,  2172,     8,  1822,\n",
      "            31, 19166,  6357,     6,     8,    99,    16, 19400, 17381,   116,\n",
      "         50118,   133,   315,   532,    34,    57,    11,     5, 11201,     9,\n",
      "            10,   720, 23387, 14414,    13,     5,   375,   367,   107,     4,\n",
      "            20, 23387, 14414,    34,    57,    10,   720, 23387, 14414,    13,\n",
      "             5,   375,   367,   107,     6,     8,     5,   315,   532,    34,\n",
      "            57,    11,     5, 11201,     9,    10,   720, 23387, 14414,    13,\n",
      "             5,   375,   367,   107,     4,    20, 23387, 14414,    34,    57,\n",
      "            10,   720, 23387, 14414,    13,     5,   375,   367,   107,     6,\n",
      "             8,     5,   315,   532,    34,    57,    11,     5, 11201,     9,\n",
      "            10,   720, 23387, 14414,    13,     5,   375,   367,   107,     4,\n",
      "         50118,   133,   315,   532,    34,    57,    11,     5, 11201,     9,\n",
      "            10,   720, 23387, 14414,    13,     5,   375,   367,   107,     4,\n",
      "            20, 23387, 14414,    34,    57,    10,   720, 23387, 14414,    13,\n",
      "             5,   375,   367,   107,     6,     8,     5,   315,   532,    34,\n",
      "            57,    11,     5, 11201,     9,    10,   720, 23387, 14414,    13,\n",
      "             5,   375,   367,   107,     4, 50118,   133,   315,   532,    34,\n",
      "            57,    11,     5, 11201,     9,    10,   720, 23387, 14414,    13,\n",
      "             5,   375,   367,   107,     4,    20, 23387, 14414,    34,    57,\n",
      "            10,   720, 23387, 14414,    13,     5,   375,   367,   107,     6,\n",
      "             8,     5,   315,   532,    34,    57,    11,     5, 11201,     9,\n",
      "            10,   720, 23387, 14414,    13,     5,   375,   367,   107,     4]],\n",
      "       device='cuda:0')\n",
      "text_energy_per_token: [10.080514990572606]\n",
      "output_tokens: 220\n",
      "flop: 55016878120\n",
      "energy_consumed:  2217.713297925973\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.04%     181.299ms        21.87%     247.136ms      17.162us     122.115ms        53.00%     122.115ms       8.480us         14400     37201.379  \n",
      "                                               aten::mm         0.25%       2.838ms         0.36%       4.072ms      20.361us      44.913ms        19.49%      44.913ms     224.567us           200     16910.696  \n",
      "                                              aten::bmm         5.07%      57.323ms         6.88%      77.739ms      16.196us       8.305ms         3.60%       8.305ms       1.730us          4800       895.058  \n",
      "                                              aten::add         5.57%      63.008ms         8.49%      95.993ms      11.996us       9.760ms         4.24%       9.760ms       1.220us          8002         7.702  \n",
      "                                              aten::mul         2.28%      25.739ms         3.40%      38.419ms      12.794us       3.243ms         1.41%       3.243ms       1.080us          3003         2.043  \n",
      "                                            aten::empty         5.12%      57.894ms         5.12%      57.894ms       3.070us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       4.011ms         6.20%      70.032ms      16.627us       0.000us         0.00%       1.485ms       0.353us          4212            --  \n",
      "                                         aten::_to_copy         0.77%       8.647ms         5.84%      66.021ms      23.537us       0.000us         0.00%       1.485ms       0.529us          2805            --  \n",
      "                                    aten::empty_strided         1.37%      15.516ms         1.37%      15.516ms       5.162us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.42%      16.034ms         4.79%      54.179ms      14.787us       2.399ms         1.04%       2.399ms       0.655us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.130s\n",
      "Self CUDA time total: 230.427ms\n",
      "\n",
      "output: tensor([[    2,  6179,   109, 32509,   173,     7,  1744,  2172,     8,  1822,\n",
      "            31, 19166,  6357,     6,     8,    99,    16, 19400, 17381,   116,\n",
      "         50118,   133,   315,   532,    34,    57,    11,     5, 11201,     9,\n",
      "            10,   720, 23387, 14414,    13,     5,   375,   367,   107,     4,\n",
      "            20, 23387, 14414,    34,    57,    10,   720, 23387, 14414,    13,\n",
      "             5,   375,   367,   107,     6,     8,     5,   315,   532,    34,\n",
      "            57,    11,     5, 11201,     9,    10,   720, 23387, 14414,    13,\n",
      "             5,   375,   367,   107,     4,    20, 23387, 14414,    34,    57,\n",
      "            10,   720, 23387, 14414,    13,     5,   375,   367,   107,     6,\n",
      "             8,     5,   315,   532,    34,    57,    11,     5, 11201,     9,\n",
      "            10,   720, 23387, 14414,    13,     5,   375,   367,   107,     4,\n",
      "         50118,   133,   315,   532,    34,    57,    11,     5, 11201,     9,\n",
      "            10,   720, 23387, 14414,    13,     5,   375,   367,   107,     4,\n",
      "            20, 23387, 14414,    34,    57,    10,   720, 23387, 14414,    13,\n",
      "             5,   375,   367,   107,     6,     8,     5,   315,   532,    34,\n",
      "            57,    11,     5, 11201,     9,    10,   720, 23387, 14414,    13,\n",
      "             5,   375,   367,   107,     4, 50118,   133,   315,   532,    34,\n",
      "            57,    11,     5, 11201,     9,    10,   720, 23387, 14414,    13,\n",
      "             5,   375,   367,   107,     4,    20, 23387, 14414,    34,    57,\n",
      "            10,   720, 23387, 14414,    13,     5,   375,   367,   107,     6,\n",
      "             8,     5,   315,   532,    34,    57,    11,     5, 11201,     9,\n",
      "            10,   720, 23387, 14414,    13,     5,   375,   367,   107,     4]],\n",
      "       device='cuda:0')\n",
      "text_energy_per_token: [10.080514990572606, 9.824999293370679]\n",
      "output_tokens: 220\n",
      "flop: 55016878120\n",
      "energy_consumed:  2161.4998445415495\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.64%     177.643ms        21.35%     242.510ms      16.841us     122.236ms        52.98%     122.236ms       8.489us         14400     38390.465  \n",
      "                                               aten::mm         0.37%       4.202ms         0.48%       5.444ms      27.222us      44.901ms        19.46%      44.901ms     224.504us           200     17451.221  \n",
      "                                              aten::bmm         5.22%      59.320ms         7.04%      79.978ms      16.662us       8.356ms         3.62%       8.356ms       1.741us          4800       958.538  \n",
      "                                              aten::add         5.94%      67.427ms         8.85%     100.552ms      12.566us       9.758ms         4.23%       9.758ms       1.220us          8002         8.084  \n",
      "                                              aten::mul         2.27%      25.782ms         3.37%      38.326ms      12.763us       3.245ms         1.41%       3.245ms       1.080us          3003         2.109  \n",
      "                                            aten::empty         4.79%      54.399ms         4.79%      54.399ms       2.885us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.998ms         6.19%      70.300ms      16.690us       0.000us         0.00%       1.486ms       0.353us          4212            --  \n",
      "                                         aten::_to_copy         0.77%       8.785ms         5.84%      66.301ms      23.637us       0.000us         0.00%       1.486ms       0.530us          2805            --  \n",
      "                                    aten::empty_strided         1.26%      14.308ms         1.26%      14.308ms       4.760us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.41%      16.034ms         4.78%      54.302ms      14.820us       2.401ms         1.04%       2.401ms       0.655us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.136s\n",
      "Self CUDA time total: 230.706ms\n",
      "\n",
      "output: tensor([[    2,  6179,   109,   592,   433,  4818,  2712,     5,   169,    82,\n",
      "         14623,     8,   458,   340,     6,     8,    99,    32,     5,   801,\n",
      "          8819,    13,     5,  2504,     9, 23038,   116, 50118, 50118,   133,\n",
      "           592,   433,  4818,    14,  2712,     5,   169,    82, 14623,     8,\n",
      "           458,   340,     6,     8,    99,    32,     5,   801,  8819,    13,\n",
      "             5,  2504,     9, 23038,   116, 50118, 50118,   133,   592,   433,\n",
      "          4818,    14,  2712,     5,   169,    82, 14623,     8,   458,   340,\n",
      "             6,     8,    99,    32,     5,   801,  8819,    13,     5,  2504,\n",
      "             9, 23038,   116, 50118, 50118,   133,   592,   433,  4818,    14,\n",
      "          2712,     5,   169,    82, 14623,     8,   458,   340,     6,     8,\n",
      "            99,    32,     5,   801,  8819,    13,     5,  2504,     9, 23038,\n",
      "           116, 50118, 50118,   133,   592,   433,  4818,    14,  2712,     5,\n",
      "           169,    82, 14623,     8,   458,   340,     6,     8,    99,    32,\n",
      "             5,   801,  8819,    13,     5,  2504,     9, 23038,   116, 50118,\n",
      "         50118,   133,   592,   433,  4818,    14,  2712,     5,   169,    82,\n",
      "         14623,     8,   458,   340,     6,     8,    99,    32,     5,   801,\n",
      "          8819,    13,     5,  2504,     9, 23038,   116, 50118, 50118,   133,\n",
      "           592,   433,  4818,    14,  2712,     5,   169,    82, 14623,     8,\n",
      "           458,   340,     6,     8,    99,    32,     5,   801,  8819,    13,\n",
      "             5,  2504,     9, 23038,   116, 50118, 50118,   133,   592,   433,\n",
      "          4818,    14,  2712,     5,   169,    82, 14623,     8,   458,   340,\n",
      "             6,     8,    99,    32,     5,   801,  8819,    13,     5,  2504,\n",
      "             9, 23038,   116, 50118, 50118,   133,   592]], device='cuda:0')\n",
      "text_energy_per_token: [9.52275183293788]\n",
      "output_tokens: 227\n",
      "flop: 56810415971\n",
      "energy_consumed:  2161.664666076899\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.08%     181.570ms        21.93%     247.701ms      17.201us     122.323ms        52.99%     122.323ms       8.495us         14400     38390.465  \n",
      "                                               aten::mm         0.25%       2.840ms         0.36%       4.069ms      20.347us      44.884ms        19.44%      44.884ms     224.420us           200     17451.221  \n",
      "                                              aten::bmm         5.07%      57.266ms         6.88%      77.684ms      16.184us       8.358ms         3.62%       8.358ms       1.741us          4800       958.538  \n",
      "                                              aten::add         5.60%      63.195ms         8.52%      96.262ms      12.030us       9.760ms         4.23%       9.760ms       1.220us          8002         8.084  \n",
      "                                              aten::mul         2.28%      25.736ms         3.39%      38.335ms      12.766us       3.248ms         1.41%       3.248ms       1.082us          3003         2.109  \n",
      "                                            aten::empty         5.13%      57.963ms         5.13%      57.963ms       3.074us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.36%       4.041ms         6.08%      68.669ms      16.303us       0.000us         0.00%       1.503ms       0.357us          4212            --  \n",
      "                                         aten::_to_copy         0.76%       8.632ms         5.72%      64.628ms      23.040us       0.000us         0.00%       1.503ms       0.536us          2805            --  \n",
      "                                    aten::empty_strided         1.35%      15.294ms         1.35%      15.294ms       5.088us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.42%      16.014ms         4.67%      52.729ms      14.391us       2.418ms         1.05%       2.418ms       0.660us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.129s\n",
      "Self CUDA time total: 230.845ms\n",
      "\n",
      "output: tensor([[    2,  6179,   109,   592,   433,  4818,  2712,     5,   169,    82,\n",
      "         14623,     8,   458,   340,     6,     8,    99,    32,     5,   801,\n",
      "          8819,    13,     5,  2504,     9, 23038,   116, 50118, 50118,   133,\n",
      "           592,   433,  4818,    14,  2712,     5,   169,    82, 14623,     8,\n",
      "           458,   340,     6,     8,    99,    32,     5,   801,  8819,    13,\n",
      "             5,  2504,     9, 23038,   116, 50118, 50118,   133,   592,   433,\n",
      "          4818,    14,  2712,     5,   169,    82, 14623,     8,   458,   340,\n",
      "             6,     8,    99,    32,     5,   801,  8819,    13,     5,  2504,\n",
      "             9, 23038,   116, 50118, 50118,   133,   592,   433,  4818,    14,\n",
      "          2712,     5,   169,    82, 14623,     8,   458,   340,     6,     8,\n",
      "            99,    32,     5,   801,  8819,    13,     5,  2504,     9, 23038,\n",
      "           116, 50118, 50118,   133,   592,   433,  4818,    14,  2712,     5,\n",
      "           169,    82, 14623,     8,   458,   340,     6,     8,    99,    32,\n",
      "             5,   801,  8819,    13,     5,  2504,     9, 23038,   116, 50118,\n",
      "         50118,   133,   592,   433,  4818,    14,  2712,     5,   169,    82,\n",
      "         14623,     8,   458,   340,     6,     8,    99,    32,     5,   801,\n",
      "          8819,    13,     5,  2504,     9, 23038,   116, 50118, 50118,   133,\n",
      "           592,   433,  4818,    14,  2712,     5,   169,    82, 14623,     8,\n",
      "           458,   340,     6,     8,    99,    32,     5,   801,  8819,    13,\n",
      "             5,  2504,     9, 23038,   116, 50118, 50118,   133,   592,   433,\n",
      "          4818,    14,  2712,     5,   169,    82, 14623,     8,   458,   340,\n",
      "             6,     8,    99,    32,     5,   801,  8819,    13,     5,  2504,\n",
      "             9, 23038,   116, 50118, 50118,   133,   592]], device='cuda:0')\n",
      "text_energy_per_token: [9.52275183293788, 9.407626163915811]\n",
      "output_tokens: 227\n",
      "flop: 56810415971\n",
      "energy_consumed:  2135.5311392088893\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.48%     178.942ms        21.16%     244.631ms      16.988us     122.237ms        52.97%     122.237ms       8.489us         14400     38560.334  \n",
      "                                               aten::mm         0.37%       4.234ms         0.48%       5.501ms      27.504us      44.911ms        19.46%      44.911ms     224.554us           200     17528.439  \n",
      "                                              aten::bmm         5.14%      59.464ms         6.94%      80.257ms      16.720us       8.370ms         3.63%       8.370ms       1.744us          4800       967.901  \n",
      "                                              aten::add         5.90%      68.153ms         8.77%     101.419ms      12.674us       9.772ms         4.23%       9.772ms       1.221us          8002         8.140  \n",
      "                                              aten::mul         2.25%      25.998ms         3.35%      38.673ms      12.878us       3.248ms         1.41%       3.248ms       1.082us          3003         2.118  \n",
      "                                            aten::empty         4.74%      54.847ms         4.74%      54.847ms       2.908us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       4.026ms         6.15%      71.052ms      16.869us       0.000us         0.00%       1.497ms       0.355us          4212            --  \n",
      "                                         aten::_to_copy         0.77%       8.955ms         5.80%      67.026ms      23.895us       0.000us         0.00%       1.497ms       0.534us          2805            --  \n",
      "                                    aten::empty_strided         1.27%      14.627ms         1.27%      14.627ms       4.866us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.39%      16.080ms         4.72%      54.594ms      14.900us       2.412ms         1.05%       2.412ms       0.658us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.156s\n",
      "Self CUDA time total: 230.785ms\n",
      "\n",
      "output: tensor([[    2,  6179,   109,  4106,     6,   592,     6,     8,   776,  2433,\n",
      "          2712,    82,    18,   689,  5717,     6,     8,   141,    64,    42,\n",
      "          2655,    28,   341,     7,  3720, 12732, 22669,   116, 50118, 50118,\n",
      "           133,   892,    21,  2964,    30,     5,   589,     9,   886,     6,\n",
      "         10817,     6,     8,     5,   589,     9,   886,     6, 10817,     6,\n",
      "             8,    21,  1027,    11,     5,  8812, 20056,     4, 50118, 50118,\n",
      "           133,   892,    21,  2964,    30,     5,   589,     9,   886,     6,\n",
      "         10817,     6,     8,     5,   589,     9,   886,     6, 10817,     6,\n",
      "             8,    21,  1027,    11,     5,  8812, 20056,     4, 50118, 50118,\n",
      "           133,   892,    21,  2964,    30,     5,   589,     9,   886,     6,\n",
      "         10817,     6,     8,     5,   589,     9,   886,     6, 10817,     6,\n",
      "             8,    21,  1027,    11,     5,  8812, 20056,     4, 50118, 50118,\n",
      "           133,   892,    21,  2964,    30,     5,   589,     9,   886,     6,\n",
      "         10817,     6,     8,     5,   589,     9,   886,     6, 10817,     6,\n",
      "             8,    21,  1027,    11,     5,  8812, 20056,     4, 50118, 50118,\n",
      "           133,   892,    21,  2964,    30,     5,   589,     9,   886,     6,\n",
      "         10817,     6,     8,     5,   589,     9,   886,     6, 10817,     6,\n",
      "             8,    21,  1027,    11,     5,  8812, 20056,     4, 50118, 50118,\n",
      "           133,   892,    21,  2964,    30,     5,   589,     9,   886,     6,\n",
      "         10817,     6,     8,     5,   589,     9,   886,     6, 10817,     6,\n",
      "             8,    21,  1027,    11,     5,  8812, 20056,     4, 50118, 50118,\n",
      "           133,   892,    21,  2964,    30,     5,   589,     9,   886,     6,\n",
      "         10817,     6,     8,     5,   589,     9,   886,     6]],\n",
      "       device='cuda:0')\n",
      "text_energy_per_token: [8.65712862955729]\n",
      "output_tokens: 228\n",
      "flop: 57066931728\n",
      "energy_consumed:  1973.8253275390623\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.25%     176.271ms        22.08%     239.579ms      16.637us     122.177ms        52.97%     122.177ms       8.484us         14400     38560.334  \n",
      "                                               aten::mm         0.26%       2.800ms         0.37%       4.048ms      20.242us      44.912ms        19.47%      44.912ms     224.560us           200     17528.439  \n",
      "                                              aten::bmm         5.25%      56.910ms         7.09%      76.950ms      16.031us       8.366ms         3.63%       8.366ms       1.743us          4800       967.901  \n",
      "                                              aten::add         5.70%      61.821ms         8.72%      94.625ms      11.825us       9.763ms         4.23%       9.763ms       1.220us          8002         8.140  \n",
      "                                              aten::mul         2.34%      25.373ms         3.50%      37.950ms      12.638us       3.243ms         1.41%       3.243ms       1.080us          3003         2.118  \n",
      "                                            aten::empty         4.96%      53.774ms         4.96%      53.774ms       2.852us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.753ms         6.16%      66.820ms      15.864us       0.000us         0.00%       1.487ms       0.353us          4212            --  \n",
      "                                         aten::_to_copy         0.75%       8.103ms         5.81%      63.067ms      22.484us       0.000us         0.00%       1.487ms       0.530us          2805            --  \n",
      "                                    aten::empty_strided         1.23%      13.303ms         1.23%      13.303ms       4.425us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.47%      15.910ms         4.97%      53.965ms      14.728us       2.402ms         1.04%       2.402ms       0.656us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.085s\n",
      "Self CUDA time total: 230.652ms\n",
      "\n",
      "output: tensor([[    2,  6179,   109,  4106,     6,   592,     6,     8,   776,  2433,\n",
      "          2712,    82,    18,   689,  5717,     6,     8,   141,    64,    42,\n",
      "          2655,    28,   341,     7,  3720, 12732, 22669,   116, 50118, 50118,\n",
      "           133,   892,    21,  2964,    30,     5,   589,     9,   886,     6,\n",
      "         10817,     6,     8,     5,   589,     9,   886,     6, 10817,     6,\n",
      "             8,    21,  1027,    11,     5,  8812, 20056,     4, 50118, 50118,\n",
      "           133,   892,    21,  2964,    30,     5,   589,     9,   886,     6,\n",
      "         10817,     6,     8,     5,   589,     9,   886,     6, 10817,     6,\n",
      "             8,    21,  1027,    11,     5,  8812, 20056,     4, 50118, 50118,\n",
      "           133,   892,    21,  2964,    30,     5,   589,     9,   886,     6,\n",
      "         10817,     6,     8,     5,   589,     9,   886,     6, 10817,     6,\n",
      "             8,    21,  1027,    11,     5,  8812, 20056,     4, 50118, 50118,\n",
      "           133,   892,    21,  2964,    30,     5,   589,     9,   886,     6,\n",
      "         10817,     6,     8,     5,   589,     9,   886,     6, 10817,     6,\n",
      "             8,    21,  1027,    11,     5,  8812, 20056,     4, 50118, 50118,\n",
      "           133,   892,    21,  2964,    30,     5,   589,     9,   886,     6,\n",
      "         10817,     6,     8,     5,   589,     9,   886,     6, 10817,     6,\n",
      "             8,    21,  1027,    11,     5,  8812, 20056,     4, 50118, 50118,\n",
      "           133,   892,    21,  2964,    30,     5,   589,     9,   886,     6,\n",
      "         10817,     6,     8,     5,   589,     9,   886,     6, 10817,     6,\n",
      "             8,    21,  1027,    11,     5,  8812, 20056,     4, 50118, 50118,\n",
      "           133,   892,    21,  2964,    30,     5,   589,     9,   886,     6,\n",
      "         10817,     6,     8,     5,   589,     9,   886,     6]],\n",
      "       device='cuda:0')\n",
      "text_energy_per_token: [8.65712862955729, 9.141871207248222]\n",
      "output_tokens: 228\n",
      "flop: 57066931728\n",
      "energy_consumed:  2084.3466352525948\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.16%     176.992ms        22.08%     241.831ms      16.794us     122.223ms        53.00%     122.223ms       8.488us         14400     37201.379  \n",
      "                                               aten::mm         0.26%       2.817ms         0.37%       4.062ms      20.309us      44.929ms        19.48%      44.929ms     224.643us           200     16910.696  \n",
      "                                              aten::bmm         5.15%      56.432ms         6.98%      76.482ms      15.934us       8.310ms         3.60%       8.310ms       1.731us          4800       895.058  \n",
      "                                              aten::add         5.66%      62.055ms         8.66%      94.877ms      11.857us       9.760ms         4.23%       9.760ms       1.220us          8002         7.702  \n",
      "                                              aten::mul         2.32%      25.397ms         3.45%      37.765ms      12.576us       3.246ms         1.41%       3.246ms       1.081us          3003         2.043  \n",
      "                                            aten::empty         5.19%      56.843ms         5.19%      56.843ms       3.014us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.805ms         6.20%      67.874ms      16.114us       0.000us         0.00%       1.491ms       0.354us          4212            --  \n",
      "                                         aten::_to_copy         0.74%       8.140ms         5.85%      64.069ms      22.841us       0.000us         0.00%       1.491ms       0.532us          2805            --  \n",
      "                                    aten::empty_strided         1.44%      15.760ms         1.44%      15.760ms       5.243us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.46%      15.944ms         4.79%      52.473ms      14.321us       2.406ms         1.04%       2.406ms       0.657us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.095s\n",
      "Self CUDA time total: 230.614ms\n",
      "\n",
      "output: tensor([[    2, 43043,  1851,     5,   609,     9,  1632,  4230,     8,   141,\n",
      "            24, 17992,     7,     5, 10795,     8, 14082,     9,  4707,     4,\n",
      "         50118, 50118, 43043,  1851,     5,   609,     9,  1632,  4230,     8,\n",
      "           141,    24, 17992,     7,     5, 10795,     8, 14082,     9,  4707,\n",
      "             4, 50118, 50118, 43043,  1851,     5,   609,     9,  1632,  4230,\n",
      "             8,   141,    24, 17992,     7,     5, 10795,     8, 14082,     9,\n",
      "          4707,     4, 50118, 50118, 43043,  1851,     5,   609,     9,  1632,\n",
      "          4230,     8,   141,    24, 17992,     7,     5, 10795,     8, 14082,\n",
      "             9,  4707,     4, 50118, 50118, 43043,  1851,     5,   609,     9,\n",
      "          1632,  4230,     8,   141,    24, 17992,     7,     5, 10795,     8,\n",
      "         14082,     9,  4707,     4, 50118, 50118, 43043,  1851,     5,   609,\n",
      "             9,  1632,  4230,     8,   141,    24, 17992,     7,     5, 10795,\n",
      "             8, 14082,     9,  4707,     4, 50118, 50118, 43043,  1851,     5,\n",
      "           609,     9,  1632,  4230,     8,   141,    24, 17992,     7,     5,\n",
      "         10795,     8, 14082,     9,  4707,     4, 50118, 50118, 43043,  1851,\n",
      "             5,   609,     9,  1632,  4230,     8,   141,    24, 17992,     7,\n",
      "             5, 10795,     8, 14082,     9,  4707,     4, 50118, 50118, 43043,\n",
      "          1851,     5,   609,     9,  1632,  4230,     8,   141,    24, 17992,\n",
      "             7,     5, 10795,     8, 14082,     9,  4707,     4, 50118, 50118,\n",
      "         43043,  1851,     5,   609,     9,  1632,  4230,     8,   141,    24,\n",
      "         17992,     7,     5, 10795,     8, 14082,     9,  4707,     4, 50118,\n",
      "         50118, 43043,  1851,     5,   609,     9,  1632,  4230,     8,   141]],\n",
      "       device='cuda:0')\n",
      "text_energy_per_token: [9.701405492167906]\n",
      "output_tokens: 220\n",
      "flop: 55016878120\n",
      "energy_consumed:  2134.3092082769394\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.51%     186.463ms        22.32%     252.136ms      17.509us     122.075ms        52.99%     122.075ms       8.477us         14400     37201.379  \n",
      "                                               aten::mm         0.26%       2.916ms         0.37%       4.170ms      20.848us      44.894ms        19.49%      44.894ms     224.469us           200     16910.696  \n",
      "                                              aten::bmm         5.27%      59.510ms         7.07%      79.814ms      16.628us       8.310ms         3.61%       8.310ms       1.731us          4800       895.058  \n",
      "                                              aten::add         5.54%      62.576ms         8.46%      95.522ms      11.937us       9.758ms         4.24%       9.758ms       1.219us          8002         7.702  \n",
      "                                              aten::mul         2.27%      25.610ms         3.38%      38.133ms      12.698us       3.244ms         1.41%       3.244ms       1.080us          3003         2.043  \n",
      "                                            aten::empty         5.11%      57.721ms         5.11%      57.721ms       3.061us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.952ms         6.17%      69.677ms      16.542us       0.000us         0.00%       1.491ms       0.354us          4212            --  \n",
      "                                         aten::_to_copy         0.75%       8.447ms         5.82%      65.725ms      23.431us       0.000us         0.00%       1.491ms       0.531us          2805            --  \n",
      "                                    aten::empty_strided         1.26%      14.243ms         1.26%      14.243ms       4.738us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.41%      15.927ms         4.80%      54.178ms      14.787us       2.405ms         1.04%       2.405ms       0.657us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.130s\n",
      "Self CUDA time total: 230.382ms\n",
      "\n",
      "output: tensor([[    2, 43043,  1851,     5,   609,     9,  1632,  4230,     8,   141,\n",
      "            24, 17992,     7,     5, 10795,     8, 14082,     9,  4707,     4,\n",
      "         50118, 50118, 43043,  1851,     5,   609,     9,  1632,  4230,     8,\n",
      "           141,    24, 17992,     7,     5, 10795,     8, 14082,     9,  4707,\n",
      "             4, 50118, 50118, 43043,  1851,     5,   609,     9,  1632,  4230,\n",
      "             8,   141,    24, 17992,     7,     5, 10795,     8, 14082,     9,\n",
      "          4707,     4, 50118, 50118, 43043,  1851,     5,   609,     9,  1632,\n",
      "          4230,     8,   141,    24, 17992,     7,     5, 10795,     8, 14082,\n",
      "             9,  4707,     4, 50118, 50118, 43043,  1851,     5,   609,     9,\n",
      "          1632,  4230,     8,   141,    24, 17992,     7,     5, 10795,     8,\n",
      "         14082,     9,  4707,     4, 50118, 50118, 43043,  1851,     5,   609,\n",
      "             9,  1632,  4230,     8,   141,    24, 17992,     7,     5, 10795,\n",
      "             8, 14082,     9,  4707,     4, 50118, 50118, 43043,  1851,     5,\n",
      "           609,     9,  1632,  4230,     8,   141,    24, 17992,     7,     5,\n",
      "         10795,     8, 14082,     9,  4707,     4, 50118, 50118, 43043,  1851,\n",
      "             5,   609,     9,  1632,  4230,     8,   141,    24, 17992,     7,\n",
      "             5, 10795,     8, 14082,     9,  4707,     4, 50118, 50118, 43043,\n",
      "          1851,     5,   609,     9,  1632,  4230,     8,   141,    24, 17992,\n",
      "             7,     5, 10795,     8, 14082,     9,  4707,     4, 50118, 50118,\n",
      "         43043,  1851,     5,   609,     9,  1632,  4230,     8,   141,    24,\n",
      "         17992,     7,     5, 10795,     8, 14082,     9,  4707,     4, 50118,\n",
      "         50118, 43043,  1851,     5,   609,     9,  1632,  4230,     8,   141]],\n",
      "       device='cuda:0')\n",
      "text_energy_per_token: [9.701405492167906, 9.540521937132857]\n",
      "output_tokens: 220\n",
      "flop: 55016878120\n",
      "energy_consumed:  2098.9148261692285\n",
      "Processing category: common-sense\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.55%     176.487ms        21.26%     241.286ms      16.756us     122.199ms        52.97%     122.199ms       8.486us         14400     38050.726  \n",
      "                                               aten::mm         0.37%       4.205ms         0.48%       5.462ms      27.311us      44.907ms        19.47%      44.907ms     224.536us           200     17296.785  \n",
      "                                              aten::bmm         5.23%      59.386ms         7.04%      79.936ms      16.653us       8.350ms         3.62%       8.350ms       1.740us          4800       940.032  \n",
      "                                              aten::add         5.93%      67.295ms         8.84%     100.372ms      12.543us       9.766ms         4.23%       9.766ms       1.220us          8002         7.973  \n",
      "                                              aten::mul         2.27%      25.792ms         3.38%      38.416ms      12.792us       3.250ms         1.41%       3.250ms       1.082us          3003         2.090  \n",
      "                                            aten::empty         4.92%      55.811ms         4.92%      55.811ms       2.960us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.955ms         6.22%      70.577ms      16.756us       0.000us         0.00%       1.505ms       0.357us          4212            --  \n",
      "                                         aten::_to_copy         0.77%       8.757ms         5.87%      66.622ms      23.751us       0.000us         0.00%       1.505ms       0.537us          2805            --  \n",
      "                                    aten::empty_strided         1.27%      14.399ms         1.27%      14.399ms       4.790us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.41%      16.011ms         4.81%      54.560ms      14.891us       2.421ms         1.05%       2.421ms       0.661us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.135s\n",
      "Self CUDA time total: 230.702ms\n",
      "\n",
      "output: tensor([[    2,  6179,    64,    47,  3094,   114,    10,  2391,    16,  1406,\n",
      "           566,  8803,    50,  4412, 21538,  6349,     6,     8,   596,   429,\n",
      "            42,   335,    28,  5616,   116, 50118, 50118,   133,  1049,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349,     4, 50118, 50118,   133,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349,     4, 50118, 50118,   133,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349,     4, 50118, 50118,   133,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349,     4, 50118, 50118,   133,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349,     4, 50118, 50118,   133,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349,     4, 50118, 50118,   133,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349,     4, 50118, 50118,   133,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349,     4, 50118, 50118,   133,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349,     4, 50118, 50118,   133,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349]], device='cuda:0')\n",
      "text_energy_per_token: [9.272540264322917]\n",
      "output_tokens: 225\n",
      "flop: 56297606505\n",
      "energy_consumed:  2086.321559472656\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.01%     179.650ms        21.88%     245.516ms      17.050us     122.269ms        52.96%     122.269ms       8.491us         14400     38050.726  \n",
      "                                               aten::mm         0.25%       2.831ms         0.36%       4.078ms      20.392us      44.937ms        19.47%      44.937ms     224.687us           200     17296.785  \n",
      "                                              aten::bmm         5.08%      57.057ms         6.90%      77.410ms      16.127us       8.355ms         3.62%       8.355ms       1.741us          4800       940.032  \n",
      "                                              aten::add         5.63%      63.173ms         8.57%      96.130ms      12.013us       9.779ms         4.24%       9.779ms       1.222us          8002         7.973  \n",
      "                                              aten::mul         2.30%      25.771ms         3.41%      38.289ms      12.750us       3.250ms         1.41%       3.250ms       1.082us          3003         2.090  \n",
      "                                            aten::empty         5.14%      57.632ms         5.14%      57.632ms       3.056us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.944ms         6.10%      68.449ms      16.251us       0.000us         0.00%       1.506ms       0.358us          4212            --  \n",
      "                                         aten::_to_copy         0.76%       8.574ms         5.75%      64.505ms      22.996us       0.000us         0.00%       1.506ms       0.537us          2805            --  \n",
      "                                    aten::empty_strided         1.38%      15.469ms         1.38%      15.469ms       5.146us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.43%      16.037ms         4.70%      52.752ms      14.397us       2.423ms         1.05%       2.423ms       0.661us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.122s\n",
      "Self CUDA time total: 230.856ms\n",
      "\n",
      "output: tensor([[    2,  6179,    64,    47,  3094,   114,    10,  2391,    16,  1406,\n",
      "           566,  8803,    50,  4412, 21538,  6349,     6,     8,   596,   429,\n",
      "            42,   335,    28,  5616,   116, 50118, 50118,   133,  1049,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349,     4, 50118, 50118,   133,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349,     4, 50118, 50118,   133,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349,     4, 50118, 50118,   133,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349,     4, 50118, 50118,   133,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349,     4, 50118, 50118,   133,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349,     4, 50118, 50118,   133,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349,     4, 50118, 50118,   133,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349,     4, 50118, 50118,   133,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349,     4, 50118, 50118,   133,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349]], device='cuda:0')\n",
      "text_energy_per_token: [9.272540264322917, 9.288968528618284]\n",
      "output_tokens: 225\n",
      "flop: 56297606505\n",
      "energy_consumed:  2090.0179189391138\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total KFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        14.78%      17.197ms        20.60%      23.959ms      18.487us      11.389ms        53.80%      11.389ms       8.788us          1296   7304380.416  \n",
      "                                               aten::mm         0.23%     270.733us         0.34%     391.044us      21.725us       4.000ms        18.90%       4.000ms     222.226us            18   3320365.056  \n",
      "                                              aten::bmm         4.85%       5.637ms         6.55%       7.621ms      17.642us     815.013us         3.85%     815.013us       1.887us           432     46854.144  \n",
      "                                              aten::add         5.29%       6.148ms         7.99%       9.299ms      12.880us     879.429us         4.15%     879.429us       1.218us           722      1008.755  \n",
      "                                              aten::mul         2.21%       2.567ms         3.26%       3.795ms      13.900us     297.089us         1.40%     297.089us       1.088us           273       396.998  \n",
      "                                            aten::empty         5.88%       6.841ms         5.88%       6.841ms       3.909us       0.000us         0.00%       0.000us       0.000us          1750            --  \n",
      "                                               aten::to         0.31%     357.321us         9.00%      10.473ms      26.853us       0.000us         0.00%     138.368us       0.355us           390            --  \n",
      "                                         aten::_to_copy         0.78%     901.735us         8.70%      10.115ms      39.359us       0.000us         0.00%     138.368us       0.538us           257            --  \n",
      "                                    aten::empty_strided         2.93%       3.405ms         2.93%       3.405ms      12.336us       0.000us         0.00%       0.000us       0.000us           276            --  \n",
      "                                            aten::copy_         1.76%       2.048ms         6.66%       7.751ms      19.976us     298.594us         1.41%     298.594us       0.770us           388            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 116.321ms\n",
      "Self CUDA time total: 21.170ms\n",
      "\n",
      "output: tensor([[    2,  2264,    32,   103, 12405, 14885,    14,  3608,   951,    16,\n",
      "         23748,     7,  1346,    10,  5674,    50,  1607,    77,    51,    32,\n",
      "           888, 10985,    50, 21969, 10312,   116, 50118,   100,   206,    24,\n",
      "            18,   142,    51,   214,    45,   269,   686,    99,    51,   214,\n",
      "          1686,    59,     4,     2]], device='cuda:0')\n",
      "text_energy_per_token: [2.5865397154873064]\n",
      "output_tokens: 44\n",
      "flop: 10673005369\n",
      "energy_consumed:  113.80774748144148\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total KFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.48%      17.564ms        21.45%      24.330ms      18.773us      11.392ms        53.81%      11.392ms       8.790us          1296   7304380.416  \n",
      "                                               aten::mm         0.25%     279.361us         0.35%     400.391us      22.244us       3.992ms        18.86%       3.992ms     221.794us            18   3320365.056  \n",
      "                                              aten::bmm         5.08%       5.758ms         6.82%       7.736ms      17.907us     815.079us         3.85%     815.079us       1.887us           432     46854.144  \n",
      "                                              aten::add         5.55%       6.293ms         8.40%       9.526ms      13.193us     879.117us         4.15%     879.117us       1.218us           722      1008.755  \n",
      "                                              aten::mul         2.30%       2.610ms         3.40%       3.858ms      14.132us     297.283us         1.40%     297.283us       1.089us           273       396.998  \n",
      "                                            aten::empty         6.08%       6.892ms         6.08%       6.892ms       3.938us       0.000us         0.00%       0.000us       0.000us          1750            --  \n",
      "                                               aten::to         0.33%     369.171us         6.43%       7.299ms      18.715us       0.000us         0.00%     139.746us       0.358us           390            --  \n",
      "                                         aten::_to_copy         0.75%     847.145us         6.11%       6.930ms      26.963us       0.000us         0.00%     139.746us       0.544us           257            --  \n",
      "                                    aten::empty_strided         1.37%       1.559ms         1.37%       1.559ms       5.647us       0.000us         0.00%       0.000us       0.000us           276            --  \n",
      "                                            aten::copy_         1.86%       2.111ms         5.72%       6.490ms      16.727us     301.315us         1.42%     301.315us       0.777us           388            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 113.436ms\n",
      "Self CUDA time total: 21.171ms\n",
      "\n",
      "output: tensor([[    2,  2264,    32,   103, 12405, 14885,    14,  3608,   951,    16,\n",
      "         23748,     7,  1346,    10,  5674,    50,  1607,    77,    51,    32,\n",
      "           888, 10985,    50, 21969, 10312,   116, 50118,   100,   206,    24,\n",
      "            18,   142,    51,   214,    45,   269,   686,    99,    51,   214,\n",
      "          1686,    59,     4,     2]], device='cuda:0')\n",
      "text_energy_per_token: [2.5865397154873064, 13.580881542876636]\n",
      "output_tokens: 44\n",
      "flop: 10673005369\n",
      "energy_consumed:  597.558787886572\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.11%     175.888ms        22.03%     240.596ms      16.708us     122.119ms        52.96%     122.119ms       8.480us         14400     38050.726  \n",
      "                                               aten::mm         0.26%       2.795ms         0.37%       4.033ms      20.166us      44.918ms        19.48%      44.918ms     224.588us           200     17296.785  \n",
      "                                              aten::bmm         5.16%      56.401ms         7.00%      76.441ms      15.925us       8.364ms         3.63%       8.364ms       1.743us          4800       940.032  \n",
      "                                              aten::add         5.68%      61.988ms         8.68%      94.810ms      11.848us       9.761ms         4.23%       9.761ms       1.220us          8002         7.973  \n",
      "                                              aten::mul         2.33%      25.423ms         3.48%      37.967ms      12.643us       3.247ms         1.41%       3.247ms       1.081us          3003         2.090  \n",
      "                                            aten::empty         5.10%      55.744ms         5.10%      55.744ms       2.956us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.768ms         6.25%      68.211ms      16.195us       0.000us         0.00%       1.497ms       0.356us          4212            --  \n",
      "                                         aten::_to_copy         0.74%       8.128ms         5.90%      64.444ms      22.975us       0.000us         0.00%       1.497ms       0.534us          2805            --  \n",
      "                                    aten::empty_strided         1.23%      13.481ms         1.23%      13.481ms       4.485us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.46%      15.915ms         5.04%      55.085ms      15.034us       2.412ms         1.05%       2.412ms       0.658us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.092s\n",
      "Self CUDA time total: 230.595ms\n",
      "\n",
      "output: tensor([[    2,  7608,   429,   951,  2807,     7,   304,    10,  2225,  5456,\n",
      "            50,  1394,    13,  9969,  1386,     9, 13304,    15,    10, 11921,\n",
      "          2187,    50,  4368,  1553,   116, 50118, 10105,    24,    18,  3013,\n",
      "             7,   304,    10,  4368,  1553,     4, 50118,   100,   437,    45,\n",
      "           584,    24,    18,  3013,     6,    38,   437,   584,    24,    18,\n",
      "          3013,     7,   304,    10,  2225,  5456,     4, 50118,   100,   437,\n",
      "           584,    24,    18,  3013,     7,   304,    10,  2225,  5456,     4,\n",
      "         50118,   100,   437,   584,    24,    18,  3013,     7,   304,    10,\n",
      "          2225,  5456,     4, 50118,   100,   437,   584,    24,    18,  3013,\n",
      "             7,   304,    10,  2225,  5456,     4, 50118,   100,   437,   584,\n",
      "            24,    18,  3013,     7,   304,    10,  2225,  5456,     4, 50118,\n",
      "           100,   437,   584,    24,    18,  3013,     7,   304,    10,  2225,\n",
      "          5456,     4, 50118,   100,   437,   584,    24,    18,  3013,     7,\n",
      "           304,    10,  2225,  5456,     4, 50118,   100,   437,   584,    24,\n",
      "            18,  3013,     7,   304,    10,  2225,  5456,     4, 50118,   100,\n",
      "           437,   584,    24,    18,  3013,     7,   304,    10,  2225,  5456,\n",
      "             4, 50118,   100,   437,   584,    24,    18,  3013,     7,   304,\n",
      "            10,  2225,  5456,     4, 50118,   100,   437,   584,    24,    18,\n",
      "          3013,     7,   304,    10,  2225,  5456,     4, 50118,   100,   437,\n",
      "           584,    24,    18,  3013,     7,   304,    10,  2225,  5456,     4,\n",
      "         50118,   100,   437,   584,    24,    18,  3013,     7,   304,    10,\n",
      "          2225,  5456,     4, 50118,   100,   437,   584,    24,    18,  3013,\n",
      "             7,   304,    10,  2225,  5456]], device='cuda:0')\n",
      "text_energy_per_token: [7.581789022197724]\n",
      "output_tokens: 225\n",
      "flop: 56297606505\n",
      "energy_consumed:  1705.9025299944878\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.76%     174.208ms        21.57%     238.356ms      16.553us     122.215ms        52.99%     122.215ms       8.487us         14400     38050.726  \n",
      "                                               aten::mm         0.25%       2.803ms         0.37%       4.057ms      20.284us      44.903ms        19.47%      44.903ms     224.516us           200     17296.785  \n",
      "                                              aten::bmm         5.18%      57.221ms         7.01%      77.513ms      16.149us       8.335ms         3.61%       8.335ms       1.737us          4800       940.032  \n",
      "                                              aten::add         5.68%      62.740ms         8.66%      95.710ms      11.961us       9.764ms         4.23%       9.764ms       1.220us          8002         7.973  \n",
      "                                              aten::mul         2.32%      25.637ms         3.46%      38.235ms      12.732us       3.244ms         1.41%       3.244ms       1.080us          3003         2.090  \n",
      "                                            aten::empty         4.79%      52.915ms         4.79%      52.915ms       2.806us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.901ms         6.34%      70.036ms      16.628us       0.000us         0.00%       1.493ms       0.355us          4212            --  \n",
      "                                         aten::_to_copy         0.77%       8.508ms         5.98%      66.135ms      23.578us       0.000us         0.00%       1.493ms       0.532us          2805            --  \n",
      "                                    aten::empty_strided         1.29%      14.244ms         1.29%      14.244ms       4.739us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.44%      15.969ms         4.93%      54.517ms      14.879us       2.409ms         1.04%       2.409ms       0.657us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.105s\n",
      "Self CUDA time total: 230.657ms\n",
      "\n",
      "output: tensor([[    2,  7608,   429,   951,  2807,     7,   304,    10,  2225,  5456,\n",
      "            50,  1394,    13,  9969,  1386,     9, 13304,    15,    10, 11921,\n",
      "          2187,    50,  4368,  1553,   116, 50118, 10105,    24,    18,  3013,\n",
      "             7,   304,    10,  4368,  1553,     4, 50118,   100,   437,    45,\n",
      "           584,    24,    18,  3013,     6,    38,   437,   584,    24,    18,\n",
      "          3013,     7,   304,    10,  2225,  5456,     4, 50118,   100,   437,\n",
      "           584,    24,    18,  3013,     7,   304,    10,  2225,  5456,     4,\n",
      "         50118,   100,   437,   584,    24,    18,  3013,     7,   304,    10,\n",
      "          2225,  5456,     4, 50118,   100,   437,   584,    24,    18,  3013,\n",
      "             7,   304,    10,  2225,  5456,     4, 50118,   100,   437,   584,\n",
      "            24,    18,  3013,     7,   304,    10,  2225,  5456,     4, 50118,\n",
      "           100,   437,   584,    24,    18,  3013,     7,   304,    10,  2225,\n",
      "          5456,     4, 50118,   100,   437,   584,    24,    18,  3013,     7,\n",
      "           304,    10,  2225,  5456,     4, 50118,   100,   437,   584,    24,\n",
      "            18,  3013,     7,   304,    10,  2225,  5456,     4, 50118,   100,\n",
      "           437,   584,    24,    18,  3013,     7,   304,    10,  2225,  5456,\n",
      "             4, 50118,   100,   437,   584,    24,    18,  3013,     7,   304,\n",
      "            10,  2225,  5456,     4, 50118,   100,   437,   584,    24,    18,\n",
      "          3013,     7,   304,    10,  2225,  5456,     4, 50118,   100,   437,\n",
      "           584,    24,    18,  3013,     7,   304,    10,  2225,  5456,     4,\n",
      "         50118,   100,   437,   584,    24,    18,  3013,     7,   304,    10,\n",
      "          2225,  5456,     4, 50118,   100,   437,   584,    24,    18,  3013,\n",
      "             7,   304,    10,  2225,  5456]], device='cuda:0')\n",
      "text_energy_per_token: [7.581789022197724, 9.34931968732622]\n",
      "output_tokens: 225\n",
      "flop: 56297606505\n",
      "energy_consumed:  2103.5969296483995\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.93%     173.994ms        21.84%     238.497ms      16.562us     122.097ms        52.97%     122.097ms       8.479us         14400     37031.510  \n",
      "                                               aten::mm         0.26%       2.789ms         0.37%       4.035ms      20.176us      44.925ms        19.49%      44.925ms     224.627us           200     16833.479  \n",
      "                                              aten::bmm         5.16%      56.334ms         6.99%      76.343ms      15.905us       8.298ms         3.60%       8.298ms       1.729us          4800       886.284  \n",
      "                                              aten::add         5.67%      61.895ms         8.67%      94.707ms      11.835us       9.776ms         4.24%       9.776ms       1.222us          8002         7.648  \n",
      "                                              aten::mul         2.32%      25.389ms         3.46%      37.809ms      12.590us       3.247ms         1.41%       3.247ms       1.081us          3003         2.033  \n",
      "                                            aten::empty         5.10%      55.723ms         5.10%      55.723ms       2.955us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.34%       3.754ms         6.18%      67.527ms      16.032us       0.000us         0.00%       1.499ms       0.356us          4212            --  \n",
      "                                         aten::_to_copy         0.75%       8.215ms         5.84%      63.773ms      22.736us       0.000us         0.00%       1.499ms       0.535us          2805            --  \n",
      "                                    aten::empty_strided         1.40%      15.344ms         1.40%      15.344ms       5.104us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.46%      15.919ms         4.81%      52.495ms      14.327us       2.415ms         1.05%       2.415ms       0.659us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.092s\n",
      "Self CUDA time total: 230.482ms\n",
      "\n",
      "output: tensor([[    2,  6179,    64,    47,  3094,   114,    10,   621,    16, 14528,\n",
      "          2509,    11,    10,  1607,    50,  1622,   145, 24908,   116, 50118,\n",
      "           100,   206,    24,    18,    55,     9,    10,    22,   100,   437,\n",
      "            45,  2509,    11,    10,  1607,   113,   631,     4,  1437,    38,\n",
      "           437,    45,   686,   114,    24,    18,    95,   162,    50,   114,\n",
      "            24,    18,    95,     5,   169,    38,   437,   341,     7,    24,\n",
      "             4,  1437,    38,   437,    45,   686,   114,    24,    18,    95,\n",
      "           162,    50,   114,    24,    18,    95,     5,   169,    38,   437,\n",
      "           341,     7,    24,     4,  1437,    38,   437,    45,   686,   114,\n",
      "            24,    18,    95,   162,    50,   114,    24,    18,    95,     5,\n",
      "           169,    38,   437,   341,     7,    24,     4,  1437,    38,   437,\n",
      "            45,   686,   114,    24,    18,    95,   162,    50,   114,    24,\n",
      "            18,    95,     5,   169,    38,   437,   341,     7,    24,     4,\n",
      "          1437,    38,   437,    45,   686,   114,    24,    18,    95,   162,\n",
      "            50,   114,    24,    18,    95,     5,   169,    38,   437,   341,\n",
      "             7,    24,     4,  1437,    38,   437,    45,   686,   114,    24,\n",
      "            18,    95,   162,    50,   114,    24,    18,    95,     5,   169,\n",
      "            38,   437,   341,     7,    24,     4,  1437,    38,   437,    45,\n",
      "           686,   114,    24,    18,    95,   162,    50,   114,    24,    18,\n",
      "            95,     5,   169,    38,   437,   341,     7,    24,     4, 50118,\n",
      "           100,   206,    24,    18,    55,     9,    10,    22,   100,   437,\n",
      "            45,  2509,    11,    10,  1607,   113,   631,     4,  1437]],\n",
      "       device='cuda:0')\n",
      "text_energy_per_token: [8.854990637261574]\n",
      "output_tokens: 219\n",
      "flop: 54760954491\n",
      "energy_consumed:  1939.2429495602846\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.11%     181.188ms        21.87%     245.923ms      17.078us     122.139ms        52.99%     122.139ms       8.482us         14400     37031.510  \n",
      "                                               aten::mm         0.25%       2.820ms         0.36%       4.058ms      20.289us      44.903ms        19.48%      44.903ms     224.513us           200     16833.479  \n",
      "                                              aten::bmm         5.09%      57.272ms         6.91%      77.676ms      16.183us       8.300ms         3.60%       8.300ms       1.729us          4800       886.284  \n",
      "                                              aten::add         5.61%      63.038ms         8.54%      96.021ms      12.000us       9.769ms         4.24%       9.769ms       1.221us          8002         7.648  \n",
      "                                              aten::mul         2.30%      25.849ms         3.59%      40.407ms      13.456us       3.246ms         1.41%       3.246ms       1.081us          3003         2.033  \n",
      "                                            aten::empty         4.95%      55.672ms         4.95%      55.672ms       2.952us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.912ms         6.11%      68.682ms      16.306us       0.000us         0.00%       1.488ms       0.353us          4212            --  \n",
      "                                         aten::_to_copy         0.77%       8.708ms         5.76%      64.769ms      23.091us       0.000us         0.00%       1.488ms       0.531us          2805            --  \n",
      "                                    aten::empty_strided         1.25%      14.067ms         1.25%      14.067ms       4.680us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.43%      16.027ms         4.84%      54.440ms      14.858us       2.403ms         1.04%       2.403ms       0.656us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.124s\n",
      "Self CUDA time total: 230.487ms\n",
      "\n",
      "output: tensor([[    2,  6179,    64,    47,  3094,   114,    10,   621,    16, 14528,\n",
      "          2509,    11,    10,  1607,    50,  1622,   145, 24908,   116, 50118,\n",
      "           100,   206,    24,    18,    55,     9,    10,    22,   100,   437,\n",
      "            45,  2509,    11,    10,  1607,   113,   631,     4,  1437,    38,\n",
      "           437,    45,   686,   114,    24,    18,    95,   162,    50,   114,\n",
      "            24,    18,    95,     5,   169,    38,   437,   341,     7,    24,\n",
      "             4,  1437,    38,   437,    45,   686,   114,    24,    18,    95,\n",
      "           162,    50,   114,    24,    18,    95,     5,   169,    38,   437,\n",
      "           341,     7,    24,     4,  1437,    38,   437,    45,   686,   114,\n",
      "            24,    18,    95,   162,    50,   114,    24,    18,    95,     5,\n",
      "           169,    38,   437,   341,     7,    24,     4,  1437,    38,   437,\n",
      "            45,   686,   114,    24,    18,    95,   162,    50,   114,    24,\n",
      "            18,    95,     5,   169,    38,   437,   341,     7,    24,     4,\n",
      "          1437,    38,   437,    45,   686,   114,    24,    18,    95,   162,\n",
      "            50,   114,    24,    18,    95,     5,   169,    38,   437,   341,\n",
      "             7,    24,     4,  1437,    38,   437,    45,   686,   114,    24,\n",
      "            18,    95,   162,    50,   114,    24,    18,    95,     5,   169,\n",
      "            38,   437,   341,     7,    24,     4,  1437,    38,   437,    45,\n",
      "           686,   114,    24,    18,    95,   162,    50,   114,    24,    18,\n",
      "            95,     5,   169,    38,   437,   341,     7,    24,     4, 50118,\n",
      "           100,   206,    24,    18,    55,     9,    10,    22,   100,   437,\n",
      "            45,  2509,    11,    10,  1607,   113,   631,     4,  1437]],\n",
      "       device='cuda:0')\n",
      "text_energy_per_token: [8.854990637261574, 9.675778473297212]\n",
      "output_tokens: 219\n",
      "flop: 54760954491\n",
      "energy_consumed:  2118.9954856520894\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.63%     175.896ms        21.56%     242.667ms      16.852us     122.232ms        52.95%     122.232ms       8.488us         14400     38730.203  \n",
      "                                               aten::mm         0.37%       4.179ms         0.48%       5.431ms      27.155us      44.874ms        19.44%      44.874ms     224.368us           200     17605.657  \n",
      "                                              aten::bmm         5.35%      60.244ms         7.18%      80.813ms      16.836us       8.370ms         3.63%       8.370ms       1.744us          4800       977.338  \n",
      "                                              aten::add         5.85%      65.849ms         8.78%      98.824ms      12.350us       9.792ms         4.24%       9.792ms       1.224us          8002         8.196  \n",
      "                                              aten::mul         2.28%      25.704ms         3.40%      38.250ms      12.737us       3.245ms         1.41%       3.245ms       1.080us          3003         2.127  \n",
      "                                            aten::empty         4.78%      53.742ms         4.78%      53.742ms       2.850us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.974ms         6.21%      69.830ms      16.579us       0.000us         0.00%       1.497ms       0.355us          4212            --  \n",
      "                                         aten::_to_copy         0.75%       8.488ms         5.85%      65.856ms      23.478us       0.000us         0.00%       1.497ms       0.534us          2805            --  \n",
      "                                    aten::empty_strided         1.26%      14.216ms         1.26%      14.216ms       4.729us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.42%      15.997ms         4.82%      54.266ms      14.811us       2.414ms         1.05%       2.414ms       0.659us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.125s\n",
      "Self CUDA time total: 230.834ms\n",
      "\n",
      "output: tensor([[    2,  7608,   429,   951,  6573,     7,  2792,    23,    10,   650,\n",
      "             6,  8094,    12,  4447,   265,  1386,     9,    10,   739,  3206,\n",
      "          1400,     6,   190,   114,     5,   850,    32,   723,   116, 50118,\n",
      "         10105,    51,   214,    45,   164,     7,    28,   441,     7,  4960,\n",
      "             7,   907,     5,   276,  2682,    23,    10,   400,  1400,     4,\n",
      "         50118,   100,   437,    45,   584,    14,    51,   214,    45,   164,\n",
      "             7,    28,   441,     7,  4960,     7,   907,     5,   276,  2682,\n",
      "            23,    10,   400,  1400,     6,    53,    51,   214,    45,   164,\n",
      "             7,    28,   441,     7,  4960,     7,   907,     5,   276,  2682,\n",
      "            23,    10,   400,  1400,     4,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437]],\n",
      "       device='cuda:0')\n",
      "text_energy_per_token: [9.136712157982407]\n",
      "output_tokens: 229\n",
      "flop: 57323521501\n",
      "energy_consumed:  2092.3070841779713\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.21%     179.829ms        22.07%     244.868ms      17.005us     122.420ms        53.01%     122.420ms       8.501us         14400     38730.203  \n",
      "                                               aten::mm         0.25%       2.816ms         0.36%       4.047ms      20.233us      44.851ms        19.42%      44.851ms     224.253us           200     17605.657  \n",
      "                                              aten::bmm         5.16%      57.260ms         6.99%      77.493ms      16.144us       8.366ms         3.62%       8.366ms       1.743us          4800       977.338  \n",
      "                                              aten::add         5.65%      62.674ms         8.61%      95.551ms      11.941us       9.775ms         4.23%       9.775ms       1.222us          8002         8.196  \n",
      "                                              aten::mul         2.31%      25.628ms         3.44%      38.167ms      12.709us       3.246ms         1.41%       3.246ms       1.081us          3003         2.127  \n",
      "                                            aten::empty         5.06%      56.144ms         5.06%      56.144ms       2.977us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.840ms         6.12%      67.845ms      16.107us       0.000us         0.00%       1.495ms       0.355us          4212            --  \n",
      "                                         aten::_to_copy         0.75%       8.342ms         5.77%      64.005ms      22.818us       0.000us         0.00%       1.495ms       0.533us          2805            --  \n",
      "                                    aten::empty_strided         1.39%      15.442ms         1.39%      15.442ms       5.137us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.44%      15.975ms         4.74%      52.556ms      14.344us       2.412ms         1.04%       2.412ms       0.658us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.109s\n",
      "Self CUDA time total: 230.935ms\n",
      "\n",
      "output: tensor([[    2,  7608,   429,   951,  6573,     7,  2792,    23,    10,   650,\n",
      "             6,  8094,    12,  4447,   265,  1386,     9,    10,   739,  3206,\n",
      "          1400,     6,   190,   114,     5,   850,    32,   723,   116, 50118,\n",
      "         10105,    51,   214,    45,   164,     7,    28,   441,     7,  4960,\n",
      "             7,   907,     5,   276,  2682,    23,    10,   400,  1400,     4,\n",
      "         50118,   100,   437,    45,   584,    14,    51,   214,    45,   164,\n",
      "             7,    28,   441,     7,  4960,     7,   907,     5,   276,  2682,\n",
      "            23,    10,   400,  1400,     6,    53,    51,   214,    45,   164,\n",
      "             7,    28,   441,     7,  4960,     7,   907,     5,   276,  2682,\n",
      "            23,    10,   400,  1400,     4,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437]],\n",
      "       device='cuda:0')\n",
      "text_energy_per_token: [9.136712157982407, 9.163871481016736]\n",
      "output_tokens: 229\n",
      "flop: 57323521501\n",
      "energy_consumed:  2098.5265691528325\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.28%     175.796ms        21.12%     243.013ms      16.876us     122.578ms        53.01%     122.578ms       8.512us         14400     39579.550  \n",
      "                                               aten::mm         0.37%       4.227ms         0.47%       5.436ms      27.181us      44.923ms        19.43%      44.923ms     224.616us           200     17991.746  \n",
      "                                              aten::bmm         5.13%      59.045ms         6.93%      79.715ms      16.607us       8.429ms         3.64%       8.429ms       1.756us          4800      1025.630  \n",
      "                                              aten::add         5.76%      66.313ms         8.65%      99.519ms      12.437us       9.768ms         4.22%       9.768ms       1.221us          8002         8.481  \n",
      "                                              aten::mul         2.37%      27.300ms         3.47%      39.861ms      13.274us       3.245ms         1.40%       3.245ms       1.081us          3003         2.174  \n",
      "                                            aten::empty         4.66%      53.653ms         4.66%      53.653ms       2.845us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       4.036ms         6.28%      72.246ms      17.152us       0.000us         0.00%       1.503ms       0.357us          4212            --  \n",
      "                                         aten::_to_copy         0.77%       8.895ms         5.93%      68.210ms      24.317us       0.000us         0.00%       1.503ms       0.536us          2805            --  \n",
      "                                    aten::empty_strided         1.38%      15.843ms         1.38%      15.843ms       5.271us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.39%      16.016ms         4.74%      54.549ms      14.888us       2.419ms         1.05%       2.419ms       0.660us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.150s\n",
      "Self CUDA time total: 231.256ms\n",
      "\n",
      "output: tensor([[    2,  6179,    64,    47,  7118,     5, 10796,     9,    10,  1300,\n",
      "             9,   335,     6,   215,    25,    10,   340,  1566,    50,  5059,\n",
      "           618,     6,   396, 13304,  9382,    15,     5,  5070,     9,     5,\n",
      "          2730,    50, 10710,   116, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4]], device='cuda:0')\n",
      "text_energy_per_token: [9.039409934942336]\n",
      "output_tokens: 234\n",
      "flop: 58607580606\n",
      "energy_consumed:  2115.2219247765065\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.58%     171.516ms        21.48%     236.499ms      16.424us     122.571ms        53.01%     122.571ms       8.512us         14400     39579.550  \n",
      "                                               aten::mm         0.25%       2.793ms         0.37%       4.118ms      20.590us      44.901ms        19.42%      44.901ms     224.507us           200     17991.746  \n",
      "                                              aten::bmm         5.19%      57.206ms         7.16%      78.883ms      16.434us       8.424ms         3.64%       8.424ms       1.755us          4800      1025.630  \n",
      "                                              aten::add         5.66%      62.308ms         8.65%      95.224ms      11.900us       9.771ms         4.23%       9.771ms       1.221us          8002         8.481  \n",
      "                                              aten::mul         2.44%      26.831ms         3.57%      39.289ms      13.083us       3.245ms         1.40%       3.245ms       1.081us          3003         2.174  \n",
      "                                            aten::empty         4.73%      52.132ms         4.73%      52.132ms       2.764us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.873ms         6.31%      69.518ms      16.505us       0.000us         0.00%       1.507ms       0.358us          4212            --  \n",
      "                                         aten::_to_copy         0.76%       8.343ms         5.96%      65.645ms      23.403us       0.000us         0.00%       1.507ms       0.537us          2805            --  \n",
      "                                    aten::empty_strided         1.28%      14.086ms         1.28%      14.086ms       4.686us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.45%      15.958ms         4.92%      54.222ms      14.799us       2.423ms         1.05%       2.423ms       0.661us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.101s\n",
      "Self CUDA time total: 231.232ms\n",
      "\n",
      "output: tensor([[    2,  6179,    64,    47,  7118,     5, 10796,     9,    10,  1300,\n",
      "             9,   335,     6,   215,    25,    10,   340,  1566,    50,  5059,\n",
      "           618,     6,   396, 13304,  9382,    15,     5,  5070,     9,     5,\n",
      "          2730,    50, 10710,   116, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4]], device='cuda:0')\n",
      "text_energy_per_token: [9.039409934942336, 8.378001347091462]\n",
      "output_tokens: 234\n",
      "flop: 58607580606\n",
      "energy_consumed:  1960.4523152194024\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.73%     174.076ms        21.48%     237.758ms      16.511us     122.341ms        52.97%     122.341ms       8.496us         14400     39069.942  \n",
      "                                               aten::mm         0.38%       4.192ms         0.49%       5.424ms      27.120us      44.914ms        19.45%      44.914ms     224.570us           200     17760.092  \n",
      "                                              aten::bmm         5.36%      59.354ms         7.19%      79.604ms      16.584us       8.392ms         3.63%       8.392ms       1.748us          4800       996.434  \n",
      "                                              aten::add         6.02%      66.659ms         8.99%      99.489ms      12.433us       9.767ms         4.23%       9.767ms       1.221us          8002         8.309  \n",
      "                                              aten::mul         2.31%      25.518ms         3.43%      38.015ms      12.659us       3.245ms         1.41%       3.245ms       1.081us          3003         2.146  \n",
      "                                            aten::empty         4.82%      53.362ms         4.82%      53.362ms       2.830us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.825ms         6.14%      68.001ms      16.145us       0.000us         0.00%       1.502ms       0.356us          4212            --  \n",
      "                                         aten::_to_copy         0.75%       8.301ms         5.80%      64.177ms      22.879us       0.000us         0.00%       1.502ms       0.535us          2805            --  \n",
      "                                    aten::empty_strided         1.30%      14.359ms         1.30%      14.359ms       4.777us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.44%      15.896ms         4.75%      52.539ms      14.339us       2.416ms         1.05%       2.416ms       0.660us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.107s\n",
      "Self CUDA time total: 230.952ms\n",
      "\n",
      "output: tensor([[    2,  7608,   109,   103,    82,  2254,     5, 15583,     9,   145,\n",
      "          8265,     6,   215,    25,    30,  2494,  8444,  4133,    50,   164,\n",
      "            15, 15950,  1029, 22125,     6,   150,   643,  1877,   209,  3734,\n",
      "           116, 50118,   100,   218,    75,   216,     6,    38,   348,   393,\n",
      "            57,  8265,     9,   932,     4,    38,   348,   393,    57,  8265,\n",
      "             9,   932,     4,    38,   348,   393,    57,  8265,     9,   932,\n",
      "             4,    38,   348,   393,    57,  8265,     9,   932,     4,    38,\n",
      "           348,   393,    57,  8265,     9,   932,     4,    38,   348,   393,\n",
      "            57,  8265,     9,   932,     4,    38,   348,   393,    57,  8265,\n",
      "             9,   932,     4,    38,   348,   393,    57,  8265,     9,   932,\n",
      "             4,    38,   348,   393,    57,  8265,     9,   932,     4,    38,\n",
      "           348,   393,    57,  8265,     9,   932,     4,    38,   348,   393,\n",
      "            57,  8265,     9,   932,     4,    38,   348,   393,    57,  8265,\n",
      "             9,   932,     4,    38,   348,   393,    57,  8265,     9,   932,\n",
      "             4,    38,   348,   393,    57,  8265,     9,   932,     4,    38,\n",
      "           348,   393,    57,  8265,     9,   932,     4,    38,   348,   393,\n",
      "            57,  8265,     9,   932,     4,    38,   348,   393,    57,  8265,\n",
      "             9,   932,     4,    38,   348,   393,    57,  8265,     9,   932,\n",
      "             4,    38,   348,   393,    57,  8265,     9,   932,     4,    38,\n",
      "           348,   393,    57,  8265,     9,   932,     4,    38,   348,   393,\n",
      "            57,  8265,     9,   932,     4,    38,   348,   393,    57,  8265,\n",
      "             9,   932,     4,    38,   348,   393,    57,  8265,     9,   932,\n",
      "             4,    38,   348,   393,    57,  8265,     9,   932,     4,    38,\n",
      "           348]], device='cuda:0')\n",
      "text_energy_per_token: [9.234469127914915]\n",
      "output_tokens: 231\n",
      "flop: 57836923095\n",
      "energy_consumed:  2133.1623685483455\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.07%     181.096ms        21.90%     246.868ms      17.144us     122.407ms        52.97%     122.407ms       8.501us         14400     39069.942  \n",
      "                                               aten::mm         0.25%       2.803ms         0.36%       4.046ms      20.228us      44.938ms        19.45%      44.938ms     224.691us           200     17760.092  \n",
      "                                              aten::bmm         5.07%      57.169ms         6.88%      77.499ms      16.146us       8.387ms         3.63%       8.387ms       1.747us          4800       996.434  \n",
      "                                              aten::add         5.59%      63.040ms         8.53%      96.154ms      12.016us       9.775ms         4.23%       9.775ms       1.222us          8002         8.309  \n",
      "                                              aten::mul         2.29%      25.775ms         3.41%      38.432ms      12.798us       3.248ms         1.41%       3.248ms       1.081us          3003         2.146  \n",
      "                                            aten::empty         5.12%      57.757ms         5.12%      57.757ms       3.063us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.994ms         6.22%      70.163ms      16.658us       0.000us         0.00%       1.502ms       0.357us          4212            --  \n",
      "                                         aten::_to_copy         0.77%       8.664ms         5.87%      66.169ms      23.590us       0.000us         0.00%       1.502ms       0.536us          2805            --  \n",
      "                                    aten::empty_strided         1.39%      15.634ms         1.39%      15.634ms       5.201us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.41%      15.893ms         4.81%      54.166ms      14.783us       2.419ms         1.05%       2.419ms       0.660us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.127s\n",
      "Self CUDA time total: 231.084ms\n",
      "\n",
      "output: tensor([[    2,  7608,   109,   103,    82,  2254,     5, 15583,     9,   145,\n",
      "          8265,     6,   215,    25,    30,  2494,  8444,  4133,    50,   164,\n",
      "            15, 15950,  1029, 22125,     6,   150,   643,  1877,   209,  3734,\n",
      "           116, 50118,   100,   218,    75,   216,     6,    38,   348,   393,\n",
      "            57,  8265,     9,   932,     4,    38,   348,   393,    57,  8265,\n",
      "             9,   932,     4,    38,   348,   393,    57,  8265,     9,   932,\n",
      "             4,    38,   348,   393,    57,  8265,     9,   932,     4,    38,\n",
      "           348,   393,    57,  8265,     9,   932,     4,    38,   348,   393,\n",
      "            57,  8265,     9,   932,     4,    38,   348,   393,    57,  8265,\n",
      "             9,   932,     4,    38,   348,   393,    57,  8265,     9,   932,\n",
      "             4,    38,   348,   393,    57,  8265,     9,   932,     4,    38,\n",
      "           348,   393,    57,  8265,     9,   932,     4,    38,   348,   393,\n",
      "            57,  8265,     9,   932,     4,    38,   348,   393,    57,  8265,\n",
      "             9,   932,     4,    38,   348,   393,    57,  8265,     9,   932,\n",
      "             4,    38,   348,   393,    57,  8265,     9,   932,     4,    38,\n",
      "           348,   393,    57,  8265,     9,   932,     4,    38,   348,   393,\n",
      "            57,  8265,     9,   932,     4,    38,   348,   393,    57,  8265,\n",
      "             9,   932,     4,    38,   348,   393,    57,  8265,     9,   932,\n",
      "             4,    38,   348,   393,    57,  8265,     9,   932,     4,    38,\n",
      "           348,   393,    57,  8265,     9,   932,     4,    38,   348,   393,\n",
      "            57,  8265,     9,   932,     4,    38,   348,   393,    57,  8265,\n",
      "             9,   932,     4,    38,   348,   393,    57,  8265,     9,   932,\n",
      "             4,    38,   348,   393,    57,  8265,     9,   932,     4,    38,\n",
      "           348]], device='cuda:0')\n",
      "text_energy_per_token: [9.234469127914915, 9.05496914145544]\n",
      "output_tokens: 231\n",
      "flop: 57836923095\n",
      "energy_consumed:  2091.6978716762064\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.32%     177.536ms        22.23%     241.727ms      16.787us     122.230ms        53.00%     122.230ms       8.488us         14400     37371.249  \n",
      "                                               aten::mm         0.26%       2.791ms         0.37%       4.020ms      20.098us      44.920ms        19.48%      44.920ms     224.601us           200     16987.914  \n",
      "                                              aten::bmm         5.16%      56.095ms         6.98%      75.956ms      15.824us       8.321ms         3.61%       8.321ms       1.734us          4800       903.905  \n",
      "                                              aten::add         5.67%      61.642ms         8.67%      94.319ms      11.787us       9.761ms         4.23%       9.761ms       1.220us          8002         7.756  \n",
      "                                              aten::mul         2.33%      25.387ms         3.49%      37.914ms      12.625us       3.247ms         1.41%       3.247ms       1.081us          3003         2.052  \n",
      "                                            aten::empty         5.11%      55.532ms         5.11%      55.532ms       2.945us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.34%       3.740ms         6.33%      68.820ms      16.339us       0.000us         0.00%       1.498ms       0.356us          4212            --  \n",
      "                                         aten::_to_copy         0.74%       8.010ms         5.98%      65.080ms      23.202us       0.000us         0.00%       1.498ms       0.534us          2805            --  \n",
      "                                    aten::empty_strided         1.41%      15.349ms         1.41%      15.349ms       5.106us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.45%      15.774ms         4.96%      53.911ms      14.714us       2.413ms         1.05%       2.413ms       0.658us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.088s\n",
      "Self CUDA time total: 230.638ms\n",
      "\n",
      "output: tensor([[    2,  6179,    64, 21981,     5,  3650,     9,    97,    82,    11,\n",
      "            10,   592,  1068,   694, 14885,    59,  4106, 14513,     8,  2113,\n",
      "           116, 50118, 50118,  1121,    42,  2225,     6,    52,  4830,     5,\n",
      "           774,     9,  4106, 14513,     8,  2113,    11,     5,  3650,     9,\n",
      "            97,    82,    11,    10,   592,  1068,     4,   166,   465,    14,\n",
      "          4106, 14513,     8,  2113,    32,    45,   129,   505,    13,     5,\n",
      "          3650,     9,    97,    82,     6,    53,    67,    13,     5,  3650,\n",
      "             9,    97,    82,    11,    10,   592,  1068,     4,   166,    67,\n",
      "           465,    14,  4106, 14513,     8,  2113,    32,    45,   129,   505,\n",
      "            13,     5,  3650,     9,    97,    82,     6,    53,    67,    13,\n",
      "             5,  3650,     9,    97,    82,    11,    10,   592,  1068,     4,\n",
      "         50118, 50118,   170,   465,    14,  4106, 14513,     8,  2113,    32,\n",
      "            45,   129,   505,    13,     5,  3650,     9,    97,    82,     6,\n",
      "            53,    67,    13,     5,  3650,     9,    97,    82,    11,    10,\n",
      "           592,  1068,     4,   166,    67,   465,    14,  4106, 14513,     8,\n",
      "          2113,    32,    45,   129,   505,    13,     5,  3650,     9,    97,\n",
      "            82,     6,    53,    67,    13,     5,  3650,     9,    97,    82,\n",
      "            11,    10,   592,  1068,     4, 50118, 50118,   170,   465,    14,\n",
      "          4106, 14513,     8,  2113,    32,    45,   129,   505,    13,     5,\n",
      "          3650,     9,    97,    82,     6,    53,    67,    13,     5,  3650,\n",
      "             9,    97,    82,    11,    10,   592,  1068,     4,   166,    67,\n",
      "           465,    14,  4106, 14513,     8,  2113,    32,    45,   129,   505,\n",
      "            13]], device='cuda:0')\n",
      "text_energy_per_token: [9.193226051432191]\n",
      "output_tokens: 221\n",
      "flop: 55272875765\n",
      "energy_consumed:  2031.7029573665143\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.14%     180.980ms        21.96%     246.227ms      17.099us     122.301ms        53.01%     122.301ms       8.493us         14400     37371.249  \n",
      "                                               aten::mm         0.25%       2.815ms         0.36%       4.080ms      20.401us      44.946ms        19.48%      44.946ms     224.729us           200     16987.914  \n",
      "                                              aten::bmm         5.09%      57.054ms         7.11%      79.759ms      16.617us       8.303ms         3.60%       8.303ms       1.730us          4800       903.905  \n",
      "                                              aten::add         5.62%      63.014ms         8.54%      95.788ms      11.971us       9.761ms         4.23%       9.761ms       1.220us          8002         7.756  \n",
      "                                              aten::mul         2.29%      25.707ms         3.40%      38.154ms      12.705us       3.250ms         1.41%       3.250ms       1.082us          3003         2.052  \n",
      "                                            aten::empty         5.04%      56.541ms         5.04%      56.541ms       2.998us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.968ms         6.11%      68.521ms      16.268us       0.000us         0.00%       1.498ms       0.356us          4212            --  \n",
      "                                         aten::_to_copy         0.77%       8.589ms         5.76%      64.554ms      23.014us       0.000us         0.00%       1.498ms       0.534us          2805            --  \n",
      "                                    aten::empty_strided         1.39%      15.563ms         1.39%      15.563ms       5.177us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.42%      15.973ms         4.69%      52.630ms      14.364us       2.415ms         1.05%       2.415ms       0.659us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.121s\n",
      "Self CUDA time total: 230.725ms\n",
      "\n",
      "output: tensor([[    2,  6179,    64, 21981,     5,  3650,     9,    97,    82,    11,\n",
      "            10,   592,  1068,   694, 14885,    59,  4106, 14513,     8,  2113,\n",
      "           116, 50118, 50118,  1121,    42,  2225,     6,    52,  4830,     5,\n",
      "           774,     9,  4106, 14513,     8,  2113,    11,     5,  3650,     9,\n",
      "            97,    82,    11,    10,   592,  1068,     4,   166,   465,    14,\n",
      "          4106, 14513,     8,  2113,    32,    45,   129,   505,    13,     5,\n",
      "          3650,     9,    97,    82,     6,    53,    67,    13,     5,  3650,\n",
      "             9,    97,    82,    11,    10,   592,  1068,     4,   166,    67,\n",
      "           465,    14,  4106, 14513,     8,  2113,    32,    45,   129,   505,\n",
      "            13,     5,  3650,     9,    97,    82,     6,    53,    67,    13,\n",
      "             5,  3650,     9,    97,    82,    11,    10,   592,  1068,     4,\n",
      "         50118, 50118,   170,   465,    14,  4106, 14513,     8,  2113,    32,\n",
      "            45,   129,   505,    13,     5,  3650,     9,    97,    82,     6,\n",
      "            53,    67,    13,     5,  3650,     9,    97,    82,    11,    10,\n",
      "           592,  1068,     4,   166,    67,   465,    14,  4106, 14513,     8,\n",
      "          2113,    32,    45,   129,   505,    13,     5,  3650,     9,    97,\n",
      "            82,     6,    53,    67,    13,     5,  3650,     9,    97,    82,\n",
      "            11,    10,   592,  1068,     4, 50118, 50118,   170,   465,    14,\n",
      "          4106, 14513,     8,  2113,    32,    45,   129,   505,    13,     5,\n",
      "          3650,     9,    97,    82,     6,    53,    67,    13,     5,  3650,\n",
      "             9,    97,    82,    11,    10,   592,  1068,     4,   166,    67,\n",
      "           465,    14,  4106, 14513,     8,  2113,    32,    45,   129,   505,\n",
      "            13]], device='cuda:0')\n",
      "text_energy_per_token: [9.193226051432191, 9.304171810422009]\n",
      "output_tokens: 221\n",
      "flop: 55272875765\n",
      "energy_consumed:  2056.221970103264\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.57%     178.256ms        21.24%     243.130ms      16.884us     122.201ms        52.99%     122.201ms       8.486us         14400     37541.118  \n",
      "                                               aten::mm         0.37%       4.201ms         0.47%       5.438ms      27.191us      44.911ms        19.48%      44.911ms     224.555us           200     17065.132  \n",
      "                                              aten::bmm         5.28%      60.425ms         7.24%      82.899ms      17.271us       8.310ms         3.60%       8.310ms       1.731us          4800       912.826  \n",
      "                                              aten::add         5.79%      66.330ms         8.67%      99.224ms      12.400us       9.766ms         4.24%       9.766ms       1.220us          8002         7.810  \n",
      "                                              aten::mul         2.27%      25.934ms         3.37%      38.531ms      12.831us       3.247ms         1.41%       3.247ms       1.081us          3003         2.061  \n",
      "                                            aten::empty         4.77%      54.627ms         4.77%      54.627ms       2.897us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.36%       4.079ms         6.16%      70.546ms      16.749us       0.000us         0.00%       1.499ms       0.356us          4212            --  \n",
      "                                         aten::_to_copy         0.77%       8.812ms         5.81%      66.467ms      23.696us       0.000us         0.00%       1.499ms       0.534us          2805            --  \n",
      "                                    aten::empty_strided         1.25%      14.283ms         1.25%      14.283ms       4.752us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.40%      16.045ms         4.75%      54.408ms      14.849us       2.414ms         1.05%       2.414ms       0.659us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.145s\n",
      "Self CUDA time total: 230.599ms\n",
      "\n",
      "output: tensor([[    2,  8275,    52,    33,    10,  7654,  9061,     7,  5393,   980,\n",
      "             6,    50,   197,    52,  1056,    15, 15582,  3875,    18,  1272,\n",
      "            78,   116, 50118, 50118,   100,   437,    45,   686,    99,    47,\n",
      "          1266,    30,    22, 23242,  5137,   980,   845, 50118, 50118,   100,\n",
      "           437,    45,   686,    99,    47,  1266,    30,    22, 23242,  5137,\n",
      "           980,   845, 50118, 50118,   100,   437,    45,   686,    99,    47,\n",
      "          1266,    30,    22, 23242,  5137,   980,   845, 50118, 50118,   100,\n",
      "           437,    45,   686,    99,    47,  1266,    30,    22, 23242,  5137,\n",
      "           980,   845, 50118, 50118,   100,   437,    45,   686,    99,    47,\n",
      "          1266,    30,    22, 23242,  5137,   980,   845, 50118, 50118,   100,\n",
      "           437,    45,   686,    99,    47,  1266,    30,    22, 23242,  5137,\n",
      "           980,   845, 50118, 50118,   100,   437,    45,   686,    99,    47,\n",
      "          1266,    30,    22, 23242,  5137,   980,   845, 50118, 50118,   100,\n",
      "           437,    45,   686,    99,    47,  1266,    30,    22, 23242,  5137,\n",
      "           980,   845, 50118, 50118,   100,   437,    45,   686,    99,    47,\n",
      "          1266,    30,    22, 23242,  5137,   980,   845, 50118, 50118,   100,\n",
      "           437,    45,   686,    99,    47,  1266,    30,    22, 23242,  5137,\n",
      "           980,   845, 50118, 50118,   100,   437,    45,   686,    99,    47,\n",
      "          1266,    30,    22, 23242,  5137,   980,   845, 50118, 50118,   100,\n",
      "           437,    45,   686,    99,    47,  1266,    30,    22, 23242,  5137,\n",
      "           980,   845, 50118, 50118,   100,   437,    45,   686,    99,    47,\n",
      "          1266,    30,    22, 23242,  5137,   980,   845, 50118, 50118,   100,\n",
      "           437,    45]], device='cuda:0')\n",
      "text_energy_per_token: [9.402694862599631]\n",
      "output_tokens: 222\n",
      "flop: 55528947426\n",
      "energy_consumed:  2087.398259497118\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.32%     177.728ms        22.10%     240.637ms      16.711us     122.188ms        52.98%     122.188ms       8.485us         14400     37541.118  \n",
      "                                               aten::mm         0.26%       2.794ms         0.37%       4.038ms      20.191us      44.943ms        19.49%      44.943ms     224.716us           200     17065.132  \n",
      "                                              aten::bmm         5.16%      56.198ms         6.99%      76.145ms      15.864us       8.316ms         3.61%       8.316ms       1.733us          4800       912.826  \n",
      "                                              aten::add         5.68%      61.821ms         8.68%      94.553ms      11.816us       9.761ms         4.23%       9.761ms       1.220us          8002         7.810  \n",
      "                                              aten::mul         2.34%      25.474ms         3.47%      37.838ms      12.600us       3.247ms         1.41%       3.247ms       1.081us          3003         2.061  \n",
      "                                            aten::empty         5.00%      54.401ms         5.00%      54.401ms       2.885us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.36%       3.932ms         6.31%      68.760ms      16.325us       0.000us         0.00%       1.500ms       0.356us          4212            --  \n",
      "                                         aten::_to_copy         0.75%       8.153ms         5.95%      64.827ms      23.111us       0.000us         0.00%       1.500ms       0.535us          2805            --  \n",
      "                                    aten::empty_strided         1.40%      15.244ms         1.40%      15.244ms       5.071us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.46%      15.875ms         4.93%      53.662ms      14.646us       2.415ms         1.05%       2.415ms       0.659us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.089s\n",
      "Self CUDA time total: 230.621ms\n",
      "\n",
      "output: tensor([[    2,  8275,    52,    33,    10,  7654,  9061,     7,  5393,   980,\n",
      "             6,    50,   197,    52,  1056,    15, 15582,  3875,    18,  1272,\n",
      "            78,   116, 50118, 50118,   100,   437,    45,   686,    99,    47,\n",
      "          1266,    30,    22, 23242,  5137,   980,   845, 50118, 50118,   100,\n",
      "           437,    45,   686,    99,    47,  1266,    30,    22, 23242,  5137,\n",
      "           980,   845, 50118, 50118,   100,   437,    45,   686,    99,    47,\n",
      "          1266,    30,    22, 23242,  5137,   980,   845, 50118, 50118,   100,\n",
      "           437,    45,   686,    99,    47,  1266,    30,    22, 23242,  5137,\n",
      "           980,   845, 50118, 50118,   100,   437,    45,   686,    99,    47,\n",
      "          1266,    30,    22, 23242,  5137,   980,   845, 50118, 50118,   100,\n",
      "           437,    45,   686,    99,    47,  1266,    30,    22, 23242,  5137,\n",
      "           980,   845, 50118, 50118,   100,   437,    45,   686,    99,    47,\n",
      "          1266,    30,    22, 23242,  5137,   980,   845, 50118, 50118,   100,\n",
      "           437,    45,   686,    99,    47,  1266,    30,    22, 23242,  5137,\n",
      "           980,   845, 50118, 50118,   100,   437,    45,   686,    99,    47,\n",
      "          1266,    30,    22, 23242,  5137,   980,   845, 50118, 50118,   100,\n",
      "           437,    45,   686,    99,    47,  1266,    30,    22, 23242,  5137,\n",
      "           980,   845, 50118, 50118,   100,   437,    45,   686,    99,    47,\n",
      "          1266,    30,    22, 23242,  5137,   980,   845, 50118, 50118,   100,\n",
      "           437,    45,   686,    99,    47,  1266,    30,    22, 23242,  5137,\n",
      "           980,   845, 50118, 50118,   100,   437,    45,   686,    99,    47,\n",
      "          1266,    30,    22, 23242,  5137,   980,   845, 50118, 50118,   100,\n",
      "           437,    45]], device='cuda:0')\n",
      "text_energy_per_token: [9.402694862599631, 9.447853774532112]\n",
      "output_tokens: 222\n",
      "flop: 55528947426\n",
      "energy_consumed:  2097.423537946129\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.79%     174.698ms        21.52%     238.127ms      16.537us     122.223ms        52.99%     122.223ms       8.488us         14400     37710.987  \n",
      "                                               aten::mm         0.38%       4.172ms         0.49%       5.396ms      26.982us      44.917ms        19.47%      44.917ms     224.585us           200     17142.350  \n",
      "                                              aten::bmm         5.26%      58.245ms         7.09%      78.483ms      16.351us       8.327ms         3.61%       8.327ms       1.735us          4800       921.821  \n",
      "                                              aten::add         6.00%      66.373ms         8.96%      99.150ms      12.391us       9.759ms         4.23%       9.759ms       1.220us          8002         7.864  \n",
      "                                              aten::mul         2.31%      25.539ms         3.43%      37.918ms      12.627us       3.245ms         1.41%       3.245ms       1.081us          3003         2.071  \n",
      "                                            aten::empty         4.83%      53.427ms         4.83%      53.427ms       2.833us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.895ms         6.15%      68.089ms      16.166us       0.000us         0.00%       1.496ms       0.355us          4212            --  \n",
      "                                         aten::_to_copy         0.75%       8.300ms         5.80%      64.194ms      22.886us       0.000us         0.00%       1.496ms       0.533us          2805            --  \n",
      "                                    aten::empty_strided         1.29%      14.299ms         1.29%      14.299ms       4.757us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.44%      15.965ms         4.75%      52.561ms      14.345us       2.410ms         1.04%       2.410ms       0.658us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.106s\n",
      "Self CUDA time total: 230.643ms\n",
      "\n",
      "output: tensor([[    2,  1121,    10,   232,   147, 11767,    16,  1959,  3150, 18689,\n",
      "             6,    16,    24,    55,   505,     7, 21790,   633,  5012,    50,\n",
      "          9874,  2017,   116, 50118, 50118,   133,  1948,    16,  4420,     4,\n",
      "         50118, 50118,   133,   864,    16,    35,    16,    24,    55,   505,\n",
      "             7, 21790,   633,  5012,    50,  9874,  2017,   116, 50118, 50118,\n",
      "           133,  1948,    16,  4420,     4, 50118, 50118,   133,   864,    16,\n",
      "            35,    16,    24,    55,   505,     7, 21790,   633,  5012,    50,\n",
      "          9874,  2017,   116, 50118, 50118,   133,  1948,    16,  4420,     4,\n",
      "         50118, 50118,   133,   864,    16,    35,    16,    24,    55,   505,\n",
      "             7, 21790,   633,  5012,    50,  9874,  2017,   116, 50118, 50118,\n",
      "           133,  1948,    16,  4420,     4, 50118, 50118,   133,   864,    16,\n",
      "            35,    16,    24,    55,   505,     7, 21790,   633,  5012,    50,\n",
      "          9874,  2017,   116, 50118, 50118,   133,  1948,    16,  4420,     4,\n",
      "         50118, 50118,   133,   864,    16,    35,    16,    24,    55,   505,\n",
      "             7, 21790,   633,  5012,    50,  9874,  2017,   116, 50118, 50118,\n",
      "           133,  1948,    16,  4420,     4, 50118, 50118,   133,   864,    16,\n",
      "            35,    16,    24,    55,   505,     7, 21790,   633,  5012,    50,\n",
      "          9874,  2017,   116, 50118, 50118,   133,  1948,    16,  4420,     4,\n",
      "         50118, 50118,   133,   864,    16,    35,    16,    24,    55,   505,\n",
      "             7, 21790,   633,  5012,    50,  9874,  2017,   116, 50118, 50118,\n",
      "           133,  1948,    16,  4420,     4, 50118, 50118,   133,   864,    16,\n",
      "            35,    16,    24,    55,   505,     7, 21790,   633,  5012,    50,\n",
      "          9874,  2017,   116]], device='cuda:0')\n",
      "text_energy_per_token: [9.174475227424594]\n",
      "output_tokens: 223\n",
      "flop: 55785093103\n",
      "energy_consumed:  2045.9079757156846\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.87%     178.649ms        21.67%     243.987ms      16.944us     122.266ms        53.00%     122.266ms       8.491us         14400     37710.987  \n",
      "                                               aten::mm         0.25%       2.802ms         0.36%       4.078ms      20.389us      44.922ms        19.47%      44.922ms     224.612us           200     17142.350  \n",
      "                                              aten::bmm         5.07%      57.125ms         6.88%      77.444ms      16.134us       8.350ms         3.62%       8.350ms       1.740us          4800       921.821  \n",
      "                                              aten::add         5.59%      62.921ms         8.50%      95.729ms      11.963us       9.755ms         4.23%       9.755ms       1.219us          8002         7.864  \n",
      "                                              aten::mul         2.29%      25.744ms         3.40%      38.230ms      12.731us       3.245ms         1.41%       3.245ms       1.080us          3003         2.071  \n",
      "                                            aten::empty         5.32%      59.913ms         5.32%      59.913ms       3.177us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.36%       4.015ms         6.20%      69.817ms      16.576us       0.000us         0.00%       1.502ms       0.357us          4212            --  \n",
      "                                         aten::_to_copy         0.76%       8.609ms         5.84%      65.802ms      23.459us       0.000us         0.00%       1.502ms       0.536us          2805            --  \n",
      "                                    aten::empty_strided         1.38%      15.511ms         1.38%      15.511ms       5.160us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.41%      15.930ms         4.79%      53.931ms      14.719us       2.417ms         1.05%       2.417ms       0.660us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.126s\n",
      "Self CUDA time total: 230.710ms\n",
      "\n",
      "output: tensor([[    2,  1121,    10,   232,   147, 11767,    16,  1959,  3150, 18689,\n",
      "             6,    16,    24,    55,   505,     7, 21790,   633,  5012,    50,\n",
      "          9874,  2017,   116, 50118, 50118,   133,  1948,    16,  4420,     4,\n",
      "         50118, 50118,   133,   864,    16,    35,    16,    24,    55,   505,\n",
      "             7, 21790,   633,  5012,    50,  9874,  2017,   116, 50118, 50118,\n",
      "           133,  1948,    16,  4420,     4, 50118, 50118,   133,   864,    16,\n",
      "            35,    16,    24,    55,   505,     7, 21790,   633,  5012,    50,\n",
      "          9874,  2017,   116, 50118, 50118,   133,  1948,    16,  4420,     4,\n",
      "         50118, 50118,   133,   864,    16,    35,    16,    24,    55,   505,\n",
      "             7, 21790,   633,  5012,    50,  9874,  2017,   116, 50118, 50118,\n",
      "           133,  1948,    16,  4420,     4, 50118, 50118,   133,   864,    16,\n",
      "            35,    16,    24,    55,   505,     7, 21790,   633,  5012,    50,\n",
      "          9874,  2017,   116, 50118, 50118,   133,  1948,    16,  4420,     4,\n",
      "         50118, 50118,   133,   864,    16,    35,    16,    24,    55,   505,\n",
      "             7, 21790,   633,  5012,    50,  9874,  2017,   116, 50118, 50118,\n",
      "           133,  1948,    16,  4420,     4, 50118, 50118,   133,   864,    16,\n",
      "            35,    16,    24,    55,   505,     7, 21790,   633,  5012,    50,\n",
      "          9874,  2017,   116, 50118, 50118,   133,  1948,    16,  4420,     4,\n",
      "         50118, 50118,   133,   864,    16,    35,    16,    24,    55,   505,\n",
      "             7, 21790,   633,  5012,    50,  9874,  2017,   116, 50118, 50118,\n",
      "           133,  1948,    16,  4420,     4, 50118, 50118,   133,   864,    16,\n",
      "            35,    16,    24,    55,   505,     7, 21790,   633,  5012,    50,\n",
      "          9874,  2017,   116]], device='cuda:0')\n",
      "text_energy_per_token: [9.174475227424594, 9.121007144294405]\n",
      "output_tokens: 223\n",
      "flop: 55785093103\n",
      "energy_consumed:  2033.9845931776522\n",
      "Processing category: coding\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.24%     175.993ms        22.14%     239.960ms      16.664us     122.202ms        52.96%     122.202ms       8.486us         14400     38560.334  \n",
      "                                               aten::mm         0.26%       2.806ms         0.37%       4.023ms      20.116us      44.906ms        19.46%      44.906ms     224.529us           200     17528.439  \n",
      "                                              aten::bmm         5.18%      56.106ms         7.01%      75.946ms      15.822us       8.373ms         3.63%       8.373ms       1.744us          4800       967.901  \n",
      "                                              aten::add         5.68%      61.573ms         8.69%      94.170ms      11.768us       9.769ms         4.23%       9.769ms       1.221us          8002         8.140  \n",
      "                                              aten::mul         2.33%      25.261ms         3.46%      37.530ms      12.498us       3.248ms         1.41%       3.248ms       1.082us          3003         2.118  \n",
      "                                            aten::empty         5.22%      56.625ms         5.22%      56.625ms       3.003us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.755ms         6.18%      67.024ms      15.913us       0.000us         0.00%       1.503ms       0.357us          4212            --  \n",
      "                                         aten::_to_copy         0.74%       8.017ms         5.84%      63.269ms      22.556us       0.000us         0.00%       1.503ms       0.536us          2805            --  \n",
      "                                    aten::empty_strided         1.41%      15.255ms         1.41%      15.255ms       5.075us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.46%      15.862ms         4.81%      52.144ms      14.231us       2.420ms         1.05%       2.420ms       0.660us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.084s\n",
      "Self CUDA time total: 230.725ms\n",
      "\n",
      "output: tensor([[    2, 42627,    10,   230, 42964,   586,    14,  7005,    10,  2788,\n",
      "          2870,   516,    30,   516,     8,  3948,     5,   346,     9, 39036,\n",
      "             9,    10,  2167,  2136,    11,     5,  2870,     4, 50118, 50118,\n",
      "         44758,    10,   230, 42964,   586,    14,  7005,    10,  2788,  2870,\n",
      "           516,    30,   516,     8,  3948,     5,   346,     9, 39036,     9,\n",
      "            10,  2167,  2136,    11,     5,  2870,     4, 50118, 50118, 44758,\n",
      "            10,   230, 42964,   586,    14,  7005,    10,  2788,  2870,   516,\n",
      "            30,   516,     8,  3948,     5,   346,     9, 39036,     9,    10,\n",
      "          2167,  2136,    11,     5,  2870,     4, 50118, 50118, 44758,    10,\n",
      "           230, 42964,   586,    14,  7005,    10,  2788,  2870,   516,    30,\n",
      "           516,     8,  3948,     5,   346,     9, 39036,     9,    10,  2167,\n",
      "          2136,    11,     5,  2870,     4, 50118, 50118, 44758,    10,   230,\n",
      "         42964,   586,    14,  7005,    10,  2788,  2870,   516,    30,   516,\n",
      "             8,  3948,     5,   346,     9, 39036,     9,    10,  2167,  2136,\n",
      "            11,     5,  2870,     4, 50118, 50118, 44758,    10,   230, 42964,\n",
      "           586,    14,  7005,    10,  2788,  2870,   516,    30,   516,     8,\n",
      "          3948,     5,   346,     9, 39036,     9,    10,  2167,  2136,    11,\n",
      "             5,  2870,     4, 50118, 50118, 44758,    10,   230, 42964,   586,\n",
      "            14,  7005,    10,  2788,  2870,   516,    30,   516,     8,  3948,\n",
      "             5,   346,     9, 39036,     9,    10,  2167,  2136,    11,     5,\n",
      "          2870,     4, 50118, 50118, 44758,    10,   230, 42964,   586,    14,\n",
      "          7005,    10,  2788,  2870,   516,    30,   516,     8,  3948,     5,\n",
      "           346,     9, 39036,     9,    10,  2167,  2136,    11]],\n",
      "       device='cuda:0')\n",
      "text_energy_per_token: [9.069642663687468]\n",
      "output_tokens: 228\n",
      "flop: 57066931728\n",
      "energy_consumed:  2067.8785273207427\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.13%     180.704ms        21.95%     245.890ms      17.076us     122.219ms        52.97%     122.219ms       8.487us         14400     38560.334  \n",
      "                                               aten::mm         0.25%       2.816ms         0.36%       4.061ms      20.307us      44.902ms        19.46%      44.902ms     224.508us           200     17528.439  \n",
      "                                              aten::bmm         5.07%      56.842ms         6.88%      77.022ms      16.046us       8.375ms         3.63%       8.375ms       1.745us          4800       967.901  \n",
      "                                              aten::add         5.61%      62.853ms         8.54%      95.649ms      11.953us       9.760ms         4.23%       9.760ms       1.220us          8002         8.140  \n",
      "                                              aten::mul         2.29%      25.620ms         3.40%      38.068ms      12.677us       3.246ms         1.41%       3.246ms       1.081us          3003         2.118  \n",
      "                                            aten::empty         5.16%      57.780ms         5.16%      57.780ms       3.064us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.865ms         6.21%      69.533ms      16.508us       0.000us         0.00%       1.500ms       0.356us          4212            --  \n",
      "                                         aten::_to_copy         0.76%       8.511ms         5.86%      65.668ms      23.411us       0.000us         0.00%       1.500ms       0.535us          2805            --  \n",
      "                                    aten::empty_strided         1.34%      15.021ms         1.34%      15.021ms       4.997us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.43%      15.995ms         4.80%      53.753ms      14.671us       2.417ms         1.05%       2.417ms       0.660us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.120s\n",
      "Self CUDA time total: 230.746ms\n",
      "\n",
      "output: tensor([[    2, 42627,    10,   230, 42964,   586,    14,  7005,    10,  2788,\n",
      "          2870,   516,    30,   516,     8,  3948,     5,   346,     9, 39036,\n",
      "             9,    10,  2167,  2136,    11,     5,  2870,     4, 50118, 50118,\n",
      "         44758,    10,   230, 42964,   586,    14,  7005,    10,  2788,  2870,\n",
      "           516,    30,   516,     8,  3948,     5,   346,     9, 39036,     9,\n",
      "            10,  2167,  2136,    11,     5,  2870,     4, 50118, 50118, 44758,\n",
      "            10,   230, 42964,   586,    14,  7005,    10,  2788,  2870,   516,\n",
      "            30,   516,     8,  3948,     5,   346,     9, 39036,     9,    10,\n",
      "          2167,  2136,    11,     5,  2870,     4, 50118, 50118, 44758,    10,\n",
      "           230, 42964,   586,    14,  7005,    10,  2788,  2870,   516,    30,\n",
      "           516,     8,  3948,     5,   346,     9, 39036,     9,    10,  2167,\n",
      "          2136,    11,     5,  2870,     4, 50118, 50118, 44758,    10,   230,\n",
      "         42964,   586,    14,  7005,    10,  2788,  2870,   516,    30,   516,\n",
      "             8,  3948,     5,   346,     9, 39036,     9,    10,  2167,  2136,\n",
      "            11,     5,  2870,     4, 50118, 50118, 44758,    10,   230, 42964,\n",
      "           586,    14,  7005,    10,  2788,  2870,   516,    30,   516,     8,\n",
      "          3948,     5,   346,     9, 39036,     9,    10,  2167,  2136,    11,\n",
      "             5,  2870,     4, 50118, 50118, 44758,    10,   230, 42964,   586,\n",
      "            14,  7005,    10,  2788,  2870,   516,    30,   516,     8,  3948,\n",
      "             5,   346,     9, 39036,     9,    10,  2167,  2136,    11,     5,\n",
      "          2870,     4, 50118, 50118, 44758,    10,   230, 42964,   586,    14,\n",
      "          7005,    10,  2788,  2870,   516,    30,   516,     8,  3948,     5,\n",
      "           346,     9, 39036,     9,    10,  2167,  2136,    11]],\n",
      "       device='cuda:0')\n",
      "text_energy_per_token: [9.069642663687468, 9.213947685791108]\n",
      "output_tokens: 228\n",
      "flop: 57066931728\n",
      "energy_consumed:  2100.7800723603727\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.06%     183.124ms        21.85%     249.118ms      17.300us     122.387ms        53.01%     122.387ms       8.499us         14400     37371.249  \n",
      "                                               aten::mm         0.25%       2.802ms         0.35%       4.045ms      20.227us      44.965ms        19.48%      44.965ms     224.827us           200     16987.914  \n",
      "                                              aten::bmm         5.04%      57.417ms         6.83%      77.853ms      16.219us       8.311ms         3.60%       8.311ms       1.731us          4800       903.905  \n",
      "                                              aten::add         5.55%      63.310ms         8.44%      96.248ms      12.028us       9.763ms         4.23%       9.763ms       1.220us          8002         7.756  \n",
      "                                              aten::mul         2.27%      25.833ms         3.38%      38.530ms      12.830us       3.245ms         1.41%       3.245ms       1.081us          3003         2.052  \n",
      "                                            aten::empty         5.15%      58.746ms         5.15%      58.746ms       3.115us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       4.016ms         6.17%      70.307ms      16.692us       0.000us         0.00%       1.498ms       0.356us          4212            --  \n",
      "                                         aten::_to_copy         0.77%       8.780ms         5.81%      66.292ms      23.633us       0.000us         0.00%       1.498ms       0.534us          2805            --  \n",
      "                                    aten::empty_strided         1.38%      15.690ms         1.38%      15.690ms       5.219us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.40%      15.983ms         4.75%      54.140ms      14.776us       2.412ms         1.04%       2.412ms       0.658us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.140s\n",
      "Self CUDA time total: 230.856ms\n",
      "\n",
      "output: tensor([[    2, 20470, 40224,    10, 31886,  5043,     7,   465,     5,  6463,\n",
      "          1537, 49734,  4086,     9,    80,  8135, 22052,   634,  6878,  8326,\n",
      "             4, 50118, 50118, 44758,    10, 31886,  5043,     7,   465,     5,\n",
      "          6463,  1537, 49734,  4086,     9,    80,  8135, 22052,   634,  6878,\n",
      "          8326,     4, 50118, 50118, 44758,    10, 31886,  5043,     7,   465,\n",
      "             5,  6463,  1537, 49734,  4086,     9,    80,  8135, 22052,   634,\n",
      "          6878,  8326,     4, 50118, 50118, 44758,    10, 31886,  5043,     7,\n",
      "           465,     5,  6463,  1537, 49734,  4086,     9,    80,  8135, 22052,\n",
      "           634,  6878,  8326,     4, 50118, 50118, 44758,    10, 31886,  5043,\n",
      "             7,   465,     5,  6463,  1537, 49734,  4086,     9,    80,  8135,\n",
      "         22052,   634,  6878,  8326,     4, 50118, 50118, 44758,    10, 31886,\n",
      "          5043,     7,   465,     5,  6463,  1537, 49734,  4086,     9,    80,\n",
      "          8135, 22052,   634,  6878,  8326,     4, 50118, 50118, 44758,    10,\n",
      "         31886,  5043,     7,   465,     5,  6463,  1537, 49734,  4086,     9,\n",
      "            80,  8135, 22052,   634,  6878,  8326,     4, 50118, 50118, 44758,\n",
      "            10, 31886,  5043,     7,   465,     5,  6463,  1537, 49734,  4086,\n",
      "             9,    80,  8135, 22052,   634,  6878,  8326,     4, 50118, 50118,\n",
      "         44758,    10, 31886,  5043,     7,   465,     5,  6463,  1537, 49734,\n",
      "          4086,     9,    80,  8135, 22052,   634,  6878,  8326,     4, 50118,\n",
      "         50118, 44758,    10, 31886,  5043,     7,   465,     5,  6463,  1537,\n",
      "         49734,  4086,     9,    80,  8135, 22052,   634,  6878,  8326,     4,\n",
      "         50118, 50118, 44758,    10, 31886,  5043,     7,   465,     5,  6463,\n",
      "          1537]], device='cuda:0')\n",
      "text_energy_per_token: [9.799420747630197]\n",
      "output_tokens: 221\n",
      "flop: 55272875765\n",
      "energy_consumed:  2165.6719852262736\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.26%     177.942ms        22.11%     241.962ms      16.803us     122.344ms        53.01%     122.344ms       8.496us         14400     37371.249  \n",
      "                                               aten::mm         0.26%       2.799ms         0.37%       4.003ms      20.016us      44.943ms        19.47%      44.943ms     224.715us           200     16987.914  \n",
      "                                              aten::bmm         5.15%      56.378ms         6.98%      76.356ms      15.908us       8.308ms         3.60%       8.308ms       1.731us          4800       903.905  \n",
      "                                              aten::add         5.65%      61.789ms         8.63%      94.428ms      11.801us       9.771ms         4.23%       9.771ms       1.221us          8002         7.756  \n",
      "                                              aten::mul         2.32%      25.415ms         3.46%      37.901ms      12.621us       3.247ms         1.41%       3.247ms       1.081us          3003         2.052  \n",
      "                                            aten::empty         5.20%      56.874ms         5.20%      56.874ms       3.016us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.791ms         6.17%      67.544ms      16.036us       0.000us         0.00%       1.499ms       0.356us          4212            --  \n",
      "                                         aten::_to_copy         0.75%       8.234ms         5.83%      63.753ms      22.728us       0.000us         0.00%       1.499ms       0.534us          2805            --  \n",
      "                                    aten::empty_strided         1.40%      15.354ms         1.40%      15.354ms       5.108us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.45%      15.872ms         4.78%      52.338ms      14.285us       2.416ms         1.05%       2.416ms       0.659us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.094s\n",
      "Self CUDA time total: 230.781ms\n",
      "\n",
      "output: tensor([[    2, 20470, 40224,    10, 31886,  5043,     7,   465,     5,  6463,\n",
      "          1537, 49734,  4086,     9,    80,  8135, 22052,   634,  6878,  8326,\n",
      "             4, 50118, 50118, 44758,    10, 31886,  5043,     7,   465,     5,\n",
      "          6463,  1537, 49734,  4086,     9,    80,  8135, 22052,   634,  6878,\n",
      "          8326,     4, 50118, 50118, 44758,    10, 31886,  5043,     7,   465,\n",
      "             5,  6463,  1537, 49734,  4086,     9,    80,  8135, 22052,   634,\n",
      "          6878,  8326,     4, 50118, 50118, 44758,    10, 31886,  5043,     7,\n",
      "           465,     5,  6463,  1537, 49734,  4086,     9,    80,  8135, 22052,\n",
      "           634,  6878,  8326,     4, 50118, 50118, 44758,    10, 31886,  5043,\n",
      "             7,   465,     5,  6463,  1537, 49734,  4086,     9,    80,  8135,\n",
      "         22052,   634,  6878,  8326,     4, 50118, 50118, 44758,    10, 31886,\n",
      "          5043,     7,   465,     5,  6463,  1537, 49734,  4086,     9,    80,\n",
      "          8135, 22052,   634,  6878,  8326,     4, 50118, 50118, 44758,    10,\n",
      "         31886,  5043,     7,   465,     5,  6463,  1537, 49734,  4086,     9,\n",
      "            80,  8135, 22052,   634,  6878,  8326,     4, 50118, 50118, 44758,\n",
      "            10, 31886,  5043,     7,   465,     5,  6463,  1537, 49734,  4086,\n",
      "             9,    80,  8135, 22052,   634,  6878,  8326,     4, 50118, 50118,\n",
      "         44758,    10, 31886,  5043,     7,   465,     5,  6463,  1537, 49734,\n",
      "          4086,     9,    80,  8135, 22052,   634,  6878,  8326,     4, 50118,\n",
      "         50118, 44758,    10, 31886,  5043,     7,   465,     5,  6463,  1537,\n",
      "         49734,  4086,     9,    80,  8135, 22052,   634,  6878,  8326,     4,\n",
      "         50118, 50118, 44758,    10, 31886,  5043,     7,   465,     5,  6463,\n",
      "          1537]], device='cuda:0')\n",
      "text_energy_per_token: [9.799420747630197, 9.173735509780522]\n",
      "output_tokens: 221\n",
      "flop: 55272875765\n",
      "energy_consumed:  2027.3955476614954\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.25%     181.145ms        21.94%     244.586ms      16.985us     122.105ms        53.04%     122.105ms       8.480us         14400     36182.163  \n",
      "                                               aten::mm         0.26%       2.863ms         0.37%       4.091ms      20.456us      44.930ms        19.51%      44.930ms     224.648us           200     16447.390  \n",
      "                                              aten::bmm         5.09%      56.734ms         6.93%      77.283ms      16.101us       8.143ms         3.54%       8.143ms       1.696us          4800       843.522  \n",
      "                                              aten::add         5.74%      64.014ms         8.68%      96.724ms      12.087us       9.753ms         4.24%       9.753ms       1.219us          8002         7.385  \n",
      "                                              aten::mul         2.42%      26.930ms         3.54%      39.417ms      13.126us       3.242ms         1.41%       3.242ms       1.080us          3003         1.986  \n",
      "                                            aten::empty         5.17%      57.604ms         5.17%      57.604ms       3.055us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.34%       3.846ms         6.22%      69.356ms      16.466us       0.000us         0.00%       1.495ms       0.355us          4212            --  \n",
      "                                         aten::_to_copy         0.75%       8.406ms         5.88%      65.511ms      23.355us       0.000us         0.00%       1.495ms       0.533us          2805            --  \n",
      "                                    aten::empty_strided         1.39%      15.467ms         1.39%      15.467ms       5.145us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.43%      15.996ms         4.84%      53.943ms      14.722us       2.409ms         1.05%       2.409ms       0.658us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.115s\n",
      "Self CUDA time total: 230.232ms\n",
      "\n",
      "output: tensor([[    2, 20470, 40224,    10,  1675,  8151,    11, 31886,     7, 28754,\n",
      "            41,  1047,  1100,     4, 50118, 50118,   713,    16,    10,   182,\n",
      "          1537,   936,    19,  1047,  8480,     4, 50118, 50118,   133,    78,\n",
      "           631,     7,   109,    16,     7,  1045,    10,    92,  1047,  1100,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758]], device='cuda:0')\n",
      "text_energy_per_token: [9.152277589913394]\n",
      "output_tokens: 214\n",
      "flop: 53482446586\n",
      "energy_consumed:  1958.5874042414664\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.54%     174.328ms        21.36%     239.550ms      16.635us     122.065ms        53.02%     122.065ms       8.477us         14400     36182.163  \n",
      "                                               aten::mm         0.37%       4.138ms         0.47%       5.317ms      26.586us      44.953ms        19.53%      44.953ms     224.763us           200     16447.390  \n",
      "                                              aten::bmm         5.32%      59.712ms         7.13%      79.984ms      16.663us       8.133ms         3.53%       8.133ms       1.694us          4800       843.522  \n",
      "                                              aten::add         5.95%      66.680ms         8.86%      99.406ms      12.423us       9.761ms         4.24%       9.761ms       1.220us          8002         7.385  \n",
      "                                              aten::mul         2.40%      26.963ms         3.52%      39.471ms      13.144us       3.245ms         1.41%       3.245ms       1.081us          3003         1.986  \n",
      "                                            aten::empty         4.80%      53.780ms         4.80%      53.780ms       2.852us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.36%       3.992ms         6.22%      69.732ms      16.555us       0.000us         0.00%       1.490ms       0.354us          4212            --  \n",
      "                                         aten::_to_copy         0.76%       8.495ms         5.86%      65.740ms      23.437us       0.000us         0.00%       1.490ms       0.531us          2805            --  \n",
      "                                    aten::empty_strided         1.27%      14.270ms         1.27%      14.270ms       4.747us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.42%      15.946ms         4.82%      54.004ms      14.739us       2.404ms         1.04%       2.404ms       0.656us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.121s\n",
      "Self CUDA time total: 230.214ms\n",
      "\n",
      "output: tensor([[    2, 20470, 40224,    10,  1675,  8151,    11, 31886,     7, 28754,\n",
      "            41,  1047,  1100,     4, 50118, 50118,   713,    16,    10,   182,\n",
      "          1537,   936,    19,  1047,  8480,     4, 50118, 50118,   133,    78,\n",
      "           631,     7,   109,    16,     7,  1045,    10,    92,  1047,  1100,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758]], device='cuda:0')\n",
      "text_energy_per_token: [9.152277589913394, 9.803454567245902]\n",
      "output_tokens: 214\n",
      "flop: 53482446586\n",
      "energy_consumed:  2097.939277390623\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.68%     171.343ms        21.55%     235.470ms      16.352us     122.126ms        53.00%     122.126ms       8.481us         14400     36691.771  \n",
      "                                               aten::mm         0.26%       2.848ms         0.37%       4.088ms      20.441us      44.920ms        19.49%      44.920ms     224.600us           200     16679.043  \n",
      "                                              aten::bmm         5.52%      60.276ms         7.35%      80.376ms      16.745us       8.282ms         3.59%       8.282ms       1.726us          4800       868.958  \n",
      "                                              aten::add         5.79%      63.274ms         8.91%      97.339ms      12.164us       9.761ms         4.24%       9.761ms       1.220us          8002         7.542  \n",
      "                                              aten::mul         2.32%      25.383ms         3.45%      37.702ms      12.555us       3.244ms         1.41%       3.244ms       1.080us          3003         2.014  \n",
      "                                            aten::empty         4.83%      52.837ms         4.83%      52.837ms       2.802us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.792ms         6.42%      70.154ms      16.656us       0.000us         0.00%       1.497ms       0.355us          4212            --  \n",
      "                                         aten::_to_copy         0.75%       8.173ms         6.07%      66.362ms      23.659us       0.000us         0.00%       1.497ms       0.534us          2805            --  \n",
      "                                    aten::empty_strided         1.28%      13.975ms         1.28%      13.975ms       4.649us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.57%      17.200ms         5.04%      55.098ms      15.038us       2.412ms         1.05%       2.412ms       0.658us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.093s\n",
      "Self CUDA time total: 230.442ms\n",
      "\n",
      "output: tensor([[    2, 45714,    10,   586,     7,   465,     5,   295,   212, 28174,\n",
      "           261, 29522,   346,   634,  6878,  8326,     4, 50118, 50118, 44758,\n",
      "            10,   586,     7,   465,     5,   295,   212, 28174,   261, 29522,\n",
      "           346,   634,  6878,  8326,     4, 50118, 50118, 44758,    10,   586,\n",
      "             7,   465,     5,   295,   212, 28174,   261, 29522,   346,   634,\n",
      "          6878,  8326,     4, 50118, 50118, 44758,    10,   586,     7,   465,\n",
      "             5,   295,   212, 28174,   261, 29522,   346,   634,  6878,  8326,\n",
      "             4, 50118, 50118, 44758,    10,   586,     7,   465,     5,   295,\n",
      "           212, 28174,   261, 29522,   346,   634,  6878,  8326,     4, 50118,\n",
      "         50118, 44758,    10,   586,     7,   465,     5,   295,   212, 28174,\n",
      "           261, 29522,   346,   634,  6878,  8326,     4, 50118, 50118, 44758,\n",
      "            10,   586,     7,   465,     5,   295,   212, 28174,   261, 29522,\n",
      "           346,   634,  6878,  8326,     4, 50118, 50118, 44758,    10,   586,\n",
      "             7,   465,     5,   295,   212, 28174,   261, 29522,   346,   634,\n",
      "          6878,  8326,     4, 50118, 50118, 44758,    10,   586,     7,   465,\n",
      "             5,   295,   212, 28174,   261, 29522,   346,   634,  6878,  8326,\n",
      "             4, 50118, 50118, 44758,    10,   586,     7,   465,     5,   295,\n",
      "           212, 28174,   261, 29522,   346,   634,  6878,  8326,     4, 50118,\n",
      "         50118, 44758,    10,   586,     7,   465,     5,   295,   212, 28174,\n",
      "           261, 29522,   346,   634,  6878,  8326,     4, 50118, 50118, 44758,\n",
      "            10,   586,     7,   465,     5,   295,   212, 28174,   261, 29522,\n",
      "           346,   634,  6878,  8326,     4, 50118, 50118]], device='cuda:0')\n",
      "text_energy_per_token: [9.315637681095502]\n",
      "output_tokens: 217\n",
      "flop: 54249329281\n",
      "energy_consumed:  2021.493376797724\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.60%     175.741ms        21.74%     244.892ms      17.006us     122.053ms        52.98%     122.053ms       8.476us         14400     36691.771  \n",
      "                                               aten::mm         0.25%       2.815ms         0.36%       4.056ms      20.279us      44.909ms        19.49%      44.909ms     224.544us           200     16679.043  \n",
      "                                              aten::bmm         5.19%      58.448ms         7.00%      78.869ms      16.431us       8.278ms         3.59%       8.278ms       1.725us          4800       868.958  \n",
      "                                              aten::add         5.58%      62.866ms         8.61%      97.036ms      12.126us       9.760ms         4.24%       9.760ms       1.220us          8002         7.542  \n",
      "                                              aten::mul         2.28%      25.731ms         3.38%      38.123ms      12.695us       3.246ms         1.41%       3.246ms       1.081us          3003         2.014  \n",
      "                                            aten::empty         4.92%      55.419ms         4.92%      55.419ms       2.939us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.941ms         6.24%      70.276ms      16.685us       0.000us         0.00%       1.498ms       0.356us          4212            --  \n",
      "                                         aten::_to_copy         0.76%       8.607ms         5.89%      66.335ms      23.649us       0.000us         0.00%       1.498ms       0.534us          2805            --  \n",
      "                                    aten::empty_strided         1.29%      14.519ms         1.29%      14.519ms       4.830us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.42%      16.040ms         4.81%      54.207ms      14.795us       2.413ms         1.05%       2.413ms       0.658us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.126s\n",
      "Self CUDA time total: 230.369ms\n",
      "\n",
      "output: tensor([[    2, 45714,    10,   586,     7,   465,     5,   295,   212, 28174,\n",
      "           261, 29522,   346,   634,  6878,  8326,     4, 50118, 50118, 44758,\n",
      "            10,   586,     7,   465,     5,   295,   212, 28174,   261, 29522,\n",
      "           346,   634,  6878,  8326,     4, 50118, 50118, 44758,    10,   586,\n",
      "             7,   465,     5,   295,   212, 28174,   261, 29522,   346,   634,\n",
      "          6878,  8326,     4, 50118, 50118, 44758,    10,   586,     7,   465,\n",
      "             5,   295,   212, 28174,   261, 29522,   346,   634,  6878,  8326,\n",
      "             4, 50118, 50118, 44758,    10,   586,     7,   465,     5,   295,\n",
      "           212, 28174,   261, 29522,   346,   634,  6878,  8326,     4, 50118,\n",
      "         50118, 44758,    10,   586,     7,   465,     5,   295,   212, 28174,\n",
      "           261, 29522,   346,   634,  6878,  8326,     4, 50118, 50118, 44758,\n",
      "            10,   586,     7,   465,     5,   295,   212, 28174,   261, 29522,\n",
      "           346,   634,  6878,  8326,     4, 50118, 50118, 44758,    10,   586,\n",
      "             7,   465,     5,   295,   212, 28174,   261, 29522,   346,   634,\n",
      "          6878,  8326,     4, 50118, 50118, 44758,    10,   586,     7,   465,\n",
      "             5,   295,   212, 28174,   261, 29522,   346,   634,  6878,  8326,\n",
      "             4, 50118, 50118, 44758,    10,   586,     7,   465,     5,   295,\n",
      "           212, 28174,   261, 29522,   346,   634,  6878,  8326,     4, 50118,\n",
      "         50118, 44758,    10,   586,     7,   465,     5,   295,   212, 28174,\n",
      "           261, 29522,   346,   634,  6878,  8326,     4, 50118, 50118, 44758,\n",
      "            10,   586,     7,   465,     5,   295,   212, 28174,   261, 29522,\n",
      "           346,   634,  6878,  8326,     4, 50118, 50118]], device='cuda:0')\n",
      "text_energy_per_token: [9.315637681095502, 9.742478944642754]\n",
      "output_tokens: 217\n",
      "flop: 54249329281\n",
      "energy_consumed:  2114.1179309874774\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.60%     177.336ms        21.74%     247.063ms      17.157us     122.152ms        52.99%     122.152ms       8.483us         14400     36691.771  \n",
      "                                               aten::mm         0.25%       2.815ms         0.36%       4.047ms      20.235us      44.927ms        19.49%      44.927ms     224.637us           200     16679.043  \n",
      "                                              aten::bmm         5.17%      58.810ms         6.97%      79.229ms      16.506us       8.284ms         3.59%       8.284ms       1.726us          4800       868.958  \n",
      "                                              aten::add         5.58%      63.414ms         8.60%      97.727ms      12.213us       9.764ms         4.24%       9.764ms       1.220us          8002         7.542  \n",
      "                                              aten::mul         2.27%      25.791ms         3.36%      38.192ms      12.718us       3.250ms         1.41%       3.250ms       1.082us          3003         2.014  \n",
      "                                            aten::empty         4.91%      55.788ms         4.91%      55.788ms       2.958us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.938ms         6.20%      70.454ms      16.727us       0.000us         0.00%       1.505ms       0.357us          4212            --  \n",
      "                                         aten::_to_copy         0.77%       8.788ms         5.85%      66.516ms      23.713us       0.000us         0.00%       1.505ms       0.537us          2805            --  \n",
      "                                    aten::empty_strided         1.26%      14.346ms         1.26%      14.346ms       4.772us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.41%      16.075ms         4.78%      54.349ms      14.833us       2.421ms         1.05%       2.421ms       0.661us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.136s\n",
      "Self CUDA time total: 230.524ms\n",
      "\n",
      "output: tensor([[    2, 20470, 40224,    10, 32771,  1707, 17194,     7,   465,    10,\n",
      "          2167,  7510,    11,    10, 24713,  8932,     4, 50118, 50118,   713,\n",
      "            16,    10,   182,  2007,     8,  1365,   169,     7,   465,    10,\n",
      "          1989,  7510,    11,    10, 24713,  8932,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4]], device='cuda:0')\n",
      "text_energy_per_token: [9.731287730321576]\n",
      "output_tokens: 217\n",
      "flop: 54249329281\n",
      "energy_consumed:  2111.689437479782\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.70%     172.706ms        21.82%     239.984ms      16.666us     122.210ms        53.00%     122.210ms       8.487us         14400     36691.771  \n",
      "                                               aten::mm         0.25%       2.803ms         0.37%       4.035ms      20.177us      44.915ms        19.48%      44.915ms     224.574us           200     16679.043  \n",
      "                                              aten::bmm         5.25%      57.742ms         7.07%      77.755ms      16.199us       8.285ms         3.59%       8.285ms       1.726us          4800       868.958  \n",
      "                                              aten::add         5.66%      62.301ms         8.75%      96.228ms      12.026us       9.766ms         4.24%       9.766ms       1.220us          8002         7.542  \n",
      "                                              aten::mul         2.32%      25.520ms         3.44%      37.858ms      12.607us       3.245ms         1.41%       3.245ms       1.081us          3003         2.014  \n",
      "                                            aten::empty         4.95%      54.413ms         4.95%      54.413ms       2.885us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.836ms         6.17%      67.862ms      16.112us       0.000us         0.00%       1.503ms       0.357us          4212            --  \n",
      "                                         aten::_to_copy         0.76%       8.335ms         5.82%      64.026ms      22.826us       0.000us         0.00%       1.503ms       0.536us          2805            --  \n",
      "                                    aten::empty_strided         1.28%      14.044ms         1.28%      14.044ms       4.672us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.45%      15.909ms         4.78%      52.588ms      14.353us       2.419ms         1.05%       2.419ms       0.660us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.100s\n",
      "Self CUDA time total: 230.597ms\n",
      "\n",
      "output: tensor([[    2, 20470, 40224,    10, 32771,  1707, 17194,     7,   465,    10,\n",
      "          2167,  7510,    11,    10, 24713,  8932,     4, 50118, 50118,   713,\n",
      "            16,    10,   182,  2007,     8,  1365,   169,     7,   465,    10,\n",
      "          1989,  7510,    11,    10, 24713,  8932,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4]], device='cuda:0')\n",
      "text_energy_per_token: [9.731287730321576, 9.83710998038762]\n",
      "output_tokens: 217\n",
      "flop: 54249329281\n",
      "energy_consumed:  2134.6528657441136\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.99%     180.027ms        21.66%     243.847ms      16.934us     122.053ms        53.02%     122.053ms       8.476us         14400     36012.294  \n",
      "                                               aten::mm         0.25%       2.855ms         0.36%       4.093ms      20.464us      44.933ms        19.52%      44.933ms     224.664us           200     16370.172  \n",
      "                                              aten::bmm         5.07%      57.095ms         6.87%      77.371ms      16.119us       8.131ms         3.53%       8.131ms       1.694us          4800       835.191  \n",
      "                                              aten::add         5.71%      64.282ms         8.63%      97.157ms      12.142us       9.755ms         4.24%       9.755ms       1.219us          8002         7.334  \n",
      "                                              aten::mul         2.40%      27.039ms         3.52%      39.631ms      13.197us       3.245ms         1.41%       3.245ms       1.081us          3003         1.977  \n",
      "                                            aten::empty         5.15%      57.948ms         5.15%      57.948ms       3.073us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.947ms         6.22%      70.046ms      16.630us       0.000us         0.00%       1.497ms       0.356us          4212            --  \n",
      "                                         aten::_to_copy         0.75%       8.488ms         5.87%      66.099ms      23.565us       0.000us         0.00%       1.497ms       0.534us          2805            --  \n",
      "                                    aten::empty_strided         1.41%      15.834ms         1.41%      15.834ms       5.267us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.42%      15.993ms         4.80%      54.090ms      14.763us       2.412ms         1.05%       2.412ms       0.658us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.126s\n",
      "Self CUDA time total: 230.198ms\n",
      "\n",
      "output: tensor([[    2, 20470, 40224,    10, 21021,   414,  3184,   634,    80, 32201,\n",
      "            11, 31886,     4, 50118, 50118,   100,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414]], device='cuda:0')\n",
      "text_energy_per_token: [9.46018960715952]\n",
      "output_tokens: 213\n",
      "flop: 53226967053\n",
      "energy_consumed:  2015.020386324978\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.61%     174.230ms        21.41%     238.962ms      16.595us     122.033ms        53.04%     122.033ms       8.475us         14400     36012.294  \n",
      "                                               aten::mm         0.37%       4.133ms         0.48%       5.339ms      26.697us      44.905ms        19.52%      44.905ms     224.524us           200     16370.172  \n",
      "                                              aten::bmm         5.35%      59.684ms         7.16%      79.923ms      16.651us       8.134ms         3.54%       8.134ms       1.695us          4800       835.191  \n",
      "                                              aten::add         5.87%      65.563ms         8.82%      98.425ms      12.300us       9.745ms         4.24%       9.745ms       1.218us          8002         7.334  \n",
      "                                              aten::mul         2.32%      25.866ms         3.44%      38.441ms      12.801us       3.243ms         1.41%       3.243ms       1.080us          3003         1.977  \n",
      "                                            aten::empty         4.82%      53.806ms         4.82%      53.806ms       2.853us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.940ms         6.26%      69.839ms      16.581us       0.000us         0.00%       1.482ms       0.352us          4212            --  \n",
      "                                         aten::_to_copy         0.76%       8.481ms         5.90%      65.899ms      23.493us       0.000us         0.00%       1.482ms       0.528us          2805            --  \n",
      "                                    aten::empty_strided         1.27%      14.140ms         1.27%      14.140ms       4.704us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.43%      15.930ms         4.86%      54.249ms      14.806us       2.392ms         1.04%       2.392ms       0.653us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.116s\n",
      "Self CUDA time total: 230.064ms\n",
      "\n",
      "output: tensor([[    2, 20470, 40224,    10, 21021,   414,  3184,   634,    80, 32201,\n",
      "            11, 31886,     4, 50118, 50118,   100,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414,  3184,     4,    38,    33,    10,   936,    19,\n",
      "             5, 21021,   414]], device='cuda:0')\n",
      "text_energy_per_token: [9.46018960715952, 9.178588360875427]\n",
      "output_tokens: 213\n",
      "flop: 53226967053\n",
      "energy_consumed:  1955.039320866466\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.37%     178.609ms        22.24%     242.615ms      16.848us     122.206ms        53.00%     122.206ms       8.486us         14400     37201.379  \n",
      "                                               aten::mm         0.26%       2.805ms         0.37%       4.053ms      20.267us      44.915ms        19.48%      44.915ms     224.575us           200     16910.696  \n",
      "                                              aten::bmm         5.15%      56.170ms         6.97%      76.064ms      15.847us       8.310ms         3.60%       8.310ms       1.731us          4800       895.058  \n",
      "                                              aten::add         5.66%      61.785ms         8.65%      94.371ms      11.793us       9.759ms         4.23%       9.759ms       1.220us          8002         7.702  \n",
      "                                              aten::mul         2.32%      25.340ms         3.46%      37.788ms      12.584us       3.246ms         1.41%       3.246ms       1.081us          3003         2.043  \n",
      "                                            aten::empty         5.01%      54.657ms         5.01%      54.657ms       2.898us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.822ms         6.17%      67.320ms      15.983us       0.000us         0.00%       1.503ms       0.357us          4212            --  \n",
      "                                         aten::_to_copy         0.74%       8.058ms         5.82%      63.498ms      22.637us       0.000us         0.00%       1.503ms       0.536us          2805            --  \n",
      "                                    aten::empty_strided         1.40%      15.263ms         1.40%      15.263ms       5.078us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.46%      15.883ms         4.80%      52.406ms      14.303us       2.418ms         1.05%       2.418ms       0.660us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.091s\n",
      "Self CUDA time total: 230.576ms\n",
      "\n",
      "output: tensor([[    2, 20470, 40224,    10,   586,     7,   465,     5,  1537,  4785,\n",
      "            11,    80, 42156,   396,   634,   143,  1823,   414,  6609,     4,\n",
      "         50118, 50118, 44758,    10,    92,  8932,    19,     5,   276,   766,\n",
      "             8,     5,   276,   766,     4, 50118, 50118, 44758,    10,    92,\n",
      "          8932,    19,     5,   276,   766,     8,     5,   276,   766,     4,\n",
      "         50118, 50118, 44758,    10,    92,  8932,    19,     5,   276,   766,\n",
      "             8,     5,   276,   766,     4, 50118, 50118, 44758,    10,    92,\n",
      "          8932,    19,     5,   276,   766,     8,     5,   276,   766,     4,\n",
      "         50118, 50118, 44758,    10,    92,  8932,    19,     5,   276,   766,\n",
      "             8,     5,   276,   766,     4, 50118, 50118, 44758,    10,    92,\n",
      "          8932,    19,     5,   276,   766,     8,     5,   276,   766,     4,\n",
      "         50118, 50118, 44758,    10,    92,  8932,    19,     5,   276,   766,\n",
      "             8,     5,   276,   766,     4, 50118, 50118, 44758,    10,    92,\n",
      "          8932,    19,     5,   276,   766,     8,     5,   276,   766,     4,\n",
      "         50118, 50118, 44758,    10,    92,  8932,    19,     5,   276,   766,\n",
      "             8,     5,   276,   766,     4, 50118, 50118, 44758,    10,    92,\n",
      "          8932,    19,     5,   276,   766,     8,     5,   276,   766,     4,\n",
      "         50118, 50118, 44758,    10,    92,  8932,    19,     5,   276,   766,\n",
      "             8,     5,   276,   766,     4, 50118, 50118, 44758,    10,    92,\n",
      "          8932,    19,     5,   276,   766,     8,     5,   276,   766,     4,\n",
      "         50118, 50118, 44758,    10,    92,  8932,    19,     5,   276,   766,\n",
      "             8,     5,   276,   766,     4, 50118, 50118, 44758,    10,    92]],\n",
      "       device='cuda:0')\n",
      "text_energy_per_token: [9.653923306991512]\n",
      "output_tokens: 220\n",
      "flop: 55016878120\n",
      "energy_consumed:  2123.8631275381326\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.92%     180.196ms        21.73%     245.895ms      17.076us     122.171ms        52.98%     122.171ms       8.484us         14400     37201.379  \n",
      "                                               aten::mm         0.25%       2.821ms         0.36%       4.062ms      20.312us      44.946ms        19.49%      44.946ms     224.729us           200     16910.696  \n",
      "                                              aten::bmm         5.06%      57.260ms         6.86%      77.608ms      16.168us       8.309ms         3.60%       8.309ms       1.731us          4800       895.058  \n",
      "                                              aten::add         5.60%      63.383ms         8.51%      96.301ms      12.035us       9.764ms         4.23%       9.764ms       1.220us          8002         7.702  \n",
      "                                              aten::mul         2.29%      25.967ms         3.41%      38.560ms      12.840us       3.247ms         1.41%       3.247ms       1.081us          3003         2.043  \n",
      "                                            aten::empty         5.16%      58.385ms         5.16%      58.385ms       3.096us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.36%       4.046ms         6.24%      70.625ms      16.767us       0.000us         0.00%       1.504ms       0.357us          4212            --  \n",
      "                                         aten::_to_copy         0.76%       8.656ms         5.88%      66.579ms      23.736us       0.000us         0.00%       1.504ms       0.536us          2805            --  \n",
      "                                    aten::empty_strided         1.41%      15.957ms         1.41%      15.957ms       5.308us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.42%      16.063ms         4.80%      54.323ms      14.826us       2.422ms         1.05%       2.422ms       0.661us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.132s\n",
      "Self CUDA time total: 230.602ms\n",
      "\n",
      "output: tensor([[    2, 20470, 40224,    10,   586,     7,   465,     5,  1537,  4785,\n",
      "            11,    80, 42156,   396,   634,   143,  1823,   414,  6609,     4,\n",
      "         50118, 50118, 44758,    10,    92,  8932,    19,     5,   276,   766,\n",
      "             8,     5,   276,   766,     4, 50118, 50118, 44758,    10,    92,\n",
      "          8932,    19,     5,   276,   766,     8,     5,   276,   766,     4,\n",
      "         50118, 50118, 44758,    10,    92,  8932,    19,     5,   276,   766,\n",
      "             8,     5,   276,   766,     4, 50118, 50118, 44758,    10,    92,\n",
      "          8932,    19,     5,   276,   766,     8,     5,   276,   766,     4,\n",
      "         50118, 50118, 44758,    10,    92,  8932,    19,     5,   276,   766,\n",
      "             8,     5,   276,   766,     4, 50118, 50118, 44758,    10,    92,\n",
      "          8932,    19,     5,   276,   766,     8,     5,   276,   766,     4,\n",
      "         50118, 50118, 44758,    10,    92,  8932,    19,     5,   276,   766,\n",
      "             8,     5,   276,   766,     4, 50118, 50118, 44758,    10,    92,\n",
      "          8932,    19,     5,   276,   766,     8,     5,   276,   766,     4,\n",
      "         50118, 50118, 44758,    10,    92,  8932,    19,     5,   276,   766,\n",
      "             8,     5,   276,   766,     4, 50118, 50118, 44758,    10,    92,\n",
      "          8932,    19,     5,   276,   766,     8,     5,   276,   766,     4,\n",
      "         50118, 50118, 44758,    10,    92,  8932,    19,     5,   276,   766,\n",
      "             8,     5,   276,   766,     4, 50118, 50118, 44758,    10,    92,\n",
      "          8932,    19,     5,   276,   766,     8,     5,   276,   766,     4,\n",
      "         50118, 50118, 44758,    10,    92,  8932,    19,     5,   276,   766,\n",
      "             8,     5,   276,   766,     4, 50118, 50118, 44758,    10,    92]],\n",
      "       device='cuda:0')\n",
      "text_energy_per_token: [9.653923306991512, 9.578274821559516]\n",
      "output_tokens: 220\n",
      "flop: 55016878120\n",
      "energy_consumed:  2107.2204607430936\n",
      "Processing category: math\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.13%     182.327ms        21.84%     246.821ms      17.140us     122.385ms        53.01%     122.385ms       8.499us         14400     38220.595  \n",
      "                                               aten::mm         0.25%       2.812ms         0.36%       4.079ms      20.394us      44.897ms        19.45%      44.897ms     224.486us           200     17374.003  \n",
      "                                              aten::bmm         5.07%      57.306ms         6.87%      77.690ms      16.185us       8.354ms         3.62%       8.354ms       1.740us          4800       949.248  \n",
      "                                              aten::add         5.59%      63.217ms         8.49%      95.986ms      11.995us       9.765ms         4.23%       9.765ms       1.220us          8002         8.029  \n",
      "                                              aten::mul         2.29%      25.885ms         3.40%      38.419ms      12.794us       3.245ms         1.41%       3.245ms       1.080us          3003         2.099  \n",
      "                                            aten::empty         5.17%      58.463ms         5.17%      58.463ms       3.100us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.954ms         6.19%      70.015ms      16.623us       0.000us         0.00%       1.500ms       0.356us          4212            --  \n",
      "                                         aten::_to_copy         0.76%       8.575ms         5.85%      66.061ms      23.551us       0.000us         0.00%       1.500ms       0.535us          2805            --  \n",
      "                                    aten::empty_strided         1.29%      14.524ms         1.29%      14.524ms       4.832us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.42%      16.015ms         4.78%      54.054ms      14.753us       2.416ms         1.05%       2.416ms       0.659us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.130s\n",
      "Self CUDA time total: 230.870ms\n",
      "\n",
      "output: tensor([[    2, 18377,    14,   856,  1640,  1178,    43,  5457,   195,  1178,\n",
      "         35227,   246,   111,   132,  1178,  2055,   155,     6,   465,     5,\n",
      "           923,     9,   856,  1640,   176,   322, 50118, 50118,  1640,  1178,\n",
      "            43,  5457,   195,  1178, 35227,   246,  2055,   132,  1178,  2055,\n",
      "           155, 50118, 50118,  1640,  1178,    43,  5457,   195,  1178, 35227,\n",
      "           246,  2055,   132,  1178,  2055,   155, 50118, 50118,  1640,  1178,\n",
      "            43,  5457,   195,  1178, 35227,   246,  2055,   132,  1178,  2055,\n",
      "           155, 50118, 50118,  1640,  1178,    43,  5457,   195,  1178, 35227,\n",
      "           246,  2055,   132,  1178,  2055,   155, 50118, 50118,  1640,  1178,\n",
      "            43,  5457,   195,  1178, 35227,   246,  2055,   132,  1178,  2055,\n",
      "           155, 50118, 50118,  1640,  1178,    43,  5457,   195,  1178, 35227,\n",
      "           246,  2055,   132,  1178,  2055,   155, 50118, 50118,  1640,  1178,\n",
      "            43,  5457,   195,  1178, 35227,   246,  2055,   132,  1178,  2055,\n",
      "           155, 50118, 50118,  1640,  1178,    43,  5457,   195,  1178, 35227,\n",
      "           246,  2055,   132,  1178,  2055,   155, 50118, 50118,  1640,  1178,\n",
      "            43,  5457,   195,  1178, 35227,   246,  2055,   132,  1178,  2055,\n",
      "           155, 50118, 50118,  1640,  1178,    43,  5457,   195,  1178, 35227,\n",
      "           246,  2055,   132,  1178,  2055,   155, 50118, 50118,  1640,  1178,\n",
      "            43,  5457,   195,  1178, 35227,   246,  2055,   132,  1178,  2055,\n",
      "           155, 50118, 50118,  1640,  1178,    43,  5457,   195,  1178, 35227,\n",
      "           246,  2055,   132,  1178,  2055,   155, 50118, 50118,  1640,  1178,\n",
      "            43,  5457,   195,  1178, 35227,   246,  2055,   132,  1178,  2055,\n",
      "           155, 50118, 50118,  1640,  1178,    43]], device='cuda:0')\n",
      "text_energy_per_token: [9.36969823276469]\n",
      "output_tokens: 226\n",
      "flop: 56553974230\n",
      "energy_consumed:  2117.55180060482\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.10%     177.006ms        21.97%     241.566ms      16.775us     122.239ms        52.97%     122.239ms       8.489us         14400     38220.595  \n",
      "                                               aten::mm         0.26%       2.807ms         0.37%       4.032ms      20.159us      44.923ms        19.47%      44.923ms     224.613us           200     17374.003  \n",
      "                                              aten::bmm         5.13%      56.376ms         6.95%      76.398ms      15.916us       8.353ms         3.62%       8.353ms       1.740us          4800       949.248  \n",
      "                                              aten::add         5.66%      62.248ms         8.63%      94.862ms      11.855us       9.774ms         4.24%       9.774ms       1.221us          8002         8.029  \n",
      "                                              aten::mul         2.32%      25.498ms         3.44%      37.774ms      12.579us       3.247ms         1.41%       3.247ms       1.081us          3003         2.099  \n",
      "                                            aten::empty         5.22%      57.388ms         5.22%      57.388ms       3.043us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.837ms         6.15%      67.630ms      16.057us       0.000us         0.00%       1.501ms       0.356us          4212            --  \n",
      "                                         aten::_to_copy         0.75%       8.222ms         5.80%      63.793ms      22.743us       0.000us         0.00%       1.501ms       0.535us          2805            --  \n",
      "                                    aten::empty_strided         1.40%      15.424ms         1.40%      15.424ms       5.131us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.44%      15.871ms         4.76%      52.385ms      14.297us       2.418ms         1.05%       2.418ms       0.660us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.100s\n",
      "Self CUDA time total: 230.774ms\n",
      "\n",
      "output: tensor([[    2, 18377,    14,   856,  1640,  1178,    43,  5457,   195,  1178,\n",
      "         35227,   246,   111,   132,  1178,  2055,   155,     6,   465,     5,\n",
      "           923,     9,   856,  1640,   176,   322, 50118, 50118,  1640,  1178,\n",
      "            43,  5457,   195,  1178, 35227,   246,  2055,   132,  1178,  2055,\n",
      "           155, 50118, 50118,  1640,  1178,    43,  5457,   195,  1178, 35227,\n",
      "           246,  2055,   132,  1178,  2055,   155, 50118, 50118,  1640,  1178,\n",
      "            43,  5457,   195,  1178, 35227,   246,  2055,   132,  1178,  2055,\n",
      "           155, 50118, 50118,  1640,  1178,    43,  5457,   195,  1178, 35227,\n",
      "           246,  2055,   132,  1178,  2055,   155, 50118, 50118,  1640,  1178,\n",
      "            43,  5457,   195,  1178, 35227,   246,  2055,   132,  1178,  2055,\n",
      "           155, 50118, 50118,  1640,  1178,    43,  5457,   195,  1178, 35227,\n",
      "           246,  2055,   132,  1178,  2055,   155, 50118, 50118,  1640,  1178,\n",
      "            43,  5457,   195,  1178, 35227,   246,  2055,   132,  1178,  2055,\n",
      "           155, 50118, 50118,  1640,  1178,    43,  5457,   195,  1178, 35227,\n",
      "           246,  2055,   132,  1178,  2055,   155, 50118, 50118,  1640,  1178,\n",
      "            43,  5457,   195,  1178, 35227,   246,  2055,   132,  1178,  2055,\n",
      "           155, 50118, 50118,  1640,  1178,    43,  5457,   195,  1178, 35227,\n",
      "           246,  2055,   132,  1178,  2055,   155, 50118, 50118,  1640,  1178,\n",
      "            43,  5457,   195,  1178, 35227,   246,  2055,   132,  1178,  2055,\n",
      "           155, 50118, 50118,  1640,  1178,    43,  5457,   195,  1178, 35227,\n",
      "           246,  2055,   132,  1178,  2055,   155, 50118, 50118,  1640,  1178,\n",
      "            43,  5457,   195,  1178, 35227,   246,  2055,   132,  1178,  2055,\n",
      "           155, 50118, 50118,  1640,  1178,    43]], device='cuda:0')\n",
      "text_energy_per_token: [9.36969823276469, 9.175488579528205]\n",
      "output_tokens: 226\n",
      "flop: 56553974230\n",
      "energy_consumed:  2073.6604189733744\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.07%     180.282ms        21.88%     245.455ms      17.046us     122.203ms        53.00%     122.203ms       8.486us         14400     37031.510  \n",
      "                                               aten::mm         0.25%       2.821ms         0.36%       4.046ms      20.228us      44.919ms        19.48%      44.919ms     224.595us           200     16833.479  \n",
      "                                              aten::bmm         5.09%      57.091ms         6.89%      77.327ms      16.110us       8.303ms         3.60%       8.303ms       1.730us          4800       886.284  \n",
      "                                              aten::add         5.61%      62.917ms         8.53%      95.693ms      11.959us       9.761ms         4.23%       9.761ms       1.220us          8002         7.648  \n",
      "                                              aten::mul         2.29%      25.713ms         3.41%      38.244ms      12.735us       3.246ms         1.41%       3.246ms       1.081us          3003         2.033  \n",
      "                                            aten::empty         5.17%      58.001ms         5.17%      58.001ms       3.076us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.919ms         6.21%      69.643ms      16.534us       0.000us         0.00%       1.497ms       0.355us          4212            --  \n",
      "                                         aten::_to_copy         0.76%       8.516ms         5.86%      65.723ms      23.431us       0.000us         0.00%       1.497ms       0.534us          2805            --  \n",
      "                                    aten::empty_strided         1.38%      15.511ms         1.38%      15.511ms       5.160us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.42%      15.952ms         4.81%      53.972ms      14.730us       2.413ms         1.05%       2.413ms       0.659us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.122s\n",
      "Self CUDA time total: 230.561ms\n",
      "\n",
      "output: tensor([[    2,   104, 18224,    13,  3023,    11,     5, 19587,   155,  1178,\n",
      "          2055,   158,  5457,   195,  1640,  1178,   111,   132,   322, 50118,\n",
      "            12,   246, 50118,   104, 18224,   111,   176,  3226,   119,  5457,\n",
      "           111,   176,  3226,   119,  2055,   204,  3226,   329,   111,   290,\n",
      "             6,   111,   306,  3226,   119,  2055,   204,  3226,   329,  5457,\n",
      "           111,   398,    13,   475,     4, 50118,    12,   176, 50118,   104,\n",
      "         18224,   111,   176,  3226,   642,  2055,   204,  3226,   642,  5457,\n",
      "           111,   306,  3226,   338,  2055,   290,     6,   111,   306,  3226,\n",
      "           642,  5457,   111,   306,  3226,   338,   111,   290,    13,   910,\n",
      "             4, 50118,    12,   176, 50118,   104, 18224,   111,   176,  3226,\n",
      "           282,  2055,   204,  3226,   329,  5457,   111,   288,  3226,   282,\n",
      "          2055,   204,     6,   111,   306,  3226,   282,  2055,   204,  3226,\n",
      "           329,  5457,   111,   306,    13,   295,     4, 50118,   288, 50118,\n",
      "           104, 18224,   111,   176,  3226,   257,  2055,   204,  3226,   257,\n",
      "          5457,   111,   306,  3226,    90,   111,   290,     6,   111,   306,\n",
      "          3226,    90,  5457,   111,   306,  3226,   257,   111,   290,    13,\n",
      "           326,     4, 50118,    12,   176, 50118,   104, 18224,   111,   176,\n",
      "          3226,   282,  2055,   155,  3226,   282,  5457,   111,   245,  3226,\n",
      "           139,   111,   158,     6,   111,   245,  3226,   282,  5457,   111,\n",
      "           245,  3226,   139,   111,   379,    13,   295,     4, 50118,    12,\n",
      "           245, 50118,   104, 18224,   111,   176,  3226,   257,  2055,   155,\n",
      "          3226,  1178,  5457,   111,   288,  3226,  1178,   111,   195]],\n",
      "       device='cuda:0')\n",
      "text_energy_per_token: [9.354718925371236]\n",
      "output_tokens: 219\n",
      "flop: 54760954491\n",
      "energy_consumed:  2048.6834446563007\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.17%     180.682ms        21.99%     245.787ms      17.069us     122.214ms        53.02%     122.214ms       8.487us         14400     37031.510  \n",
      "                                               aten::mm         0.25%       2.822ms         0.36%       4.046ms      20.228us      44.893ms        19.47%      44.893ms     224.467us           200     16833.479  \n",
      "                                              aten::bmm         5.11%      57.099ms         6.92%      77.291ms      16.102us       8.315ms         3.61%       8.315ms       1.732us          4800       886.284  \n",
      "                                              aten::add         5.63%      62.920ms         8.56%      95.695ms      11.959us       9.756ms         4.23%       9.756ms       1.219us          8002         7.648  \n",
      "                                              aten::mul         2.30%      25.735ms         3.43%      38.331ms      12.764us       3.244ms         1.41%       3.244ms       1.080us          3003         2.033  \n",
      "                                            aten::empty         5.07%      56.659ms         5.07%      56.659ms       3.005us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.949ms         6.05%      67.565ms      16.041us       0.000us         0.00%       1.492ms       0.354us          4212            --  \n",
      "                                         aten::_to_copy         0.76%       8.498ms         5.69%      63.615ms      22.679us       0.000us         0.00%       1.492ms       0.532us          2805            --  \n",
      "                                    aten::empty_strided         1.21%      13.543ms         1.21%      13.543ms       4.505us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.43%      15.946ms         4.82%      53.839ms      14.694us       2.407ms         1.04%       2.407ms       0.657us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.118s\n",
      "Self CUDA time total: 230.527ms\n",
      "\n",
      "output: tensor([[    2,   104, 18224,    13,  3023,    11,     5, 19587,   155,  1178,\n",
      "          2055,   158,  5457,   195,  1640,  1178,   111,   132,   322, 50118,\n",
      "            12,   246, 50118,   104, 18224,   111,   176,  3226,   119,  5457,\n",
      "           111,   176,  3226,   119,  2055,   204,  3226,   329,   111,   290,\n",
      "             6,   111,   306,  3226,   119,  2055,   204,  3226,   329,  5457,\n",
      "           111,   398,    13,   475,     4, 50118,    12,   176, 50118,   104,\n",
      "         18224,   111,   176,  3226,   642,  2055,   204,  3226,   642,  5457,\n",
      "           111,   306,  3226,   338,  2055,   290,     6,   111,   306,  3226,\n",
      "           642,  5457,   111,   306,  3226,   338,   111,   290,    13,   910,\n",
      "             4, 50118,    12,   176, 50118,   104, 18224,   111,   176,  3226,\n",
      "           282,  2055,   204,  3226,   329,  5457,   111,   288,  3226,   282,\n",
      "          2055,   204,     6,   111,   306,  3226,   282,  2055,   204,  3226,\n",
      "           329,  5457,   111,   306,    13,   295,     4, 50118,   288, 50118,\n",
      "           104, 18224,   111,   176,  3226,   257,  2055,   204,  3226,   257,\n",
      "          5457,   111,   306,  3226,    90,   111,   290,     6,   111,   306,\n",
      "          3226,    90,  5457,   111,   306,  3226,   257,   111,   290,    13,\n",
      "           326,     4, 50118,    12,   176, 50118,   104, 18224,   111,   176,\n",
      "          3226,   282,  2055,   155,  3226,   282,  5457,   111,   245,  3226,\n",
      "           139,   111,   158,     6,   111,   245,  3226,   282,  5457,   111,\n",
      "           245,  3226,   139,   111,   379,    13,   295,     4, 50118,    12,\n",
      "           245, 50118,   104, 18224,   111,   176,  3226,   257,  2055,   155,\n",
      "          3226,  1178,  5457,   111,   288,  3226,  1178,   111,   195]],\n",
      "       device='cuda:0')\n",
      "text_energy_per_token: [9.354718925371236, 9.28511736420246]\n",
      "output_tokens: 219\n",
      "flop: 54760954491\n",
      "energy_consumed:  2033.4407027603388\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        15.78%     172.813ms        21.51%     235.656ms      16.365us     122.359ms        52.98%     122.359ms       8.497us         14400     38900.072  \n",
      "                                               aten::mm         0.38%       4.181ms         0.49%       5.409ms      27.046us      44.902ms        19.44%      44.902ms     224.509us           200     17682.874  \n",
      "                                              aten::bmm         5.40%      59.097ms         7.23%      79.200ms      16.500us       8.405ms         3.64%       8.405ms       1.751us          4800       986.849  \n",
      "                                              aten::add         6.04%      66.208ms         9.02%      98.764ms      12.342us       9.766ms         4.23%       9.766ms       1.220us          8002         8.252  \n",
      "                                              aten::mul         2.32%      25.374ms         3.44%      37.729ms      12.564us       3.245ms         1.41%       3.245ms       1.081us          3003         2.137  \n",
      "                                            aten::empty         4.84%      52.973ms         4.84%      52.973ms       2.809us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       3.798ms         6.14%      67.284ms      15.974us       0.000us         0.00%       1.507ms       0.358us          4212            --  \n",
      "                                         aten::_to_copy         0.75%       8.201ms         5.80%      63.486ms      22.633us       0.000us         0.00%       1.507ms       0.537us          2805            --  \n",
      "                                    aten::empty_strided         1.28%      14.002ms         1.28%      14.002ms       4.658us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.45%      15.923ms         4.76%      52.149ms      14.233us       2.422ms         1.05%       2.422ms       0.661us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.095s\n",
      "Self CUDA time total: 230.963ms\n",
      "\n",
      "output: tensor([[    2,  1106,     5,   253, 21996,     9,    10,   516,  2835,    32,\n",
      "            36,   176,     6,   111,   176,    43,     8,    36,   698,     6,\n",
      "           204,   238,    99,    16,     5,  5933,     9,     5,  2835,   116,\n",
      "         50118, 50118,   133,  5933,     9,     5,  2835,    16,     5,  5933,\n",
      "             9,     5,  2835,     4, 50118, 50118,   133,  5933,     9,     5,\n",
      "          2835,    16,     5,  5933,     9,     5,  2835,     4, 50118, 50118,\n",
      "           133,  5933,     9,     5,  2835,    16,     5,  5933,     9,     5,\n",
      "          2835,     4, 50118, 50118,   133,  5933,     9,     5,  2835,    16,\n",
      "             5,  5933,     9,     5,  2835,     4, 50118, 50118,   133,  5933,\n",
      "             9,     5,  2835,    16,     5,  5933,     9,     5,  2835,     4,\n",
      "         50118, 50118,   133,  5933,     9,     5,  2835,    16,     5,  5933,\n",
      "             9,     5,  2835,     4, 50118, 50118,   133,  5933,     9,     5,\n",
      "          2835,    16,     5,  5933,     9,     5,  2835,     4, 50118, 50118,\n",
      "           133,  5933,     9,     5,  2835,    16,     5,  5933,     9,     5,\n",
      "          2835,     4, 50118, 50118,   133,  5933,     9,     5,  2835,    16,\n",
      "             5,  5933,     9,     5,  2835,     4, 50118, 50118,   133,  5933,\n",
      "             9,     5,  2835,    16,     5,  5933,     9,     5,  2835,     4,\n",
      "         50118, 50118,   133,  5933,     9,     5,  2835,    16,     5,  5933,\n",
      "             9,     5,  2835,     4, 50118, 50118,   133,  5933,     9,     5,\n",
      "          2835,    16,     5,  5933,     9,     5,  2835,     4, 50118, 50118,\n",
      "           133,  5933,     9,     5,  2835,    16,     5,  5933,     9,     5,\n",
      "          2835,     4, 50118, 50118,   133,  5933,     9,     5,  2835,    16,\n",
      "             5,  5933,     9,     5,  2835,     4, 50118, 50118,   133,  5933]],\n",
      "       device='cuda:0')\n",
      "text_energy_per_token: [8.857062986285788]\n",
      "output_tokens: 230\n",
      "flop: 57580185290\n",
      "energy_consumed:  2037.1244868457313\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.05%     183.740ms        21.83%     249.878ms      17.353us     122.221ms        52.95%     122.221ms       8.488us         14400     38900.072  \n",
      "                                               aten::mm         0.25%       2.845ms         0.36%       4.078ms      20.388us      44.927ms        19.46%      44.927ms     224.636us           200     17682.874  \n",
      "                                              aten::bmm         5.03%      57.606ms         6.82%      78.043ms      16.259us       8.398ms         3.64%       8.398ms       1.750us          4800       986.849  \n",
      "                                              aten::add         5.56%      63.601ms         8.43%      96.522ms      12.062us       9.770ms         4.23%       9.770ms       1.221us          8002         8.252  \n",
      "                                              aten::mul         2.27%      25.974ms         3.36%      38.493ms      12.818us       3.247ms         1.41%       3.247ms       1.081us          3003         2.137  \n",
      "                                            aten::empty         5.11%      58.538ms         5.11%      58.538ms       3.104us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.35%       4.052ms         6.14%      70.311ms      16.693us       0.000us         0.00%       1.499ms       0.356us          4212            --  \n",
      "                                         aten::_to_copy         0.78%       8.880ms         5.79%      66.259ms      23.622us       0.000us         0.00%       1.499ms       0.534us          2805            --  \n",
      "                                    aten::empty_strided         1.37%      15.664ms         1.37%      15.664ms       5.211us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.41%      16.097ms         4.73%      54.112ms      14.769us       2.416ms         1.05%       2.416ms       0.659us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.145s\n",
      "Self CUDA time total: 230.816ms\n",
      "\n",
      "output: tensor([[    2,  1106,     5,   253, 21996,     9,    10,   516,  2835,    32,\n",
      "            36,   176,     6,   111,   176,    43,     8,    36,   698,     6,\n",
      "           204,   238,    99,    16,     5,  5933,     9,     5,  2835,   116,\n",
      "         50118, 50118,   133,  5933,     9,     5,  2835,    16,     5,  5933,\n",
      "             9,     5,  2835,     4, 50118, 50118,   133,  5933,     9,     5,\n",
      "          2835,    16,     5,  5933,     9,     5,  2835,     4, 50118, 50118,\n",
      "           133,  5933,     9,     5,  2835,    16,     5,  5933,     9,     5,\n",
      "          2835,     4, 50118, 50118,   133,  5933,     9,     5,  2835,    16,\n",
      "             5,  5933,     9,     5,  2835,     4, 50118, 50118,   133,  5933,\n",
      "             9,     5,  2835,    16,     5,  5933,     9,     5,  2835,     4,\n",
      "         50118, 50118,   133,  5933,     9,     5,  2835,    16,     5,  5933,\n",
      "             9,     5,  2835,     4, 50118, 50118,   133,  5933,     9,     5,\n",
      "          2835,    16,     5,  5933,     9,     5,  2835,     4, 50118, 50118,\n",
      "           133,  5933,     9,     5,  2835,    16,     5,  5933,     9,     5,\n",
      "          2835,     4, 50118, 50118,   133,  5933,     9,     5,  2835,    16,\n",
      "             5,  5933,     9,     5,  2835,     4, 50118, 50118,   133,  5933,\n",
      "             9,     5,  2835,    16,     5,  5933,     9,     5,  2835,     4,\n",
      "         50118, 50118,   133,  5933,     9,     5,  2835,    16,     5,  5933,\n",
      "             9,     5,  2835,     4, 50118, 50118,   133,  5933,     9,     5,\n",
      "          2835,    16,     5,  5933,     9,     5,  2835,     4, 50118, 50118,\n",
      "           133,  5933,     9,     5,  2835,    16,     5,  5933,     9,     5,\n",
      "          2835,     4, 50118, 50118,   133,  5933,     9,     5,  2835,    16,\n",
      "             5,  5933,     9,     5,  2835,     4, 50118, 50118,   133,  5933]],\n",
      "       device='cuda:0')\n",
      "text_energy_per_token: [8.857062986285788, 9.533727485721629]\n",
      "output_tokens: 230\n",
      "flop: 57580185290\n",
      "energy_consumed:  2192.7573217159747\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch\n",
    "import pynvml\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.profiler import profile, ProfilerActivity\n",
    "\n",
    "# Specify the GPU device you want to use\n",
    "device = \"cuda:0\"  # Change this to your preferred GPU\n",
    "\n",
    "# Initialize NVML for power measurement\n",
    "def initialize_nvml():\n",
    "    pynvml.nvmlInit()\n",
    "\n",
    "def shutdown_nvml():\n",
    "    pynvml.nvmlShutdown()\n",
    "\n",
    "def get_gpu_handle(gpu_index=0):\n",
    "    return pynvml.nvmlDeviceGetHandleByIndex(gpu_index)\n",
    "\n",
    "# Measure GPU power consumption over a period of time\n",
    "def measure_power_consumption(handle, duration_sec=1.0, interval_sec=0.1):\n",
    "    power_readings = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while (time.time() - start_time) < duration_sec:\n",
    "        power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0  # Convert from mW to W\n",
    "        power_readings.append(power)\n",
    "        time.sleep(interval_sec)\n",
    "    \n",
    "    return sum(power_readings) / len(power_readings) if power_readings else 0\n",
    "\n",
    "# Measure energy consumed during inference and FLOPs\n",
    "def measure_energy_during_inference(handle, inference_function, model, inputs, max_new_tokens=200):\n",
    "    power_start = measure_power_consumption(handle, duration_sec=0.5)\n",
    "    \n",
    "    # Start time for inference\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Measure FLOPs using PyTorch profiler\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], with_flops=True) as prof:\n",
    "        with torch.no_grad():\n",
    "            result = inference_function(inputs['input_ids'], max_new_tokens=max_new_tokens)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    power_end = measure_power_consumption(handle, duration_sec=0.5)\n",
    "    \n",
    "    avg_power = (power_start + power_end) / 2\n",
    "    elapsed_time = end_time - start_time\n",
    "    energy_consumed = avg_power * elapsed_time\n",
    "    print(\"prof keys flops table\")\n",
    "    print(prof.key_averages().table(sort_by=\"flops\", row_limit=10)) \n",
    "    # Calculate FLOPs\n",
    "    flops = sum([event.flops for event in prof.key_averages() if event.flops is not None])\n",
    "\n",
    "    return energy_consumed, elapsed_time, flops, result\n",
    "\n",
    "# Measure energy consumed during inference and FLOPs\n",
    "def NOTWORKING_measure_energy_during_inferenceWRONG_NOTWORKING(handle, inference_function, model, inputs, max_new_tokens=200):\n",
    "    # Measure initial power consumption\n",
    "    power_start = measure_power_consumption(handle, duration_sec=0.2)\n",
    "\n",
    "    # Start time for inference\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Measure FLOPs using PyTorch profiler\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], with_flops=True) as prof:\n",
    "        with torch.no_grad():\n",
    "            result = inference_function(model, inputs['input_ids'], max_new_tokens=max_new_tokens)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    # Measure final power consumption\n",
    "    power_end = measure_power_consumption(handle, duration_sec=0.2)\n",
    "\n",
    "    # Calculate average power and elapsed time\n",
    "    avg_power = (power_start + power_end) / 2\n",
    "    elapsed_time = end_time - start_time\n",
    "    energy_consumed = avg_power * elapsed_time\n",
    "    print(\"prof keys flops table\")\n",
    "    print(prof.key_averages().table(sort_by=\"flops\", row_limit=10))\n",
    "    # Calculate FLOPs\n",
    "    flops = sum(event.flops for event in prof.key_averages() if event.flops is not None)\n",
    "\n",
    "    return energy_consumed, elapsed_time, flops, result\n",
    "\n",
    "\n",
    "\n",
    "# Calculate perplexity for generated text\n",
    "def calculate_perplexity(model, input_text, tokenizer):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)  # Ensure input is on the same device\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "        perplexity = torch.exp(loss)\n",
    "    return perplexity.item()\n",
    "\n",
    "# Run the experiment for a list of texts\n",
    "def run_experiment_for_texts(texts, bootstrapping, handle, model, tokenizer):\n",
    "    latencies = []\n",
    "    energy_per_token = []\n",
    "    energy_per_flops = []\n",
    "    energy_per_task = []\n",
    "    throughputs = []\n",
    "    generated_texts = []\n",
    "    perplexities = []\n",
    "\n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\").to(device)  # Ensure input is on the same device\n",
    "        text_latencies = []\n",
    "        text_energy_per_token = []\n",
    "        text_energy_per_flops = []\n",
    "        text_energy_per_task = []\n",
    "        text_throughput = []\n",
    "        text_generated = []\n",
    "        text_perplexities = []\n",
    "\n",
    "        for _ in range(bootstrapping):\n",
    "            energy_consumed, latency, flops, output = measure_energy_during_inference(\n",
    "                handle, model.generate, model, inputs, max_new_tokens=200\n",
    "            )\n",
    "            text_latencies.append(latency)\n",
    "\n",
    "            print(\"output:\", output)\n",
    "            output_tokens = output.size(-1)\n",
    "            energy_token = energy_consumed / output_tokens if output_tokens > 0 else 0\n",
    "            text_energy_per_token.append(energy_token)\n",
    "\n",
    "            # Energy per FLOPs calculation\n",
    "\n",
    "            print(\"text_energy_per_token:\", text_energy_per_token)\n",
    "            print(\"output_tokens:\", output_tokens)\n",
    "            print(\"flop:\", flops)\n",
    "            print(\"energy_consumed: \",energy_consumed)\n",
    "            energy_flop = energy_consumed / flops #if flops > 0 else 0\n",
    "            text_energy_per_flops.append(energy_flop)\n",
    "\n",
    "            # Energy per task (full inference energy)\n",
    "            text_energy_per_task.append(energy_consumed)\n",
    "\n",
    "            throughput = output_tokens / latency\n",
    "            text_throughput.append(throughput)\n",
    "\n",
    "            perplexity = calculate_perplexity(model, text, tokenizer)\n",
    "            text_perplexities.append(perplexity)\n",
    "\n",
    "            generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            filtered_generated_text = generated_text.replace(text, \"\").strip()\n",
    "            text_generated.append(filtered_generated_text)\n",
    "\n",
    "        latencies.append(text_latencies)\n",
    "        energy_per_token.append(text_energy_per_token)\n",
    "        energy_per_flops.append(text_energy_per_flops)\n",
    "        energy_per_task.append(text_energy_per_task)\n",
    "        throughputs.append(text_throughput)\n",
    "        generated_texts.append(text_generated)\n",
    "        perplexities.append(text_perplexities)\n",
    "\n",
    "    return latencies, energy_per_token, energy_per_flops, energy_per_task, throughputs, generated_texts, perplexities\n",
    "\n",
    "# Collect metrics for each category\n",
    "def collect_metrics_for_categories(df, categories, bootstrapping, model, tokenizer):\n",
    "    category_metrics = {}\n",
    "    handle = get_gpu_handle(gpu_index=0)\n",
    "\n",
    "    for category in categories:\n",
    "        print(f\"Processing category: {category}\")\n",
    "        texts = filter_texts_by_category(df, category)\n",
    "        latencies, energy_per_token, energy_per_flops, energy_per_task, throughputs, generated_texts, perplexities = run_experiment_for_texts(\n",
    "            texts, bootstrapping, handle, model, tokenizer\n",
    "        )\n",
    "\n",
    "        category_metrics[category] = {\n",
    "            \"latencies\": latencies,\n",
    "            \"energy_per_token\": energy_per_token,\n",
    "            \"energy_per_flops\": energy_per_flops,\n",
    "            \"energy_per_task\": energy_per_task,\n",
    "            \"throughput\": throughputs,\n",
    "            \"generated_texts\": generated_texts,\n",
    "            \"perplexities\": perplexities\n",
    "        }\n",
    "\n",
    "    shutdown_nvml()  \n",
    "    return category_metrics\n",
    "\n",
    "def filter_texts_by_category(df, category):\n",
    "    return df[df['category'] == category]['text'].values\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Example Usage\n",
    "file_path = \"question.jsonl\"\n",
    "# bootstrapping = 2 \n",
    "df_mtconversation = load_dataset(file_path)\n",
    "\n",
    "#categories = [ 'common-sense']\n",
    "categories = ['knowledge', 'common-sense', 'coding', 'math']\n",
    "\n",
    "initialize_nvml()\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "metrics = collect_metrics_for_categories(df_mtconversation, categories, bootstrapping, model, tokenizer)\n",
    "\n",
    "# (Optionally, you can visualize the collected metrics here)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"flop_{model_name.replace('/','-').replace('.', '_')}_bootstrapping={bootstrapping}_metrics.json\", \"w\") as json_file:\n",
    "    json.dump(metrics, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x800 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to plot the metrics\n",
    "def plot_metrics(metrics, categories):\n",
    "    num_categories = len(categories)\n",
    "\n",
    "    # Prepare the data for plotting\n",
    "    avg_latencies = []\n",
    "    avg_perplexities = []\n",
    "    avg_energy_per_flops = []\n",
    "    avg_energy_per_token = []\n",
    "    avg_energy_per_task = []\n",
    "\n",
    "    for category in categories:\n",
    "        if category in metrics:\n",
    "            avg_latencies.append(np.mean(metrics[category][\"latencies\"]))\n",
    "            avg_perplexities.append(np.mean(metrics[category][\"perplexities\"]))\n",
    "            avg_energy_per_flops.append(np.mean(metrics[category][\"energy_per_flops\"]))\n",
    "            avg_energy_per_token.append(np.mean(metrics[category][\"energy_per_token\"]))\n",
    "            avg_energy_per_task.append(np.mean(metrics[category][\"energy_per_task\"]))\n",
    "        else:\n",
    "            avg_latencies.append(0)\n",
    "            avg_perplexities.append(0)\n",
    "            avg_energy_per_flops.append(0)\n",
    "            avg_energy_per_token.append(0)\n",
    "            avg_energy_per_task.append(0)\n",
    "\n",
    "    x = np.arange(num_categories)  # the label locations\n",
    "    width = 0.15  # the width of the bars\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "    # Plot latencies\n",
    "    bars1 = ax1.bar(x - 2*width, avg_latencies, width, label='Average Latency (s)', color='b')\n",
    "    ax1.set_ylabel('Average Latency (s)', color='b')\n",
    "    ax1.tick_params(axis='y', labelcolor='b')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(categories)\n",
    "\n",
    "    # Create a second y-axis for perplexities\n",
    "    ax2 = ax1.twinx()\n",
    "    bars2 = ax2.bar(x - width, avg_perplexities, width, label='Average Perplexity', color='g')\n",
    "    ax2.set_ylabel('Average Perplexity', color='g')\n",
    "    ax2.tick_params(axis='y', labelcolor='g')\n",
    "\n",
    "    # Create a third y-axis for energy per FLOPs\n",
    "    ax3 = ax1.twinx()\n",
    "    bars3 = ax3.bar(x, avg_energy_per_flops, width, label='Energy per FLOP (Joules)', color='r')\n",
    "    ax3.spines['right'].set_position(('outward', 60))  # move the third y-axis to the right\n",
    "    ax3.set_ylabel('Energy per FLOP (Joules)', color='r')\n",
    "    ax3.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "    # Create a fourth y-axis for energy per token\n",
    "    ax4 = ax1.twinx()\n",
    "    bars4 = ax4.bar(x + width, avg_energy_per_token, width, label='Energy per Token (Joules)', color='purple')\n",
    "    ax4.spines['right'].set_position(('outward', 120))  # move the fourth y-axis to the right\n",
    "    ax4.set_ylabel('Energy per Token (Joules)', color='purple')\n",
    "    ax4.tick_params(axis='y', labelcolor='purple')\n",
    "\n",
    "\n",
    "plot_metrics(metrics, categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# energy_per_flops with asynchronous energy measuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"facebook/opt-125m\"\n",
    "bootstrapping = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import threading\n",
    "import torch\n",
    "import pynvml\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.profiler import profile, ProfilerActivity\n",
    "\n",
    "# Specify the GPU device you want to use\n",
    "device = \"cuda:0\"  # Change this to your preferred GPU\n",
    "\n",
    "# Initialize NVML for power measurement\n",
    "def initialize_nvml():\n",
    "    pynvml.nvmlInit()\n",
    "\n",
    "def shutdown_nvml():\n",
    "    pynvml.nvmlShutdown()\n",
    "\n",
    "def get_gpu_handle(gpu_index=0):\n",
    "    return pynvml.nvmlDeviceGetHandleByIndex(gpu_index)\n",
    "\n",
    "def start_power_monitoring(handle, interval_sec=0.1):\n",
    "    power_readings = []\n",
    "    running = True\n",
    "\n",
    "    def monitor():\n",
    "        while running:\n",
    "            power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0  # Convert from mW to W\n",
    "            timestamp = time.time()\n",
    "            power_readings.append((timestamp, power))\n",
    "            time.sleep(interval_sec)\n",
    "\n",
    "    thread = threading.Thread(target=monitor)\n",
    "    thread.start()\n",
    "\n",
    "    def stop():\n",
    "        nonlocal running\n",
    "        running = False\n",
    "        thread.join()\n",
    "\n",
    "    return power_readings, stop\n",
    "\n",
    "\n",
    "# Measure energy consumed during inference and FLOPs\n",
    "def measure_energy_during_inference(handle, inference_function, model, inputs, max_new_tokens=200):\n",
    "    # Start power monitoring\n",
    "    power_readings, stop_monitoring = start_power_monitoring(handle, interval_sec=0.05)\n",
    "    \n",
    "    \n",
    "    # Start time for inference\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Measure FLOPs using PyTorch profiler\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], with_flops=True) as prof:\n",
    "        with torch.no_grad():\n",
    "            result = inference_function(inputs['input_ids'], max_new_tokens=max_new_tokens)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Stop power monitoring\n",
    "    stop_monitoring()\n",
    "\n",
    "    # Filter power readings during inference\n",
    "    power_during_inference = [p for t, p in power_readings if start_time <= t <= end_time]\n",
    "\n",
    "    \n",
    "    # Calculate average power and energy consumed\n",
    "    if power_during_inference:\n",
    "        avg_power = sum(power_during_inference) / len(power_during_inference)\n",
    "        elapsed_time = end_time - start_time\n",
    "        energy_consumed = avg_power * elapsed_time\n",
    "    else:\n",
    "        avg_power = 0\n",
    "        energy_consumed = 0\n",
    "        elapsed_time = end_time - start_time\n",
    "    print(\"prof keys flops table\")\n",
    "    print(prof.key_averages().table(sort_by=\"flops\", row_limit=10)) \n",
    "    # Calculate FLOPs\n",
    "    flops = sum([event.flops for event in prof.key_averages() if event.flops is not None])\n",
    "\n",
    "    return energy_consumed, elapsed_time, flops, result\n",
    "\n",
    "# Measure energy consumed during inference and FLOPs\n",
    "def NOTWORKING_measure_energy_during_inferenceWRONG_NOTWORKING(handle, inference_function, model, inputs, max_new_tokens=200):\n",
    "    # Measure initial power consumption\n",
    "    power_start = measure_power_consumption(handle, duration_sec=0.2)\n",
    "\n",
    "    # Start time for inference\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Measure FLOPs using PyTorch profiler\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], with_flops=True) as prof:\n",
    "        with torch.no_grad():\n",
    "            result = inference_function(model, inputs['input_ids'], max_new_tokens=max_new_tokens)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    # Measure final power consumption\n",
    "    power_end = measure_power_consumption(handle, duration_sec=0.2)\n",
    "\n",
    "    # Calculate average power and elapsed time\n",
    "    avg_power = (power_start + power_end) / 2\n",
    "    elapsed_time = end_time - start_time\n",
    "    energy_consumed = avg_power * elapsed_time\n",
    "    print(\"prof keys flops table\")\n",
    "    print(prof.key_averages().table(sort_by=\"flops\", row_limit=10))\n",
    "    # Calculate FLOPs\n",
    "    flops = sum(event.flops for event in prof.key_averages() if event.flops is not None)\n",
    "\n",
    "    return energy_consumed, elapsed_time, flops, result\n",
    "\n",
    "\n",
    "\n",
    "# Calculate perplexity for generated text\n",
    "def calculate_perplexity(model, input_text, tokenizer):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)  # Ensure input is on the same device\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "        perplexity = torch.exp(loss)\n",
    "    return perplexity.item()\n",
    "\n",
    "# Run the experiment for a list of texts\n",
    "def run_experiment_for_texts(texts, bootstrapping, handle, model, tokenizer):\n",
    "    latencies = []\n",
    "    energy_per_token = []\n",
    "    energy_per_flops = []\n",
    "    energy_per_task = []\n",
    "    throughputs = []\n",
    "    generated_texts = []\n",
    "    perplexities = []\n",
    "\n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\").to(device)  # Ensure input is on the same device\n",
    "        text_latencies = []\n",
    "        text_energy_per_token = []\n",
    "        text_energy_per_flops = []\n",
    "        text_energy_per_task = []\n",
    "        text_throughput = []\n",
    "        text_generated = []\n",
    "        text_perplexities = []\n",
    "\n",
    "        for _ in range(bootstrapping):\n",
    "            energy_consumed, latency, flops, output = measure_energy_during_inference(\n",
    "                handle, model.generate, model, inputs, max_new_tokens=200\n",
    "            )\n",
    "            text_latencies.append(latency)\n",
    "\n",
    "            print(\"output:\", output)\n",
    "            output_tokens = output.size(-1)\n",
    "            energy_token = energy_consumed / output_tokens if output_tokens > 0 else 0\n",
    "            text_energy_per_token.append(energy_token)\n",
    "\n",
    "            # Energy per FLOPs calculation\n",
    "\n",
    "            print(\"text_energy_per_token:\", text_energy_per_token)\n",
    "            print(\"output_tokens:\", output_tokens)\n",
    "            print(\"flop:\", flops)\n",
    "            print(\"energy_consumed: \",energy_consumed)\n",
    "            energy_flop = energy_consumed / flops #if flops > 0 else 0\n",
    "            text_energy_per_flops.append(energy_flop)\n",
    "\n",
    "            # Energy per task (full inference energy)\n",
    "            text_energy_per_task.append(energy_consumed)\n",
    "\n",
    "            throughput = output_tokens / latency\n",
    "            text_throughput.append(throughput)\n",
    "\n",
    "            perplexity = calculate_perplexity(model, text, tokenizer)\n",
    "            text_perplexities.append(perplexity)\n",
    "\n",
    "            generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            filtered_generated_text = generated_text.replace(text, \"\").strip()\n",
    "            text_generated.append(filtered_generated_text)\n",
    "\n",
    "        latencies.append(text_latencies)\n",
    "        energy_per_token.append(text_energy_per_token)\n",
    "        energy_per_flops.append(text_energy_per_flops)\n",
    "        energy_per_task.append(text_energy_per_task)\n",
    "        throughputs.append(text_throughput)\n",
    "        generated_texts.append(text_generated)\n",
    "        perplexities.append(text_perplexities)\n",
    "\n",
    "    return latencies, energy_per_token, energy_per_flops, energy_per_task, throughputs, generated_texts, perplexities\n",
    "\n",
    "# Collect metrics for each category\n",
    "def collect_metrics_for_categories(df, categories, bootstrapping, model, tokenizer):\n",
    "    category_metrics = {}\n",
    "    handle = get_gpu_handle(gpu_index=0)\n",
    "\n",
    "    for category in categories:\n",
    "        print(f\"Processing category: {category}\")\n",
    "        texts = filter_texts_by_category(df, category)\n",
    "        latencies, energy_per_token, energy_per_flops, energy_per_task, throughputs, generated_texts, perplexities = run_experiment_for_texts(\n",
    "            texts, bootstrapping, handle, model, tokenizer\n",
    "        )\n",
    "\n",
    "        category_metrics[category] = {\n",
    "            \"latencies\": latencies,\n",
    "            \"energy_per_token\": energy_per_token,\n",
    "            \"energy_per_flops\": energy_per_flops,\n",
    "            \"energy_per_task\": energy_per_task,\n",
    "            \"throughput\": throughputs,\n",
    "            \"generated_texts\": generated_texts,\n",
    "            \"perplexities\": perplexities\n",
    "        }\n",
    "\n",
    "    shutdown_nvml()  \n",
    "    return category_metrics\n",
    "\n",
    "def filter_texts_by_category(df, category):\n",
    "    return df[df['category'] == category]['text'].values\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Example Usage\n",
    "file_path = \"./question.jsonl\"\n",
    "# bootstrapping = 2 \n",
    "df_mtconversation = load_dataset(file_path)\n",
    "\n",
    "#categories = [ 'common-sense']\n",
    "categories = ['knowledge', 'common-sense', 'coding', 'math']\n",
    "\n",
    "initialize_nvml()\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "metrics = collect_metrics_for_categories(df_mtconversation, categories, bootstrapping, model, tokenizer)\n",
    "\n",
    "# (Optionally, you can visualize the collected metrics here)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"flop_async_{model_name.replace('/','-').replace('.', '_')}_bootstrapping={bootstrapping}_metrics.json\", \"w\") as json_file:\n",
    "    json.dump(metrics, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the metrics\n",
    "def plot_metrics(metrics, categories):\n",
    "    num_categories = len(categories)\n",
    "\n",
    "    # Prepare the data for plotting\n",
    "    avg_latencies = []\n",
    "    avg_perplexities = []\n",
    "    avg_energy_per_flops = []\n",
    "    avg_energy_per_token = []\n",
    "    avg_energy_per_task = []\n",
    "\n",
    "    for category in categories:\n",
    "        if category in metrics:\n",
    "            avg_latencies.append(np.mean(metrics[category][\"latencies\"]))\n",
    "            avg_perplexities.append(np.mean(metrics[category][\"perplexities\"]))\n",
    "            avg_energy_per_flops.append(np.mean(metrics[category][\"energy_per_flops\"]))\n",
    "            avg_energy_per_token.append(np.mean(metrics[category][\"energy_per_token\"]))\n",
    "            avg_energy_per_task.append(np.mean(metrics[category][\"energy_per_task\"]))\n",
    "        else:\n",
    "            avg_latencies.append(0)\n",
    "            avg_perplexities.append(0)\n",
    "            avg_energy_per_flops.append(0)\n",
    "            avg_energy_per_token.append(0)\n",
    "            avg_energy_per_task.append(0)\n",
    "\n",
    "    x = np.arange(num_categories)  # the label locations\n",
    "    width = 0.15  # the width of the bars\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "    # Plot latencies\n",
    "    bars1 = ax1.bar(x - 2*width, avg_latencies, width, label='Average Latency (s)', color='b')\n",
    "    ax1.set_ylabel('Average Latency (s)', color='b')\n",
    "    ax1.tick_params(axis='y', labelcolor='b')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(categories)\n",
    "\n",
    "    # Create a second y-axis for perplexities\n",
    "    ax2 = ax1.twinx()\n",
    "    bars2 = ax2.bar(x - width, avg_perplexities, width, label='Average Perplexity', color='g')\n",
    "    ax2.set_ylabel('Average Perplexity', color='g')\n",
    "    ax2.tick_params(axis='y', labelcolor='g')\n",
    "\n",
    "    # Create a third y-axis for energy per FLOPs\n",
    "    ax3 = ax1.twinx()\n",
    "    bars3 = ax3.bar(x, avg_energy_per_flops, width, label='Energy per FLOP (Joules)', color='r')\n",
    "    ax3.spines['right'].set_position(('outward', 60))  # move the third y-axis to the right\n",
    "    ax3.set_ylabel('Energy per FLOP (Joules)', color='r')\n",
    "    ax3.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "    # Create a fourth y-axis for energy per token\n",
    "    ax4 = ax1.twinx()\n",
    "    bars4 = ax4.bar(x + width, avg_energy_per_token, width, label='Energy per Token (Joules)', color='purple')\n",
    "    ax4.spines['right'].set_position(('outward', 120))  # move the fourth y-axis to the right\n",
    "    ax4.set_ylabel('Energy per Token (Joules)', color='purple')\n",
    "    ax4.tick_params(axis='y', labelcolor='purple')\n",
    "\n",
    "\n",
    "plot_metrics(metrics, categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# energy_per_flops with asynchronous energy measuring MMLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Science, Technology, Engineering, Mathematics = stem\n",
    "stem = [\"clinical_knowledge\",\n",
    "\"medical_genetics\", \n",
    "\"high_school_physics\",\n",
    "\"virology\",\n",
    "\"high_school_biology\",\n",
    "\"abstract_algebra\",\n",
    "\"professional_medicine\",\n",
    "\"nutrition\",\n",
    "\"machine_learning\",\n",
    "\"anatomy\",\n",
    "\"college_medicine\",\n",
    "\"college_chemistry\",\n",
    "\"elementary_mathematics\",\n",
    "\"human_aging\",\n",
    "\"college_mathematics\",\n",
    "\"high_school_statistics\",\n",
    "\"high_school_mathematics\",\n",
    "\"high_school_computer_science\",\n",
    "\"conceptual_physics\",\n",
    "\"high_school_chemistry\",\n",
    "\"college_physics\",\n",
    "\"electrical_engineering\",\n",
    "\"astronomy\",\n",
    "\"college_biology\",\n",
    "\"computer_security\"]\n",
    "\n",
    "humanities= [\"high_school_european_history\",\n",
    "\"high_school_us_history\",\n",
    "\"high_school_world_history\",\n",
    "\"philosophy\",\n",
    "\"global_facts\",\n",
    "\"security_studies\",\n",
    "\"prehistory\",\n",
    "\"high_school_government_and_politics\",\n",
    "\"logical_fallacies\",\n",
    "\"international_law\",\n",
    "\"jurisprudence\",\n",
    "\"world_religions\",\n",
    "\"us_foreign_policy\",\n",
    "\"moral_scenarios\",\n",
    "\"moral_disputes\"\n",
    "]\n",
    "\n",
    "sociology = [\"sociology\",\n",
    "\"professional_psychology\",\n",
    "\"high_school_psychology\",\n",
    "\"human_sexuality\"]\n",
    "\n",
    "others = [\"business_ethics\",\n",
    "\"high_school_microeconomics\",\n",
    "\"econometrics\",\n",
    "\"professional_accounting\",\n",
    "\"public_relations\",\n",
    "\"marketing\",\n",
    "\"professional_law\",\n",
    "\"management\",\n",
    "\"miscellaneous\",\n",
    "\"high_school_macroeconomics\"]\n",
    "\n",
    "math = [\"abstract_algebra\",\n",
    "\t\"college_mathematics\",\n",
    "\t\"elementary_mathematics\",\n",
    "\t\"high_school_mathematics\",\n",
    "\t\"high_school_statistics\"]\n",
    "\n",
    "math1 = [\"abstract_algebra\",\n",
    "\t\"college_mathematics\",\n",
    "#\t\"elementary_mathematics\",\n",
    "#\t\"high_school_mathematics\",\n",
    "\t\"high_school_statistics\"]\n",
    "\n",
    "computer_science = [\"college_computer_science\",\n",
    "\t\"computer_security\",\n",
    "\t\"high_school_computer_science\",\n",
    "\t\"machine_learning\"]\n",
    "\n",
    "health = [\"anatomy\",\n",
    "\t\"clinical_knowledge\",\n",
    "\t\"college_medicine\",\n",
    "\t\"human_aging\",\n",
    "\t\"medical_genetics\",\n",
    "\t\"nutrition\",\n",
    "\t\"professional_medicine\",\n",
    "\t\"virology\"],\n",
    "\n",
    "semanticdifferent = [\"abstract_algebra\",\"college_mathematics\",\"college_computer_science\",\"computer_security\", \"anatomy\",\"virology\", \"professional_medicine\",\"econometrics\", \"management\",\"sociology\", \"high_school_world_history\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import threading\n",
    "import torch\n",
    "import pynvml\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.profiler import profile, ProfilerActivity\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import subprocess\n",
    "\n",
    "# Initialize NVML for power measurement\n",
    "def initialize_nvml():\n",
    "    pynvml.nvmlInit()\n",
    "\n",
    "def shutdown_nvml():\n",
    "    pynvml.nvmlShutdown()\n",
    "\n",
    "def get_gpu_handle(gpu_index=0):\n",
    "    return pynvml.nvmlDeviceGetHandleByIndex(gpu_index)\n",
    "\n",
    "def start_power_monitoring(handle, interval_sec=0.1):\n",
    "    power_readings = []\n",
    "    running = True\n",
    "\n",
    "    def monitor():\n",
    "        while running:\n",
    "            power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0  # Convert from mW to W\n",
    "            timestamp = time.time()\n",
    "            power_readings.append((timestamp, power))\n",
    "            time.sleep(interval_sec)\n",
    "\n",
    "    thread = threading.Thread(target=monitor)\n",
    "    thread.start()\n",
    "\n",
    "    def stop():\n",
    "        nonlocal running\n",
    "        running = False\n",
    "        thread.join()\n",
    "\n",
    "    return power_readings, stop\n",
    "\n",
    "\n",
    "# Map generated text to one of the options A, B, C, D\n",
    "def map_generated_text_to_option(generated_text):\n",
    "    valid_options = ['A', 'B', 'C', 'D']\n",
    "    generated_text = generated_text.strip().upper()\n",
    "    if generated_text in valid_options:\n",
    "        return generated_text\n",
    "    #else:\n",
    "        # Attempt to extract the option from the text\n",
    "        #for option in valid_options:\n",
    "            #if option in generated_text:\n",
    "                #return option\n",
    "        # If no valid option is found, return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def calculate_perplexity1(model, inputs):\n",
    "    #inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)  # Ensure input is on the same device\n",
    "    with torch.no_grad():\n",
    "        outputs = model( labels=inputs)\n",
    "        loss = outputs.loss\n",
    "        perplexity = torch.exp(loss)\n",
    "    return perplexity.item()\n",
    "\n",
    "\n",
    "def calculate_perplexity(model, inputs, attention_mask=None):\n",
    "    # Assume `inputs` is a tensor directly containing input_ids\n",
    "    input_ids = inputs  # Directly use inputs if it's a tensor\n",
    "    labels = input_ids.clone()  # Copy input_ids to use as labels\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Pass input_ids and optionally attention_mask to the model\n",
    "        if attention_mask is not None:\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        else:\n",
    "            outputs = model(input_ids=input_ids, labels=labels)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        perplexity = torch.exp(loss)\n",
    "    \n",
    "    return perplexity.item()\n",
    "\n",
    "\n",
    "\n",
    "# Measure energy consumed during inference and FLOPs\n",
    "def measure_energy_during_inference(handle, inference_function, model, inputs, max_new_tokens=1):\n",
    "    print(f\"tokens: {max_new_tokens}\")\n",
    "    \n",
    "    # Start power monitoring\n",
    "    power_readings, stop_monitoring = start_power_monitoring(handle, interval_sec=0.05)\n",
    "    \n",
    "    # Start time for inference\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Measure FLOPs using PyTorch profiler\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], with_flops=True, record_shapes=False) as prof:\n",
    "        with torch.no_grad():\n",
    "            result = inference_function(inputs['input_ids'], max_new_tokens=max_new_tokens, do_sample=False )#num_beams=1)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Stop power monitoring\n",
    "    stop_monitoring()\n",
    "\n",
    "    # Filter power readings during inference\n",
    "    power_during_inference = [p for t, p in power_readings if start_time <= t <= end_time]\n",
    "\n",
    "    # Calculate average power and energy consumed\n",
    "    if power_during_inference:\n",
    "        avg_power = sum(power_during_inference) / len(power_during_inference)\n",
    "        elapsed_time = end_time - start_time\n",
    "        energy_consumed = avg_power * elapsed_time\n",
    "    else:\n",
    "        avg_power = 0\n",
    "        energy_consumed = 0\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "    # Calculate FLOPs\n",
    "    flops = sum([event.flops for event in prof.key_averages() if event.flops is not None])\n",
    "\n",
    "    perplexity = calculate_perplexity(model, inputs['input_ids'])\n",
    "\n",
    "    return energy_consumed, elapsed_time, flops, result, power_during_inference, perplexity\n",
    "\n",
    "# Load the MMLU dataset for specified categories\n",
    "def load_mmlu_data(categories):\n",
    "    category_dataframes = {}  # Dictionary to store DataFrames for each category\n",
    "        \n",
    "    for category in categories:\n",
    "        print(\"Loading Data for category: \", category)\n",
    "            \n",
    "        # Load the dataset for the given category\n",
    "        mmlu_dataset = load_dataset(\"lukaemon/mmlu\", category, split='validation', trust_remote_code=True)\n",
    "        \n",
    "        # Create a DataFrame for the current category\n",
    "        df_category = pd.DataFrame({\n",
    "            'input': mmlu_dataset['input'],  # The question or prompt\n",
    "            'A': mmlu_dataset['A'],          # Option A\n",
    "            'B': mmlu_dataset['B'],          # Option B\n",
    "            'C': mmlu_dataset['C'],          # Option C\n",
    "            'D': mmlu_dataset['D'],          # Option D\n",
    "            'target': mmlu_dataset['target'] # The correct answer (e.g., 'A', 'B', 'C', 'D')\n",
    "        })\n",
    "        \n",
    "        # Store the DataFrame in the dictionary, with the category as the key\n",
    "        category_dataframes[category] = df_category\n",
    "        \n",
    "    return category_dataframes\n",
    "\n",
    "# Run the experiment for a category in the MMLU dataset\n",
    "def run_experiment_for_mmlu_category(data, bootstrapping, handle, model, tokenizer, max_new_tokens):\n",
    "    latencies = []\n",
    "    energy_per_token = []\n",
    "    energy_per_flops = []\n",
    "    energy_per_task = []\n",
    "    throughputs = []\n",
    "    generated_texts = []\n",
    "    accuracies = []\n",
    "    flopslisttotal = []\n",
    "    energy_over_time = []\n",
    "    perplexities = []\n",
    "    power_over_time = []\n",
    "\n",
    "    for idx, row in data.iterrows():\n",
    "        # Construct the prompt\n",
    "        prompt = f\"Question: {row['input']}\\nA) {row['A']}\\nB) {row['B']}\\nC) {row['C']}\\nD) {row['D']}\\nAnswer:\"\n",
    "        #prompt = \"Hello, how are you my friend?\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")  # Ensure input is on the same device\n",
    "        text_latencies = []\n",
    "        text_energy_per_token = []\n",
    "        text_energy_per_flops = []\n",
    "        text_energy_per_task = []\n",
    "        text_throughput = []\n",
    "        text_generated = []\n",
    "        correct_predictions = 0  # To calculate accuracy\n",
    "        floplist = []\n",
    "        energy = []\n",
    "        power_inf = []\n",
    "        perplexity_prompt = []\n",
    "\n",
    "        for _ in range(bootstrapping):\n",
    "            energy_consumed, latency, flops, output, power_during_inference, perplexity = measure_energy_during_inference(\n",
    "                handle, model.generate, model, inputs, max_new_tokens=max_new_tokens\n",
    "            )\n",
    "            perplexity_prompt.append(perplexity)\n",
    "            power_inf.append(power_during_inference)\n",
    "            energy.append(energy_consumed)\n",
    "            text_latencies.append(latency)\n",
    "            output_tokens = output.size(-1) - inputs['input_ids'].size(-1)\n",
    "            energy_token = energy_consumed / output_tokens if output_tokens > 0 else 0\n",
    "            text_energy_per_token.append(energy_token)\n",
    "\n",
    "            energy_flop = energy_consumed / flops if flops > 0 else 0\n",
    "            text_energy_per_flops.append(energy_flop)\n",
    "            text_energy_per_task.append(energy_consumed)\n",
    "            throughput = output_tokens / latency if latency > 0 else 0\n",
    "            text_throughput.append(throughput)\n",
    "\n",
    "            # Decode the generated token\n",
    "            generated_text = tokenizer.decode(output[0][inputs['input_ids'].size(-1):], skip_special_tokens=True)\n",
    "            generated_text = generated_text.strip()\n",
    "            print(f\"generated text: {generated_text}\")\n",
    "            text_generated.append(generated_text)\n",
    "\n",
    "            floplist.append(flops)\n",
    "            \n",
    "            # Map the generated text to an option\n",
    "            mapped_answer = map_generated_text_to_option(generated_text)\n",
    "            print(f\"Generated answer: '{mapped_answer}' | Correct answer: '{row['target']}'\")\n",
    "            if mapped_answer == row['target']:\n",
    "                print(\"Adding to correct predictions\")\n",
    "                correct_predictions += 1\n",
    "\n",
    "        perplexities.append(perplexity_prompt)\n",
    "        power_over_time.append(power_inf)\n",
    "        energy_over_time.append(energy)\n",
    "        flopslisttotal.append(floplist)\n",
    "        accuracy = correct_predictions / bootstrapping\n",
    "        accuracies.append(accuracy)\n",
    "        latencies.append(text_latencies)\n",
    "        energy_per_token.append(text_energy_per_token)\n",
    "        energy_per_flops.append(text_energy_per_flops)\n",
    "        energy_per_task.append(text_energy_per_task)\n",
    "        throughputs.append(text_throughput)\n",
    "        generated_texts.append(text_generated)\n",
    "\n",
    "    overall_accuracy = np.mean(accuracies)\n",
    "    return latencies, energy_per_token, energy_per_flops, energy_per_task, throughputs, generated_texts, overall_accuracy, flopslisttotal, energy_over_time, power_over_time, perplexities\n",
    "\n",
    "# Collect metrics for each category\n",
    "def collect_metrics_for_categories(data_dict, categories, bootstrapping, model, tokenizer, max_new_tokens):\n",
    "    category_metrics = {}\n",
    "    handle = get_gpu_handle(gpu_index=0)\n",
    "\n",
    "    for category in categories:\n",
    "        print(f\"Processing category: {category}\")\n",
    "        data = data_dict[category]\n",
    "        latencies, energy_per_token, energy_per_flops, energy_per_task, throughputs, generated_texts, overall_accuracy, flopslisttotal, energy_over_time, power_over_time, perplexities = run_experiment_for_mmlu_category(\n",
    "            data, bootstrapping, handle, model, tokenizer, max_new_tokens\n",
    "        )\n",
    "\n",
    "        category_metrics[category] = {\n",
    "            \"latencies\": latencies,\n",
    "            \"energy_per_token\": energy_per_token,\n",
    "            \"energy_per_flops\": energy_per_flops,\n",
    "            \"energy_per_task\": energy_per_task,\n",
    "            \"throughput\": throughputs,\n",
    "            \"generated_texts\": generated_texts,\n",
    "            \"accuracy\": overall_accuracy,\n",
    "            \"flopstotal\": flopslisttotal,\n",
    "            \"energy_over_time\": energy_over_time,\n",
    "            \"power_over_time\": power_over_time,\n",
    "            \"perplexity\": perplexities\n",
    "        }\n",
    "\n",
    "    shutdown_nvml()  \n",
    "    return category_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data for category:  abstract_algebra\n",
      "Loading Data for category:  college_mathematics\n",
      "Loading Data for category:  college_computer_science\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 4706.51 examples/s]\n",
      "Generating validation split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 1504.95 examples/s]\n",
      "Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 817.28 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data for category:  computer_security\n",
      "Loading Data for category:  anatomy\n",
      "Loading Data for category:  virology\n",
      "Loading Data for category:  professional_medicine\n",
      "Loading Data for category:  econometrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 114/114 [00:00<00:00, 9937.04 examples/s]\n",
      "Generating validation split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:00<00:00, 2357.68 examples/s]\n",
      "Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 1072.99 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data for category:  management\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [00:00<00:00, 7264.63 examples/s]\n",
      "Generating validation split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 1665.97 examples/s]\n",
      "Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 829.18 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data for category:  sociology\n",
      "Loading Data for category:  high_school_world_history\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 237/237 [00:00<00:00, 7690.79 examples/s]\n",
      "Generating validation split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:00<00:00, 3002.03 examples/s]\n",
      "Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 788.11 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing category: abstract_algebra\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "Processing category: college_mathematics\n",
      "tokens: 1\n",
      "generated text: If\n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: If\n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: Which\n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: Which\n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: A\n",
      "Generated answer: 'A' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: A\n",
      "Generated answer: 'A' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: The\n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: The\n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "Processing category: college_computer_science\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: It\n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: It\n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "Processing category: computer_security\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: Which\n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: Which\n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: Yes\n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: Yes\n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: Yes\n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: Yes\n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "Processing category: anatomy\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: The\n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: The\n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: A\n",
      "Generated answer: 'A' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: A\n",
      "Generated answer: 'A' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: The\n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: The\n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: Which\n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: Which\n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: T\n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: T\n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: The\n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: The\n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: Smooth\n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: Smooth\n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: Which\n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: Which\n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: The\n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: The\n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: A\n",
      "Generated answer: 'A' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: A\n",
      "Generated answer: 'A' | Correct answer: 'D'\n",
      "Processing category: virology\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: Gen\n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: Gen\n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: Vir\n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: Vir\n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: (\n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: (\n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: Which\n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: Which\n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "Processing category: professional_medicine\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: A\n",
      "Generated answer: 'A' | Correct answer: 'A'\n",
      "Adding to correct predictions\n",
      "tokens: 1\n",
      "generated text: A\n",
      "Generated answer: 'A' | Correct answer: 'A'\n",
      "Adding to correct predictions\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: A\n",
      "Generated answer: 'A' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: A\n",
      "Generated answer: 'A' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: A\n",
      "Generated answer: 'A' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: A\n",
      "Generated answer: 'A' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: A\n",
      "Generated answer: 'A' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: A\n",
      "Generated answer: 'A' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: A\n",
      "Generated answer: 'A' | Correct answer: 'A'\n",
      "Adding to correct predictions\n",
      "tokens: 1\n",
      "generated text: A\n",
      "Generated answer: 'A' | Correct answer: 'A'\n",
      "Adding to correct predictions\n",
      "tokens: 1\n",
      "generated text: A\n",
      "Generated answer: 'A' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: A\n",
      "Generated answer: 'A' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: A\n",
      "Generated answer: 'A' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: A\n",
      "Generated answer: 'A' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: A\n",
      "Generated answer: 'A' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: A\n",
      "Generated answer: 'A' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: A\n",
      "Generated answer: 'A' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: A\n",
      "Generated answer: 'A' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: A\n",
      "Generated answer: 'A' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: A\n",
      "Generated answer: 'A' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: A\n",
      "Generated answer: 'A' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: A\n",
      "Generated answer: 'A' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: Ac\n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: Ac\n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: A\n",
      "Generated answer: 'A' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: A\n",
      "Generated answer: 'A' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: A\n",
      "Generated answer: 'A' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: A\n",
      "Generated answer: 'A' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: A\n",
      "Generated answer: 'A' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: A\n",
      "Generated answer: 'A' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: A\n",
      "Generated answer: 'A' | Correct answer: 'A'\n",
      "Adding to correct predictions\n",
      "tokens: 1\n",
      "generated text: A\n",
      "Generated answer: 'A' | Correct answer: 'A'\n",
      "Adding to correct predictions\n",
      "tokens: 1\n",
      "generated text: A\n",
      "Generated answer: 'A' | Correct answer: 'A'\n",
      "Adding to correct predictions\n",
      "tokens: 1\n",
      "generated text: A\n",
      "Generated answer: 'A' | Correct answer: 'A'\n",
      "Adding to correct predictions\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "Processing category: econometrics\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: The\n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: The\n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "Processing category: management\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: Which\n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: Which\n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: Which\n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: Which\n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: The\n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: The\n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: Yes\n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: Yes\n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: R\n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: R\n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \"\n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \"\n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: Which\n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: Which\n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "Processing category: sociology\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: A\n",
      "Generated answer: 'A' | Correct answer: 'A'\n",
      "Adding to correct predictions\n",
      "tokens: 1\n",
      "generated text: A\n",
      "Generated answer: 'A' | Correct answer: 'A'\n",
      "Adding to correct predictions\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: The\n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: The\n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: Soci\n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: Soci\n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: The\n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: The\n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: Yes\n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: Yes\n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: The\n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: The\n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: Dis\n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: Dis\n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: Which\n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: Which\n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "Processing category: high_school_world_history\n",
      "tokens: 1\n",
      "generated text: \"\n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \"\n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: The\n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: The\n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: Conf\n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: Conf\n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \"\n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \"\n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'B'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'C'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: It\n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: It\n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'A'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n",
      "tokens: 1\n",
      "generated text: \n",
      "Generated answer: 'None' | Correct answer: 'D'\n"
     ]
    }
   ],
   "source": [
    "categories = semanticdifferent # math computer_science health semanticdifferent\n",
    "\n",
    "category_text = \"math\"\n",
    "\n",
    "# Bootstrapping iterations\n",
    "bootstrapping = 2\n",
    "\n",
    "# max new output tokens\n",
    "max_new_tokens = 1\n",
    "\n",
    "initialize_nvml()\n",
    "\n",
    "# HF Access Token\n",
    "access_token = \"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\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = 'facebook/opt-125m'\n",
    "            #\"meta-llama/Llama-3.1-8B\" \n",
    "            #\"meta-llama/Llama-3.1-8B\"  \n",
    "            #\"facebook/opt-125m\"\n",
    "            #\"tiiuae/falcon-7b\"\n",
    "            #\"ProbeMedicalYonseiMAILab/medllama3-v20\"\n",
    "            #\"NTQAI/Nxcode-CQ-7B-orpo\"\n",
    "            #\"MathLLMs/MathCoder-L-7B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", token=access_token)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=access_token)\n",
    "\n",
    "# Load MMLU data\n",
    "data_dict = load_mmlu_data(categories)\n",
    "\n",
    "# Collect metrics\n",
    "flop_mmlu_metrics = collect_metrics_for_categories(data_dict, categories, bootstrapping, model, tokenizer, max_new_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_double_entries(metrics):\n",
    "    for i, outer_list in enumerate(metrics):\n",
    "        for j, inner_list in enumerate(outer_list):\n",
    "            # Remove duplicates within each list and retain only unique values\n",
    "            metrics[i][j] = list(set(inner_list))\n",
    "    return metrics\n",
    "flop_mmlu_metrics['abstract_algebra']['power_over_time'] = clean_double_entries(flop_mmlu_metrics['abstract_algebra']['power_over_time'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shape mismatch: objects cannot be broadcast to a single shape.  Mismatch is between arg 0 with shape (3,) and arg 1 with shape (0,).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 85\u001b[0m\n\u001b[1;32m     82\u001b[0m     ax5\u001b[38;5;241m.\u001b[39mtick_params(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m, labelcolor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpurple\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Call the plotting function\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m \u001b[43mplot_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflop_mmlu_metrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategories\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[53], line 58\u001b[0m, in \u001b[0;36mplot_metrics\u001b[0;34m(metrics, categories)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Create a second y-axis for perplexities\u001b[39;00m\n\u001b[1;32m     57\u001b[0m ax2 \u001b[38;5;241m=\u001b[39m ax1\u001b[38;5;241m.\u001b[39mtwinx()\n\u001b[0;32m---> 58\u001b[0m bars2 \u001b[38;5;241m=\u001b[39m \u001b[43max2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mavg_perplexities\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAverage Perplexity\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m ax2\u001b[38;5;241m.\u001b[39mset_ylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAverage Perplexity\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     60\u001b[0m ax2\u001b[38;5;241m.\u001b[39mtick_params(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m, labelcolor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/projects/energy_per_token/venv/lib/python3.10/site-packages/matplotlib/__init__.py:1473\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m   1471\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1472\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1473\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1474\u001b[0m \u001b[43m            \u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1475\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1476\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1478\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1479\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[1;32m   1480\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[0;32m~/projects/energy_per_token/venv/lib/python3.10/site-packages/matplotlib/axes/_axes.py:2520\u001b[0m, in \u001b[0;36mAxes.bar\u001b[0;34m(self, x, height, width, bottom, align, **kwargs)\u001b[0m\n\u001b[1;32m   2517\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m yerr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2518\u001b[0m         yerr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_dx(yerr, y0, y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_yunits)\n\u001b[0;32m-> 2520\u001b[0m x, height, width, y, linewidth, hatch \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2521\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Make args iterable too.\u001b[39;49;00m\n\u001b[1;32m   2522\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43matleast_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinewidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2524\u001b[0m \u001b[38;5;66;03m# Now that units have been converted, set the tick locations.\u001b[39;00m\n\u001b[1;32m   2525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m orientation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/projects/energy_per_token/venv/lib/python3.10/site-packages/numpy/lib/_stride_tricks_impl.py:558\u001b[0m, in \u001b[0;36mbroadcast_arrays\u001b[0;34m(subok, *args)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;66;03m# nditer is not used here to avoid the limit of 32 arrays.\u001b[39;00m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;66;03m# Otherwise, something like the following one-liner would suffice:\u001b[39;00m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;66;03m# return np.nditer(args, flags=['multi_index', 'zerosize_ok'],\u001b[39;00m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;66;03m#                  order='C').itviews\u001b[39;00m\n\u001b[1;32m    556\u001b[0m args \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39marray(_m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, subok\u001b[38;5;241m=\u001b[39msubok) \u001b[38;5;28;01mfor\u001b[39;00m _m \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[0;32m--> 558\u001b[0m shape \u001b[38;5;241m=\u001b[39m \u001b[43m_broadcast_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    560\u001b[0m result \u001b[38;5;241m=\u001b[39m [array \u001b[38;5;28;01mif\u001b[39;00m array\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m shape\n\u001b[1;32m    561\u001b[0m           \u001b[38;5;28;01melse\u001b[39;00m _broadcast_to(array, shape, subok\u001b[38;5;241m=\u001b[39msubok, readonly\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    562\u001b[0m                           \u001b[38;5;28;01mfor\u001b[39;00m array \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(result)\n",
      "File \u001b[0;32m~/projects/energy_per_token/venv/lib/python3.10/site-packages/numpy/lib/_stride_tricks_impl.py:433\u001b[0m, in \u001b[0;36m_broadcast_shape\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the shape of the arrays that would result from broadcasting the\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;124;03msupplied arrays against each other.\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;66;03m# use the old-iterator because np.nditer does not handle size 0 arrays\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;66;03m# consistently\u001b[39;00m\n\u001b[0;32m--> 433\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# unfortunately, it cannot handle 32 or more arguments directly\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pos \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;28mlen\u001b[39m(args), \u001b[38;5;241m31\u001b[39m):\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;66;03m# ironically, np.broadcast does not properly handle np.broadcast\u001b[39;00m\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;66;03m# objects (it treats them as scalars)\u001b[39;00m\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;66;03m# use broadcasting to avoid allocating the full array\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: shape mismatch: objects cannot be broadcast to a single shape.  Mismatch is between arg 0 with shape (3,) and arg 1 with shape (0,)."
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to plot the metrics\n",
    "def plot_metrics(metrics, categories):\n",
    "    num_categories = len(categories)\n",
    "\n",
    "    # Prepare the data for plotting\n",
    "    avg_latencies = []\n",
    "    avg_perplexities = []\n",
    "    avg_energy_per_flops = []\n",
    "    avg_energy_per_token = []\n",
    "    avg_energy_per_task = []\n",
    "    avg_accuracy = []\n",
    "    avg_flops = []\n",
    "    avg_energy = []\n",
    "    avg_power = []\n",
    "    avg_perplexity = []\n",
    "\n",
    "\n",
    "    for category in categories:\n",
    "        if category in metrics:\n",
    "            avg_latencies.append(np.mean(metrics[category][\"latencies\"]))\n",
    "            #avg_perplexities.append(np.mean(metrics[category][\"perplexities\"]))\n",
    "            avg_energy_per_flops.append(np.mean(metrics[category][\"energy_per_flops\"]))\n",
    "            avg_energy_per_token.append(np.mean(metrics[category][\"energy_per_token\"]))\n",
    "            avg_energy_per_task.append(np.mean(metrics[category][\"energy_per_task\"]))\n",
    "            avg_accuracy.append(np.mean(metrics[category][\"accuracy\"]))\n",
    "            avg_flops.append(np.mean(metrics[category][\"flopstotal\"]))\n",
    "            avg_energy.append(np.mean(metrics[category][\"energy_over_time\"]))\n",
    "            avg_power.append(np.mean(metrics[category][\"power_over_time\"]))\n",
    "            avg_perplexity.append(np.mean(metrics[category][\"perplexity\"]))\n",
    "\n",
    "        else:\n",
    "            avg_latencies.append(0)\n",
    "            avg_perplexities.append(0)\n",
    "            avg_energy_per_flops.append(0)\n",
    "            avg_energy_per_token.append(0)\n",
    "            avg_energy_per_task.append(0)\n",
    "            avg_accuracy.append(0)\n",
    "            avg_flops.append(0)\n",
    "            avg_energy.append(0)\n",
    "            avg_power.append(0)\n",
    "            avg_perplexity.append(0)\n",
    "            \n",
    "\n",
    "    x = np.arange(num_categories)  # the label locations\n",
    "    width = 0.15  # the width of the bars\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "    # Plot latencies\n",
    "    bars1 = ax1.bar(x - 2*width, avg_latencies, width, label='Average Latency (s)', color='b')\n",
    "    ax1.set_ylabel('Average Latency (s)', color='b')\n",
    "    ax1.tick_params(axis='y', labelcolor='b')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(categories)\n",
    "\n",
    "    # Create a second y-axis for perplexities\n",
    "    ax2 = ax1.twinx()\n",
    "    bars2 = ax2.bar(x - width, avg_perplexities, width, label='Average Perplexity', color='g')\n",
    "    ax2.set_ylabel('Average Perplexity', color='g')\n",
    "    ax2.tick_params(axis='y', labelcolor='g')\n",
    "\n",
    "    # Create a third y-axis for energy per FLOPs\n",
    "    ax3 = ax1.twinx()\n",
    "    bars3 = ax3.bar(x, accuracy, width, label='Accuracy', color='grey')\n",
    "    ax3.spines['right'].set_position(('outward', 60))  # move the third y-axis to the right\n",
    "    ax3.set_ylabel('Accuracy', color='grey')\n",
    "    ax3.tick_params(axis='y', labelcolor='grey')\n",
    "\n",
    "\n",
    "    # Create a third y-axis for energy per FLOPs\n",
    "    ax4 = ax1.twinx()\n",
    "    bars4 = ax4.bar(x + width, avg_energy_per_flops, width, label='Energy per FLOP (Joules)', color='r')\n",
    "    ax4.spines['right'].set_position(('outward', 60))  # move the third y-axis to the right\n",
    "    ax4.set_ylabel('Energy per FLOP (Joules)', color='r')\n",
    "    ax4.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "    # Create a fourth y-axis for energy per token\n",
    "    ax5 = ax1.twinx()\n",
    "    bars5 = ax5.bar(x + 2*width, avg_energy_per_token, width, label='Energy per Token (Joules)', color='purple')\n",
    "    ax5.spines['right'].set_position(('outward', 120))  # move the fourth y-axis to the right\n",
    "    ax5.set_ylabel('Energy per Token (Joules)', color='purple')\n",
    "    ax5.tick_params(axis='y', labelcolor='purple')\n",
    "\n",
    "# Call the plotting function\n",
    "plot_metrics(flop_mmlu_metrics, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x800 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metrics(metrics, categories):\n",
    "    num_categories = len(categories)\n",
    "\n",
    "    # Prepare the data for plotting\n",
    "    avg_latencies = []\n",
    "    avg_perplexities = []\n",
    "    avg_energy_per_flops = []\n",
    "    avg_energy_per_token = []\n",
    "    avg_accuracy = []\n",
    "\n",
    "    # Populate the lists with average values or set to zero if the category is missing\n",
    "    for category in categories:\n",
    "        if category in metrics:\n",
    "            avg_latencies.append(np.mean(metrics[category].get(\"latencies\", [0])))\n",
    "            avg_perplexities.append(np.mean(metrics[category].get(\"perplexity\", [0])))\n",
    "            avg_energy_per_flops.append(np.mean(metrics[category].get(\"energy_per_flops\", [0])))\n",
    "            avg_energy_per_token.append(np.mean(metrics[category].get(\"energy_per_token\", [0])))\n",
    "            avg_accuracy.append(np.mean(metrics[category].get(\"accuracy\", [0])))\n",
    "        else:\n",
    "            avg_latencies.append(0)\n",
    "            avg_perplexities.append(0)\n",
    "            avg_energy_per_flops.append(0)\n",
    "            avg_energy_per_token.append(0)\n",
    "            avg_accuracy.append(0)\n",
    "\n",
    "    x = np.arange(num_categories)  # the label locations\n",
    "    width = 0.15  # the width of the bars\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "    # Plot latencies\n",
    "    bars1 = ax1.bar(x - 2 * width, avg_latencies, width, label='Average Latency (s)', color='b')\n",
    "    ax1.set_ylabel('Average Latency (s)', color='b')\n",
    "    ax1.tick_params(axis='y', labelcolor='b')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(categories)\n",
    "\n",
    "    # Create a second y-axis for perplexities\n",
    "    ax2 = ax1.twinx()\n",
    "    bars2 = ax2.bar(x - width, avg_perplexities, width, label='Average Perplexity', color='g')\n",
    "    ax2.set_ylabel('Average Perplexity', color='g')\n",
    "    ax2.tick_params(axis='y', labelcolor='g')\n",
    "\n",
    "    # Create a third y-axis for accuracy\n",
    "    ax3 = ax1.twinx()\n",
    "    bars3 = ax3.bar(x, avg_accuracy, width, label='Accuracy', color='grey')\n",
    "    ax3.spines['right'].set_position(('outward', 60))\n",
    "    ax3.set_ylabel('Accuracy', color='grey')\n",
    "    ax3.tick_params(axis='y', labelcolor='grey')\n",
    "\n",
    "    # Create a fourth y-axis for energy per FLOPs\n",
    "    ax4 = ax1.twinx()\n",
    "    bars4 = ax4.bar(x + width, avg_energy_per_flops, width, label='Energy per FLOP (Joules)', color='r')\n",
    "    ax4.spines['right'].set_position(('outward', 120))\n",
    "    ax4.set_ylabel('Energy per FLOP (Joules)', color='r')\n",
    "    ax4.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "    # Create a fifth y-axis for energy per token\n",
    "    ax5 = ax1.twinx()\n",
    "    bars5 = ax5.bar(x + 2 * width, avg_energy_per_token, width, label='Energy per Token (Joules)', color='purple')\n",
    "    ax5.spines['right'].set_position(('outward', 180))\n",
    "    ax5.set_ylabel('Energy per Token (Joules)', color='purple')\n",
    "    ax5.tick_params(axis='y', labelcolor='purple')\n",
    "\n",
    "    # Legend for all bars\n",
    "    fig.legend(loc='upper left', bbox_to_anchor=(0.1, 0.9))\n",
    "\n",
    "    plt.title(\"Metrics Across Categories\")\n",
    "    plt.show()\n",
    "\n",
    "# Call the plotting function\n",
    "plot_metrics(flop_mmlu_metrics, categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save metrics to JSON file\n",
    "with open(f\"flop_MMLU_model={model_name.replace('/','-').replace('.', '_')}_maxnewtokens={max_new_tokens}_bootstrapping={bootstrapping}_category={category_text}_metrics.json\", \"w\") as json_file:\n",
    "    json.dump(flop_mmlu_metrics, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the metrics\n",
    "def plot_metrics(metrics, categories):\n",
    "    num_categories = len(categories)\n",
    "\n",
    "    # Prepare the data for plotting\n",
    "    avg_latencies = []\n",
    "    #avg_perplexities = []\n",
    "    avg_energy_per_flops = []\n",
    "    avg_energy_per_token = []\n",
    "    avg_energy_per_task = []\n",
    "\n",
    "    for category in categories:\n",
    "        if category in metrics:\n",
    "            avg_latencies.append(np.mean(metrics[category][\"latencies\"]))\n",
    "            #avg_perplexities.append(np.mean(metrics[category][\"perplexities\"]))\n",
    "            avg_energy_per_flops.append(np.mean(metrics[category][\"energy_per_flops\"]))\n",
    "            avg_energy_per_token.append(np.mean(metrics[category][\"energy_per_token\"]))\n",
    "            avg_energy_per_task.append(np.mean(metrics[category][\"energy_per_task\"]))\n",
    "        else:\n",
    "            avg_latencies.append(0)\n",
    "            #avg_perplexities.append(0)\n",
    "            avg_energy_per_flops.append(0)\n",
    "            avg_energy_per_token.append(0)\n",
    "            avg_energy_per_task.append(0)\n",
    "\n",
    "    x = np.arange(num_categories)  # the label locations\n",
    "    width = 0.15  # the width of the bars\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "    # Plot latencies\n",
    "    bars1 = ax1.bar(x - 2*width, avg_latencies, width, label='Average Latency (s)', color='b')\n",
    "    ax1.set_ylabel('Average Latency (s)', color='b')\n",
    "    ax1.tick_params(axis='y', labelcolor='b')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(categories)\n",
    "\n",
    "    # Create a second y-axis for perplexities\n",
    "    #ax2 = ax1.twinx()\n",
    "    #bars2 = ax2.bar(x - width, avg_perplexities, width, label='Average Perplexity', color='g')\n",
    "    #ax2.set_ylabel('Average Perplexity', color='g')\n",
    "    #ax2.tick_params(axis='y', labelcolor='g')\n",
    "\n",
    "    # Create a third y-axis for energy per FLOPs\n",
    "    ax3 = ax1.twinx()\n",
    "    bars3 = ax3.bar(x, avg_energy_per_flops, width, label='Energy per FLOP (Joules)', color='r')\n",
    "    ax3.spines['right'].set_position(('outward', 60))  # move the third y-axis to the right\n",
    "    ax3.set_ylabel('Energy per FLOP (Joules)', color='r')\n",
    "    ax3.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "    # Create a fourth y-axis for energy per token\n",
    "    ax4 = ax1.twinx()\n",
    "    bars4 = ax4.bar(x + width, avg_energy_per_token, width, label='Energy per Token (Joules)', color='purple')\n",
    "    ax4.spines['right'].set_position(('outward', 120))  # move the fourth y-axis to the right\n",
    "    ax4.set_ylabel('Energy per Token (Joules)', color='purple')\n",
    "    ax4.tick_params(axis='y', labelcolor='purple')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# energy_per_flops with asynchronous energy measuring MMLU quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Science, Technology, Engineering, Mathematics = stem\n",
    "stem = [\"clinical_knowledge\",\n",
    "\"medical_genetics\", \n",
    "\"high_school_physics\",\n",
    "\"virology\",\n",
    "\"high_school_biology\",\n",
    "\"abstract_algebra\",\n",
    "\"professional_medicine\",\n",
    "\"nutrition\",\n",
    "\"machine_learning\",\n",
    "\"anatomy\",\n",
    "\"college_medicine\",\n",
    "\"college_chemistry\",\n",
    "\"elementary_mathematics\",\n",
    "\"human_aging\",\n",
    "\"college_mathematics\",\n",
    "\"high_school_statistics\",\n",
    "\"high_school_mathematics\",\n",
    "\"high_school_computer_science\",\n",
    "\"conceptual_physics\",\n",
    "\"high_school_chemistry\",\n",
    "\"college_physics\",\n",
    "\"electrical_engineering\",\n",
    "\"astronomy\",\n",
    "\"college_biology\",\n",
    "\"computer_security\"]\n",
    "\n",
    "humanities= [\"high_school_european_history\",\n",
    "\"high_school_us_history\",\n",
    "\"high_school_world_history\",\n",
    "\"philosophy\",\n",
    "\"global_facts\",\n",
    "\"security_studies\",\n",
    "\"prehistory\",\n",
    "\"high_school_government_and_politics\",\n",
    "\"logical_fallacies\",\n",
    "\"international_law\",\n",
    "\"jurisprudence\",\n",
    "\"world_religions\",\n",
    "\"us_foreign_policy\",\n",
    "\"moral_scenarios\",\n",
    "\"moral_disputes\"\n",
    "]\n",
    "\n",
    "sociology = [\"sociology\",\n",
    "\"professional_psychology\",\n",
    "\"high_school_psychology\",\n",
    "\"human_sexuality\"]\n",
    "\n",
    "others = [\"business_ethics\",\n",
    "\"high_school_microeconomics\",\n",
    "\"econometrics\",\n",
    "\"professional_accounting\",\n",
    "\"public_relations\",\n",
    "\"marketing\",\n",
    "\"professional_law\",\n",
    "\"management\",\n",
    "\"miscellaneous\",\n",
    "\"high_school_macroeconomics\"]\n",
    "\n",
    "math = [\"abstract_algebra\",\n",
    "\t\"college_mathematics\",\n",
    "\t\"elementary_mathematics\",\n",
    "\t\"high_school_mathematics\",\n",
    "\t\"high_school_statistics\"]\n",
    "\n",
    "computer_science = [\"college_computer_science\",\n",
    "\t\"computer_security\",\n",
    "\t\"high_school_computer_science\",\n",
    "\t\"machine_learning\"]\n",
    "\n",
    "health = [\"anatomy\",\n",
    "\t\"clinical_knowledge\",\n",
    "\t\"college_medicine\",\n",
    "\t\"human_aging\",\n",
    "\t\"medical_genetics\",\n",
    "\t\"nutrition\",\n",
    "\t\"professional_medicine\",\n",
    "\t\"virology\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import threading\n",
    "import torch\n",
    "import pynvml\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from torch.profiler import profile, ProfilerActivity\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import bitsandbytes\n",
    "\n",
    "# Initialize NVML for power measurement\n",
    "def initialize_nvml():\n",
    "    pynvml.nvmlInit()\n",
    "\n",
    "def shutdown_nvml():\n",
    "    pynvml.nvmlShutdown()\n",
    "\n",
    "def get_gpu_handle(gpu_index=0):\n",
    "    return pynvml.nvmlDeviceGetHandleByIndex(gpu_index)\n",
    "\n",
    "def start_power_monitoring(handle, interval_sec=0.1):\n",
    "    power_readings = []\n",
    "    running = True\n",
    "\n",
    "    def monitor():\n",
    "        while running:\n",
    "            power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0  # Convert from mW to W\n",
    "            timestamp = time.time()\n",
    "            power_readings.append((timestamp, power))\n",
    "            time.sleep(interval_sec)\n",
    "\n",
    "    thread = threading.Thread(target=monitor)\n",
    "    thread.start()\n",
    "\n",
    "    def stop():\n",
    "        nonlocal running\n",
    "        running = False\n",
    "        thread.join()\n",
    "\n",
    "    return power_readings, stop\n",
    "\n",
    "# Map generated text to one of the options A, B, C, D\n",
    "def map_generated_text_to_option(generated_text):\n",
    "    valid_options = ['A', 'B', 'C', 'D']\n",
    "    generated_text = generated_text.strip().upper()\n",
    "    if generated_text in valid_options:\n",
    "        return generated_text\n",
    "    #else:\n",
    "        # Attempt to extract the option from the text\n",
    "        #for option in valid_options:\n",
    "            #if option in generated_text:\n",
    "                #return option\n",
    "        # If no valid option is found, return None\n",
    "    return None\n",
    "\n",
    "# Measure energy consumed during inference and FLOPs\n",
    "def measure_energy_during_inference(handle, inference_function, model, inputs, max_new_tokens=1):\n",
    "    # Start power monitoring\n",
    "    power_readings, stop_monitoring = start_power_monitoring(handle, interval_sec=0.05)\n",
    "    \n",
    "    # Start time for inference\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Measure FLOPs using PyTorch profiler\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], with_flops=True, record_shapes=False) as prof:\n",
    "        with torch.no_grad():\n",
    "            result = inference_function(inputs['input_ids'], max_new_tokens=max_new_tokens, do_sample=False, num_beams=1)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Stop power monitoring\n",
    "    stop_monitoring()\n",
    "\n",
    "    # Filter power readings during inference\n",
    "    power_during_inference = [p for t, p in power_readings if start_time <= t <= end_time]\n",
    "\n",
    "    # Calculate average power and energy consumed\n",
    "    if power_during_inference:\n",
    "        avg_power = sum(power_during_inference) / len(power_during_inference)\n",
    "        elapsed_time = end_time - start_time\n",
    "        energy_consumed = avg_power * elapsed_time\n",
    "    else:\n",
    "        avg_power = 0\n",
    "        energy_consumed = 0\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "    # Calculate FLOPs\n",
    "    flops = sum([event.flops for event in prof.key_averages() if event.flops is not None])\n",
    "\n",
    "    return energy_consumed, elapsed_time, flops, result\n",
    "\n",
    "# Load the MMLU dataset for specified categories\n",
    "def load_mmlu_data(categories):\n",
    "    category_dataframes = {}  # Dictionary to store DataFrames for each category\n",
    "        \n",
    "    for category in categories:\n",
    "        print(\"Loading Data for category: \", category)\n",
    "            \n",
    "        # Load the dataset for the given category\n",
    "        mmlu_dataset = load_dataset(\"lukaemon/mmlu\", category, split='validation', trust_remote_code=True)\n",
    "        \n",
    "        # Create a DataFrame for the current category\n",
    "        df_category = pd.DataFrame({\n",
    "            'input': mmlu_dataset['input'],  # The question or prompt\n",
    "            'A': mmlu_dataset['A'],          # Option A\n",
    "            'B': mmlu_dataset['B'],          # Option B\n",
    "            'C': mmlu_dataset['C'],          # Option C\n",
    "            'D': mmlu_dataset['D'],          # Option D\n",
    "            'target': mmlu_dataset['target'] # The correct answer (e.g., 'A', 'B', 'C', 'D')\n",
    "        })\n",
    "        \n",
    "        # Store the DataFrame in the dictionary, with the category as the key\n",
    "        category_dataframes[category] = df_category\n",
    "        \n",
    "    return category_dataframes\n",
    "\n",
    "# Run the experiment for a category in the MMLU dataset\n",
    "def run_experiment_for_mmlu_category(data, bootstrapping, handle, model, tokenizer):\n",
    "    latencies = []\n",
    "    energy_per_token = []\n",
    "    energy_per_flops = []\n",
    "    energy_per_task = []\n",
    "    throughputs = []\n",
    "    generated_texts = []\n",
    "    accuracies = []\n",
    "    \n",
    "    for idx, row in data.iterrows():\n",
    "        # Construct the prompt\n",
    "        prompt = f\"Question: {row['input']}\\nA) {row['A']}\\nB) {row['B']}\\nC) {row['C']}\\nD) {row['D']}\\nAnswer:\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")  # Ensure input is on the same device\n",
    "        text_latencies = []\n",
    "        text_energy_per_token = []\n",
    "        text_energy_per_flops = []\n",
    "        text_energy_per_task = []\n",
    "        text_throughput = []\n",
    "        text_generated = []\n",
    "        correct_predictions = 0  # To calculate accuracy\n",
    "\n",
    "        for _ in range(bootstrapping):\n",
    "            energy_consumed, latency, flops, output = measure_energy_during_inference(\n",
    "                handle, model.generate, model, inputs, max_new_tokens=1\n",
    "            )\n",
    "            text_latencies.append(latency)\n",
    "            output_tokens = output.size(-1) - inputs['input_ids'].size(-1)\n",
    "            energy_token = energy_consumed / output_tokens if output_tokens > 0 else 0\n",
    "            text_energy_per_token.append(energy_token)\n",
    "\n",
    "            energy_flop = energy_consumed / flops if flops > 0 else 0\n",
    "            text_energy_per_flops.append(energy_flop)\n",
    "            text_energy_per_task.append(energy_consumed)\n",
    "            throughput = output_tokens / latency if latency > 0 else 0\n",
    "            text_throughput.append(throughput)\n",
    "\n",
    "            # Decode the generated token\n",
    "            generated_text = tokenizer.decode(output[0][inputs['input_ids'].size(-1):], skip_special_tokens=True)\n",
    "            generated_text = generated_text.strip()\n",
    "            text_generated.append(generated_text)\n",
    "\n",
    "            # Map the generated text to an option\n",
    "            mapped_answer = map_generated_text_to_option(generated_text)\n",
    "            print(f\"Generated answer: '{mapped_answer}' | Correct answer: '{row['target']}'\")\n",
    "            if mapped_answer == row['target']:\n",
    "                correct_predictions += 1\n",
    "\n",
    "        accuracy = correct_predictions / bootstrapping\n",
    "        accuracies.append(accuracy)\n",
    "        latencies.append(text_latencies)\n",
    "        energy_per_token.append(text_energy_per_token)\n",
    "        energy_per_flops.append(text_energy_per_flops)\n",
    "        energy_per_task.append(text_energy_per_task)\n",
    "        throughputs.append(text_throughput)\n",
    "        generated_texts.append(text_generated)\n",
    "\n",
    "    overall_accuracy = np.mean(accuracies)\n",
    "    return latencies, energy_per_token, energy_per_flops, energy_per_task, throughputs, generated_texts, overall_accuracy\n",
    "\n",
    "# Collect metrics for each category\n",
    "def collect_metrics_for_categories(data_dict, categories, bootstrapping, model, tokenizer):\n",
    "    category_metrics = {}\n",
    "    handle = get_gpu_handle(gpu_index=0)\n",
    "\n",
    "    for category in categories:\n",
    "        print(f\"Processing category: {category}\")\n",
    "        data = data_dict[category]\n",
    "        latencies, energy_per_token, energy_per_flops, energy_per_task, throughputs, generated_texts, overall_accuracy = run_experiment_for_mmlu_category(\n",
    "            data, bootstrapping, handle, model, tokenizer\n",
    "        )\n",
    "\n",
    "        category_metrics[category] = {\n",
    "            \"latencies\": latencies,\n",
    "            \"energy_per_token\": energy_per_token,\n",
    "            \"energy_per_flops\": energy_per_flops,\n",
    "            \"energy_per_task\": energy_per_task,\n",
    "            \"throughput\": throughputs,\n",
    "            \"generated_texts\": generated_texts,\n",
    "            \"accuracy\": overall_accuracy\n",
    "        }\n",
    "\n",
    "    shutdown_nvml()  \n",
    "    return category_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = math # math computer_science health\n",
    "category_text = \"math\"\n",
    "\n",
    "# Bootstrapping iterations\n",
    "bootstrapping = 2\n",
    "\n",
    "initialize_nvml()\n",
    "\n",
    "# HF Access Token\n",
    "access_token = \"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\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"NTQAI/Nxcode-CQ-7B-orpo\"  \n",
    "            #\"meta-llama/Llama-3.1-8B\"  \n",
    "            #\"facebook/opt-125m\"\n",
    "            #\"tiiuae/falcon-7b\"\n",
    "            #\"ProbeMedicalYonseiMAILab/medllama3-v20\"\n",
    "            #\"NTQAI/Nxcode-CQ-7B-orpo\"\n",
    "            #\"MathLLMs/MathCoder-L-7B\"\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",  # Specify the quantization type\n",
    "    bnb_4bit_use_double_quant=True,  # Use double quantization if needed\n",
    "    bnb_4bit_compute_dtype=torch.float16  # Specify computation dtype\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", quantization_config=quant_config, token=access_token)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=access_token)\n",
    "\n",
    "# Load MMLU data\n",
    "data_dict = load_mmlu_data(categories)\n",
    "\n",
    "# Collect metrics\n",
    "flop_mmlu_metrics = collect_metrics_for_categories(data_dict, categories, bootstrapping, model, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save metrics to JSON file\n",
    "with open(f\"flop_MMLU_NF4_{model_name.replace('/','-').replace('.', '_')}_bootstrapping={bootstrapping}_category={category_text}_metrics.json\", \"w\") as json_file:\n",
    "    json.dump(flop_mmlu_metrics, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# energy_per_flops\n",
    "\n",
    "# with nvidia nsights - nvtx (does not work yet): youtube:\n",
    "short video: https://www.youtube.com/watch?v=5Gxx59Q0g6o\n",
    "hands on : https://www.youtube.com/watch?v=3DAYN-onSzY\n",
    "\n",
    "\n",
    "multiple different videos available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 77\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Example usage with a loop over your text inputs\u001b[39;00m\n\u001b[1;32m     76\u001b[0m texts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample input 1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample input 2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample input 3\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# Replace with actual text inputs\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m()  \u001b[38;5;66;03m# Your model loading logic\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Running the experiment to collect metrics\u001b[39;00m\n\u001b[1;32m     80\u001b[0m latencies, energy_per_task, energy_per_flop \u001b[38;5;241m=\u001b[39m run_experiment_for_texts(texts, model, handle)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_model' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import nvtx\n",
    "import pynvml\n",
    "from torch.profiler import profile, ProfilerActivity\n",
    "\n",
    "# Initialize NVML for GPU power usage\n",
    "pynvml.nvmlInit()\n",
    "handle = pynvml.nvmlDeviceGetHandleByIndex(0)  # Assuming single GPU\n",
    "\n",
    "# Function to measure GPU energy consumption (in Joules)\n",
    "def get_gpu_energy(handle, duration_sec):\n",
    "    power_draw = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000  # Convert from mW to W\n",
    "    return power_draw * duration_sec  # Energy in Joules\n",
    "\n",
    "# Function to calculate FLOPs using PyTorch's profiler\n",
    "def calculate_flops(model, inputs):\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], profile_memory=True, record_shapes=True) as prof:\n",
    "        with torch.no_grad():\n",
    "            model(inputs)\n",
    "    # Sum the FLOPs from the profiler\n",
    "    flops = sum([event.cpu_time for event in prof.key_averages()])\n",
    "    return flops\n",
    "    \n",
    "def calculate_perplexity(model, input_text, tokenizer):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)  # Ensure input is on the same device\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "        perplexity = torch.exp(loss)\n",
    "    return perplexity.item()\n",
    "\n",
    "# Function to run the experiment for each text (inference)\n",
    "def run_experiment_for_texts(texts, model, handle):\n",
    "    latencies = []\n",
    "    energy_per_task = []\n",
    "    energy_per_flop = []\n",
    "    perplexities = []\n",
    "    \n",
    "    for text in texts:\n",
    "        input_tensor = preprocess_text(text)\n",
    "        \n",
    "        # Start energy and time measurement\n",
    "        start_time = time.time()\n",
    "        nvtx.range_push(\"Task Inference\")\n",
    "\n",
    "        # Measure initial GPU energy\n",
    "        initial_energy = get_gpu_energy(handle, 0)\n",
    "\n",
    "        # Run inference\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "\n",
    "        # End NVTX range\n",
    "        nvtx.range_pop()\n",
    "\n",
    "        # Measure final GPU energy\n",
    "        end_time = time.time()\n",
    "        final_energy = get_gpu_energy(handle, end_time - start_time)\n",
    "\n",
    "        # Calculate task energy consumption\n",
    "        task_energy = final_energy - initial_energy\n",
    "\n",
    "        # Measure latency\n",
    "        latency = end_time - start_time\n",
    "        latencies.append(latency)\n",
    "\n",
    "        # Calculate FLOPs for this task\n",
    "        task_flops = calculate_flops(model, input_tensor)\n",
    "\n",
    "        # Calculate Energy per Task (Joules per task)\n",
    "        energy_per_task.append(task_energy)\n",
    "\n",
    "        # Calculate Energy per FLOP (Joules per FLOP)\n",
    "        if task_flops > 0:\n",
    "            energy_per_flop.append(task_energy / task_flops)\n",
    "        else:\n",
    "            energy_per_flop.append(np.nan)  # Handle case where FLOP estimation is zero\n",
    "        \n",
    "    return latencies, energy_per_task, energy_per_flop\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# chat gpt:\n",
    "# Example usage with a loop over your text inputs\n",
    "texts = [\"Sample input 1\", \"Sample input 2\", \"Sample input 3\"]  # Replace with actual text inputs\n",
    "model = load_model()  # Your model loading logic\n",
    "\n",
    "# Running the experiment to collect metrics\n",
    "latencies, energy_per_task, energy_per_flop = run_experiment_for_texts(texts, model, handle)\n",
    "\n",
    "# Example output\n",
    "for i, text in enumerate(texts):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Latency: {latencies[i]:.4f} seconds\")\n",
    "    print(f\"Energy per Task: {energy_per_task[i]:.4f} Joules\")\n",
    "    print(f\"Energy per FLOP: {energy_per_flop[i]:.4e} Joules/FLOP\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Clean up NVML\n",
    "pynvml.nvmlShutdown()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MT_Bench with vLLM\n",
    "\n",
    "\n",
    "### Das funktioniert noch nicht mit vLLM!!!  out of memory error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Patrick/vllmenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-09-26 16:22:31,750\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import transformers\n",
    "import accelerate\n",
    "import vllm\n",
    "import bitsandbytes\n",
    "from vllm import LLM, SamplingParams\n",
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib\n",
    "from collections import Counter\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "#matplotlib.use('TkAgg')\n",
    "#from awq import AutoAWQForCausalLM\n",
    "#from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MMLU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Science, Technology, Engineering, Mathematics = stem\n",
    "stem = [\"clinical_knowledge\",\n",
    "\"medical_genetics\", \n",
    "\"high_school_physics\",\n",
    "\"virology\",\n",
    "\"high_school_biology\",\n",
    "\"abstract_algebra\",\n",
    "\"professional_medicine\",\n",
    "\"nutrition\",\n",
    "\"machine_learning\",\n",
    "\"anatomy\",\n",
    "\"college_medicine\",\n",
    "\"college_chemistry\",\n",
    "\"elementary_mathematics\",\n",
    "\"human_aging\",\n",
    "\"college_mathematics\",\n",
    "\"high_school_statistics\",\n",
    "\"high_school_mathematics\",\n",
    "\"high_school_computer_science\",\n",
    "\"conceptual_physics\",\n",
    "\"high_school_chemistry\",\n",
    "\"college_physics\",\n",
    "\"electrical_engineering\",\n",
    "\"astronomy\",\n",
    "\"college_biology\",\n",
    "\"computer_security\"]\n",
    "\n",
    "humanities= [\"high_school_european_history\",\n",
    "\"high_school_us_history\",\n",
    "\"high_school_world_history\",\n",
    "\"philosophy\",\n",
    "\"global_facts\",\n",
    "\"security_studies\",\n",
    "\"prehistory\",\n",
    "\"high_school_government_and_politics\",\n",
    "\"logical_fallacies\",\n",
    "\"international_law\",\n",
    "\"jurisprudence\",\n",
    "\"world_religions\",\n",
    "\"us_foreign_policy\",\n",
    "\"moral_scenarios\",\n",
    "\"moral_disputes\"\n",
    "]\n",
    "\n",
    "sociology = [\"sociology\",\n",
    "\"professional_psychology\",\n",
    "\"high_school_psychology\",\n",
    "\"human_sexuality\"]\n",
    "\n",
    "others = [\"business_ethics\",\n",
    "\"high_school_microeconomics\",\n",
    "\"econometrics\",\n",
    "\"professional_accounting\",\n",
    "\"public_relations\",\n",
    "\"marketing\",\n",
    "\"professional_law\",\n",
    "\"management\",\n",
    "\"miscellaneous\",\n",
    "\"high_school_macroeconomics\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "def convert_to_dataframe(categories):\n",
    "    category_dataframes = {}  # Dictionary to store DataFrames for each category\n",
    "    \n",
    "    for category in categories:\n",
    "        print(\"Loading Data for category: \", category)\n",
    "        \n",
    "        # Load the dataset for the given category\n",
    "        mmlu_dataset = load_dataset(\"lukaemon/mmlu\", category, split='validation', trust_remote_code=True)\n",
    "        \n",
    "        # Create a DataFrame for the current category\n",
    "        df_category = pd.DataFrame({\n",
    "            'input': mmlu_dataset['input'],  # The question or prompt\n",
    "            'A': mmlu_dataset['A'],          # Option A\n",
    "            'B': mmlu_dataset['B'],          # Option B\n",
    "            'C': mmlu_dataset['C'],          # Option C\n",
    "            'D': mmlu_dataset['D'],          # Option D\n",
    "            'target': mmlu_dataset['target'] # The correct answer (e.g., 'A', 'B', 'C', 'D')\n",
    "        })\n",
    "        \n",
    "        # Store the DataFrame in the dictionary, with the category as the key\n",
    "        category_dataframes[category] = df_category\n",
    "    \n",
    "    return category_dataframes\n",
    "\n",
    "# Access a DataFrame for a specific category\n",
    "#print(category_dfs.head())  # Example for checking 'high_school_biology'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "stem, humanities ..etc\n",
    "\"\"\"\n",
    "\n",
    "# Example list of categories\n",
    "categories = ['high_school_biology', 'abstract_algebra', 'professional_medicine', 'nutrition']\n",
    "\n",
    "# Call the function and get the dictionary of DataFrames\n",
    "category_dfs = convert_to_dataframe(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from datasets import load_dataset\n",
    "\n",
    "#cataegory = \"sociology\"\n",
    "# Load the MMLU dataset\n",
    "#def load_mmlu_dataset():\n",
    "#    mmlu_dataset = load_dataset(\"lukaemon/mmlu\",category, split='validation',trust_remote_code=True)\n",
    "#    return mmlu_dataset\n",
    "\n",
    "\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\", device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")\n",
    "\n",
    "def get_gpu_power(gpu_index=1): #measuring for gpu1\n",
    "    \"\"\"Fetches the current power consumption of the GPU using nvidia-smi.\"\"\"\n",
    "    result = subprocess.run(['nvidia-smi', '--query-gpu=power.draw', '--format=csv,noheader,nounits', '-i',str(gpu_index)], \n",
    "                            stdout=subprocess.PIPE, text=True)\n",
    "    power = float(result.stdout.strip())  # Power in watts\n",
    "    return power\n",
    "\n",
    "def convert_to_dataframe(categories):\n",
    "    category_dataframes = {}  # Dictionary to store DataFrames for each category\n",
    "    \n",
    "    for category in categories:\n",
    "        print(\"Loading Data for category: \", category)\n",
    "        \n",
    "        # Load the dataset for the given category\n",
    "        mmlu_dataset = load_dataset(\"lukaemon/mmlu\",category, split='validation',trust_remote_code=True)\n",
    "        #print(type(mmlu_dataset))\n",
    "        # Create a DataFrame for the current category\n",
    "        df_category = pd.DataFrame({\n",
    "            'input': mmlu_dataset['input'],  # The question or prompt\n",
    "            'A': mmlu_dataset['A'],          # Option A\n",
    "            'B': mmlu_dataset['B'],          # Option B\n",
    "            'C': mmlu_dataset['C'],          # Option C\n",
    "            'D': mmlu_dataset['D'],          # Option D\n",
    "            'target': mmlu_dataset['target'] # The correct answer (e.g., 'A', 'B', 'C', 'D')\n",
    "        })\n",
    "        \n",
    "        # Store the DataFrame in the dictionary, with the category as the key\n",
    "        category_dataframes[category] = df_category\n",
    "    print(\"loading_ data finish\")\n",
    "    return category_dataframes\n",
    "\n",
    "# Filter dataset by category\n",
    "#def filter_texts_by_category(df, category):\n",
    "#    return df[df['category'] == category]['text'].values\n",
    "\n",
    "# Filter dataset by subject category (e.g., 'high_school_biology', 'abstract_algebra')\n",
    "def filter_dict_by_category(df, category):\n",
    "    return df[category] \n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "# Run the bootstrapping experiment for multiple-choice questions in a given category\n",
    "def run_experiment_for_texts(datadictionary,categories, bootstrapping ):\n",
    "    category_latencies = []\n",
    "    category_energy_per_token = []\n",
    "    \n",
    "    \n",
    "    #print(categories)\n",
    "    category_accuracy= []\n",
    "\n",
    "\n",
    "\n",
    "    # task in one category\n",
    "    for category in categories:\n",
    "        data = datadictionary[category]#filter_dict_by_category(datadictionary, category)\n",
    "        question_text = data['input'].values\n",
    "        choices = [data['A'].values, data['B'].values, data['C'].values, data['D'].values]\n",
    "        correct_answer = data['target'].values\n",
    "        \n",
    "        task_accuracy= []\n",
    "        task_latencies = []\n",
    "        task_energy_per_token = []\n",
    "        #print(\" type question_text: \", question_text)\n",
    "        #print(\"len question_text: \", len(question_text))\n",
    "        print(\"processing category: \", category)\n",
    "\n",
    "\n",
    "        # Prompts of one tasks\n",
    "        for i, tasks in enumerate (question_text):\n",
    "\n",
    "\n",
    "\n",
    "            print(\"i : \", i)\n",
    "            # Concatenate question with options for LLM input\n",
    "            full_input = f\"Question: {question_text[i]}\\nA) {choices[0][i]}\\nB) ,{choices[1][i]}\\nC) ,{choices[2][i]}\\nD) ,{choices[3][i]}\"\n",
    "            #print(\"full input: \", full_input)\n",
    "            inputs = tokenizer(full_input, return_tensors=\"pt\").to(\"cuda\")  # Prepare input tensors\n",
    "\n",
    "            text_latencies = []\n",
    "            text_energy_per_token = []\n",
    "            correct_predictions = 0  # To calculate accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # Prompt Bootstrapping \n",
    "            for _ in range(bootstrapping):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                print(_)\n",
    "                power_start = get_gpu_power()  # Assuming a function to get GPU power\n",
    "                start_time = time.time()\n",
    "\n",
    "                # Generate the model's response\n",
    "                output = model.generate(inputs['input_ids'], max_new_tokens=200, do_sample=False)  # Adjust tokens if necessary\n",
    "\n",
    "                end_time = time.time()\n",
    "                power_end = get_gpu_power()\n",
    "\n",
    "                # Measure latency\n",
    "                latency = end_time - start_time\n",
    "                text_latencies.append(latency)\n",
    "\n",
    "                # Calculate energy consumption\n",
    "                avg_power = (power_start + power_end) / 2\n",
    "                energy = avg_power * latency\n",
    "\n",
    "                # Token count from output (assuming a tensor output)\n",
    "                output_tokens = output[0].shape[0]\n",
    "                energy_token = energy / output_tokens if output_tokens > 0 else 0\n",
    "                text_energy_per_token.append(energy_token)\n",
    "\n",
    "                # Decode the model's generated answer (you might need to adjust based on model output format)\n",
    "                generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "                # Check if the model's generated answer matches the correct answer\n",
    "                if correct_answer[i] in generated_text:\n",
    "                    correct_predictions += 1\n",
    "\n",
    "            task_latencies.append(np.mean(text_latencies))\n",
    "            task_energy_per_token.append(np.mean(text_energy_per_token))\n",
    "            accuracy = correct_predictions / bootstrapping \n",
    "            task_accuracy.append(accuracy)\n",
    "    category_accuracy.append(np.array(task_accuracy)/len(question_text))\n",
    "    return task_latencies, task_energy_per_token, category_accuracy\n",
    "\n",
    "\n",
    "# Store and collect metrics for each category\n",
    "# Collect metrics for each category\n",
    "def collect_metrics_for_categories(df, categories, bootstrapping):\n",
    "    category_metrics = {}\n",
    "\n",
    "    \"\"\"    for category in categories:\n",
    "            print(f\"Processing category: {category}\")\n",
    "            texts = filter_texts_by_category(df, category)\n",
    "\n",
    "            if texts.empty:\n",
    "                print(f\"No texts found for category {category}\")\n",
    "                continue\n",
    "    \"\"\"\n",
    "    latencies, energy_per_token, accuracy = run_experiment_for_texts(data_dict, categories, bootstrapping)\n",
    "\n",
    "    # Store metrics for each category\n",
    "    category_metrics[category] = {\n",
    "        \"latencies\": latencies,\n",
    "        \"energy_per_token\": energy_per_token,\n",
    "        \"accuracy\": accuracy\n",
    "    }\n",
    "\n",
    "    return category_metrics\n",
    "\n",
    "# Plot the energy consumption per token comparison\n",
    "def plot_energy_vs_latency(metrics, categories):\n",
    "    for category in categories:\n",
    "        category_data = metrics[category]\n",
    "        energy_per_token = category_data[\"energy_per_token\"]\n",
    "        latencies = category_data[\"latencies\"]\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "\n",
    "        # Plot energy per token\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot([sum(x)/len(x) for x in energy_per_token], marker='o', color='blue', label='Energy per Token (J)')\n",
    "        plt.title(f\"Energy per Token for {category}\")\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Energy (J)')\n",
    "        plt.legend()\n",
    "\n",
    "        # Plot latencies\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot([sum(x)/len(x) for x in latencies], marker='o', color='green', label='Latency (s)')\n",
    "        plt.title(f\"Latency for {category}\")\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Latency (s)')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    #plot_energy_vs_latency(metrics, categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Processing data\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "categories = stem\n",
    "categories = sociology  #sociology is shorter / less prompts\n",
    "#mmlu_dataset = load_mmlu_dataset()\n",
    "data_dict = convert_to_dataframe(categories)\n",
    "print(\"data_dict geladen\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run Experiments\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "bootstrapping = 3  # Number of iterations for each prompt\n",
    "\n",
    "# Collect metrics for each category\n",
    "metrics = collect_metrics_for_categories(data_dict, categories, bootstrapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BACK UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import pynvml\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch.profiler\n",
    "\n",
    "# Specify the GPU device\n",
    "device = \"cuda:0\"  # Change to the appropriate GPU if needed\n",
    "\n",
    "# Initialize NVML for power measurement\n",
    "def initialize_nvml():\n",
    "    pynvml.nvmlInit()\n",
    "\n",
    "def shutdown_nvml():\n",
    "    pynvml.nvmlShutdown()\n",
    "\n",
    "def get_gpu_handle(gpu_index=0):\n",
    "    return pynvml.nvmlDeviceGetHandleByIndex(gpu_index)\n",
    "\n",
    "# Measure GPU power consumption over a period of time\n",
    "def measure_power_consumption(handle, duration_sec=1.0, interval_sec=0.1):\n",
    "    power_readings = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    while (time.time() - start_time) < duration_sec:\n",
    "        power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0  # Convert from mW to W\n",
    "        power_readings.append(power)\n",
    "        time.sleep(interval_sec)\n",
    "    \n",
    "    return sum(power_readings) / len(power_readings) if power_readings else 0\n",
    "\n",
    "\n",
    "#inputs['input_ids'], max_new_tokens=200, do_sample=True\n",
    "# Measure energy consumed during inference\n",
    "def measure_energy_during_inference(handle, inference_function):\n",
    "    power_start = measure_power_consumption(handle, duration_sec=0.5)  # Measure power before inference\n",
    "    \n",
    "    start_time = time.time()  # Start time for inference\n",
    "    result = inference_function()  # Run inference\n",
    "    end_time = time.time()  # End time for inference\n",
    "    \n",
    "    power_end = measure_power_consumption(handle, duration_sec=0.5)  # Measure power after inference\n",
    "    \n",
    "    # Average the power readings before and after inference\n",
    "    avg_power = (power_start + power_end) / 2\n",
    "    elapsed_time = end_time - start_time  # Total inference time\n",
    "    energy_consumed = avg_power * elapsed_time  # Energy consumed in Joules\n",
    "\n",
    "    return energy_consumed, elapsed_time, result\n",
    "\n",
    "# Count FLOPs using PyTorch Profiler\n",
    "def count_flops(model, input_ids):\n",
    "    # Use torch.profiler to count FLOPs\n",
    "    with torch.profiler.profile(\n",
    "        activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],\n",
    "        with_stack=True,\n",
    "        record_shapes=True,\n",
    "    ) as prof:\n",
    "        with torch.no_grad():\n",
    "            model.generate(input_ids)\n",
    "\n",
    "    # Sum the FLOPs from profiler events\n",
    "    flops = sum(event.flops for event in prof.events() if event.flops is not None)\n",
    "    return flops\n",
    "\n",
    "# Calculate perplexity for generated text\n",
    "def calculate_perplexity(model, input_text, tokenizer):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)  # Tokenize input and move to device\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "        perplexity = torch.exp(loss)\n",
    "    return perplexity.item()\n",
    "\n",
    "# Main function to run experiments and measure energy & FLOPs\n",
    "def run_experiment_for_texts(texts, bootstrapping, handle, model, tokenizer):\n",
    "    latencies = []\n",
    "    energy_per_task = []\n",
    "    energy_per_flop = []\n",
    "    generated_texts = []\n",
    "    perplexities = []\n",
    "\n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\").to(device)  # Tokenize and move input to device\n",
    "        text_latencies = []\n",
    "        text_energy_per_task = []\n",
    "        text_energy_per_flop = []\n",
    "        text_generated = []\n",
    "        text_perplexities = []\n",
    "\n",
    "        for _ in range(bootstrapping):\n",
    "            # Count FLOPs for the input\n",
    "            flops = count_flops(model, inputs['input_ids'])\n",
    "\n",
    "            # Measure energy during inference\n",
    "            energy_consumed, latency, output = measure_energy_during_inference(\n",
    "                handle, model.generate(inputs['input_ids'], max_new_tokens=200, do_sample=True)\n",
    "            )\n",
    "            text_latencies.append(latency)\n",
    "\n",
    "            # Energy per Task\n",
    "            text_energy_per_task.append(energy_consumed)\n",
    "\n",
    "            # Energy per FLOP\n",
    "            energy_flop = energy_consumed / flops if flops > 0 else 0\n",
    "            text_energy_per_flop.append(energy_flop)\n",
    "\n",
    "            # Generate text\n",
    "            generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            filtered_generated_text = generated_text.replace(text, \"\").strip()\n",
    "            text_generated.append(filtered_generated_text)\n",
    "\n",
    "            # Calculate perplexity\n",
    "            perplexity = calculate_perplexity(model, text, tokenizer)\n",
    "            text_perplexities.append(perplexity)\n",
    "\n",
    "        latencies.append(text_latencies)\n",
    "        energy_per_task.append(text_energy_per_task)\n",
    "        energy_per_flop.append(text_energy_per_flop)\n",
    "        generated_texts.append(text_generated)\n",
    "        perplexities.append(text_perplexities)\n",
    "\n",
    "    return latencies, energy_per_task, energy_per_flop, generated_texts, perplexities\n",
    "\n",
    "# Function to collect metrics for multiple text categories\n",
    "def collect_metrics_for_categories(df, categories, bootstrapping, model, tokenizer):\n",
    "    category_metrics = {}\n",
    "    handle = get_gpu_handle(gpu_index=0)\n",
    "\n",
    "    for category in categories:\n",
    "        print(f\"Processing category: {category}\")\n",
    "        texts = filter_texts_by_category(df, category)\n",
    "        latencies, energy_per_task, energy_per_flop, generated_texts, perplexities = run_experiment_for_texts(\n",
    "            texts, bootstrapping, handle, model, tokenizer\n",
    "        )\n",
    "        \n",
    "\n",
    "        category_metrics[category] = {\n",
    "            \"latencies\": latencies,\n",
    "            \"energy_per_task\": energy_per_task,\n",
    "            \"energy_per_flop\": energy_per_flop,\n",
    "            \"generated_texts\": generated_texts,\n",
    "            \"perplexities\": perplexities,\n",
    "        }\n",
    "\n",
    "    shutdown_nvml()  # Close NVML after measurements\n",
    "    return category_metrics\n",
    "\n",
    "# Function to filter texts based on category\n",
    "def filter_texts_by_category(df, category):\n",
    "    return df[df['category'] == category]['text'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
