{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MT_Bench dataset\n",
    "import numpy as np\n",
    "import transformers\n",
    "import accelerate\n",
    "#import vllm\n",
    "import bitsandbytes\n",
    "#from vllm import LLM, SamplingParams\n",
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib\n",
    "from collections import Counter\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import threading\n",
    "import torch\n",
    "import pynvml\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.profiler import profile, ProfilerActivity\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import gc\n",
    "\n",
    "\n",
    "# Initialize NVML for power measurement\n",
    "def initialize_nvml():\n",
    "    pynvml.nvmlInit()\n",
    "\n",
    "def shutdown_nvml():\n",
    "    pynvml.nvmlShutdown()\n",
    "\n",
    "def get_gpu_handle(gpu_index=0):\n",
    "    return pynvml.nvmlDeviceGetHandleByIndex(gpu_index)\n",
    "\n",
    "def start_power_monitoring(handle, interval_sec=0.1):\n",
    "    power_readings = []\n",
    "    running = True\n",
    "\n",
    "    def monitor():\n",
    "        while running:\n",
    "            power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0  # Convert from mW to W\n",
    "            timestamp = time.time()\n",
    "            power_readings.append((timestamp, power))\n",
    "            time.sleep(interval_sec)\n",
    "\n",
    "    thread = threading.Thread(target=monitor)\n",
    "    thread.start()\n",
    "\n",
    "    def stop():\n",
    "        nonlocal running\n",
    "        running = False\n",
    "        thread.join()\n",
    "\n",
    "    return power_readings, stop\n",
    "\n",
    "\n",
    "\n",
    "def calculate_perplexity1(model, inputs):\n",
    "    #inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)  # Ensure input is on the same device\n",
    "    with torch.no_grad():\n",
    "        outputs = model( labels=inputs)\n",
    "        loss = outputs.loss\n",
    "        perplexity = torch.exp(loss)\n",
    "    return perplexity.item()\n",
    "\n",
    "\n",
    "def calculate_perplexity(model, inputs, attention_mask=None):\n",
    "    # Assume `inputs` is a tensor directly containing input_ids\n",
    "    input_ids = inputs  # Directly use inputs if it's a tensor\n",
    "    labels = input_ids.clone()  # Copy input_ids to use as labels\n",
    "\n",
    "   # print(attention_mask)\n",
    "    with torch.no_grad():\n",
    "        # Pass input_ids and optionally attention_mask to the model\n",
    "        if attention_mask is not None:\n",
    "            outputs = model(input_ids=input_ids, labels=labels)\n",
    "        else:\n",
    "            outputs = model(input_ids=input_ids, labels=labels)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        perplexity = torch.exp(loss)\n",
    "    \n",
    "    return perplexity.item()\n",
    "\n",
    "\n",
    "\n",
    "# Measure energy consumed during inference and FLOPs\n",
    "def measure_energy_during_inference(handle, inference_function, model, inputs, max_new_tokens=1):\n",
    "    #print(f\"tokens: {max_new_tokens}\")\n",
    "    \n",
    "    # Start power monitoring\n",
    "    power_readings, stop_monitoring = start_power_monitoring(handle, interval_sec=0.05)\n",
    "    #attention_mask=inputs['attention_mask']\n",
    "    # Start time for inference\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Measure FLOPs using PyTorch profiler\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], with_flops=True, record_shapes=False) as prof:\n",
    "        with torch.no_grad():\n",
    "            result = inference_function(inputs['input_ids'],max_new_tokens=max_new_tokens, do_sample=False )#num_beams=1)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Stop power monitoring\n",
    "    stop_monitoring()\n",
    "\n",
    "    # Filter power readings during inference\n",
    "    power_during_inference = [p for t, p in power_readings if start_time <= t <= end_time]\n",
    "\n",
    "    # Calculate average power and energy consumed\n",
    "    if power_during_inference:\n",
    "        avg_power = sum(power_during_inference) / len(power_during_inference)\n",
    "        elapsed_time = end_time - start_time\n",
    "        energy_consumed = avg_power * elapsed_time\n",
    "    else:\n",
    "        avg_power = 0\n",
    "        energy_consumed = 0\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "    # Calculate FLOPs\n",
    "    flops = sum([event.flops for event in prof.key_averages() if event.flops is not None])\n",
    "\n",
    "    perplexity = calculate_perplexity(model, inputs['input_ids'])\n",
    "\n",
    "    return energy_consumed, elapsed_time, flops, result, power_during_inference, perplexity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Map generated text to the corresponding option in ARC\n",
    "def map_generated_text_to_option(generated_text, choices):\n",
    "    # ARC choices are usually labeled as 'A', 'B', 'C', 'D'\n",
    "    option_map = {str(idx): choice for idx, choice in enumerate(choices)}\n",
    "    if generated_text in option_map:\n",
    "        return option_map[generated_text]\n",
    "    return None\n",
    "\n",
    "# Load the ARC dataset\n",
    "def load_arc_data(subset=\"ARC-Challenge\"):\n",
    "    arc_dataset = load_dataset(\"ai2_arc\", subset, split=\"test\")  # Load test split for evaluation\n",
    "    df_arc = pd.DataFrame({\n",
    "        'question': arc_dataset['question'],   # The question text\n",
    "        'choices': arc_dataset['choices']['text'],   # List of possible answer choices\n",
    "        'answer_key': arc_dataset['answerKey']       # The correct answer (e.g., 'A', 'B', 'C', 'D')\n",
    "    })\n",
    "    return df_arc\n",
    "\n",
    "# Run experiment for ARC dataset\n",
    "def run_experiment_for_arc(data, bootstrapping, handle, model, tokenizer, max_new_tokens):\n",
    "    latencies = []\n",
    "    energy_per_token = []\n",
    "    energy_per_flops = []\n",
    "    energy_per_task = []\n",
    "    throughputs = []\n",
    "    generated_texts = []\n",
    "    accuracies = []\n",
    "    flopslisttotal = []\n",
    "    energy_over_time = []\n",
    "    power_over_time = []\n",
    "    perplexities = []\n",
    "    \n",
    "    model.eval()\n",
    "    for idx, row in data.iterrows():\n",
    "        # Construct the prompt\n",
    "        prompt = f\"Question: {row['question']}\\n\"\n",
    "        for i, choice in enumerate(row['choices']):\n",
    "            prompt += f\"{i}) {choice}\\n\"\n",
    "        prompt += \"Answer:\"\n",
    "\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")  # Ensure input is on the same device\n",
    "        text_latencies = []\n",
    "        text_energy_per_token = []\n",
    "        text_energy_per_flops = []\n",
    "        text_energy_per_task = []\n",
    "        text_throughput = []\n",
    "        text_generated = []\n",
    "        correct_predictions = 0  # To calculate accuracy\n",
    "        floplist = []\n",
    "        energy = []\n",
    "        power_inf = []\n",
    "        perplexity_prompt = []\n",
    "\n",
    "        for _ in range(bootstrapping):\n",
    "            energy_consumed, latency, flops, output, power_during_inference, perplexity = measure_energy_during_inference(\n",
    "                handle, model.generate, model, inputs, max_new_tokens=max_new_tokens\n",
    "            )\n",
    "            perplexity_prompt.append(perplexity)\n",
    "            power_inf.append(power_during_inference)\n",
    "            energy.append(energy_consumed)\n",
    "            text_latencies.append(latency)\n",
    "            output_tokens = output.size(-1) - inputs['input_ids'].size(-1)\n",
    "            energy_token = energy_consumed / output_tokens if output_tokens > 0 else 0\n",
    "            text_energy_per_token.append(energy_token)\n",
    "\n",
    "            energy_flop = energy_consumed / flops if flops > 0 else 0\n",
    "            text_energy_per_flops.append(energy_flop)\n",
    "            text_energy_per_task.append(energy_consumed)\n",
    "            throughput = output_tokens / latency if latency > 0 else 0\n",
    "            text_throughput.append(throughput)\n",
    "\n",
    "            # Decode the generated token\n",
    "            generated_text = tokenizer.decode(output[0][inputs['input_ids'].size(-1):], skip_special_tokens=True)\n",
    "            generated_text = generated_text.strip()\n",
    "            text_generated.append(generated_text)\n",
    "\n",
    "            floplist.append(flops)\n",
    "            \n",
    "            # Map the generated text to an option and check correctness\n",
    "            mapped_answer = map_generated_text_to_option(generated_text, row['choices'])\n",
    "            if mapped_answer == row['answer_key']:\n",
    "                correct_predictions += 1\n",
    "\n",
    "        # Append bootstrapping metrics\n",
    "        perplexities.append(perplexity_prompt)\n",
    "        power_over_time.append(power_inf)\n",
    "        energy_over_time.append(energy)\n",
    "        flopslisttotal.append(floplist)\n",
    "        accuracy = correct_predictions / bootstrapping\n",
    "        accuracies.append(accuracy)\n",
    "        latencies.append(text_latencies)\n",
    "        energy_per_token.append(text_energy_per_token)\n",
    "        energy_per_flops.append(text_energy_per_flops)\n",
    "        energy_per_task.append(text_energy_per_task)\n",
    "        throughputs.append(text_throughput)\n",
    "        generated_texts.append(text_generated)\n",
    "\n",
    "    overall_accuracy = np.mean(accuracies)\n",
    "    return (latencies, energy_per_token, energy_per_flops, energy_per_task, throughputs,\n",
    "            generated_texts, overall_accuracy, flopslisttotal, energy_over_time, power_over_time, perplexities)\n",
    "\n",
    "# Collect metrics for ARC\n",
    "def collect_metrics_for_arc(data, bootstrapping, model, tokenizer, max_new_tokens):\n",
    "    handle = get_gpu_handle(gpu_index=0)\n",
    "\n",
    "    # Run the experiment\n",
    "    metrics = run_experiment_for_arc(\n",
    "        data, bootstrapping, handle, model, tokenizer, max_new_tokens\n",
    "    )\n",
    "\n",
    "    # Store metrics in a dictionary\n",
    "    arc_metrics = {\n",
    "        \"latencies\": metrics[0],\n",
    "        \"energy_per_token\": metrics[1],\n",
    "        \"energy_per_flops\": metrics[2],\n",
    "        \"energy_per_task\": metrics[3],\n",
    "        \"throughput\": metrics[4],\n",
    "        \"generated_texts\": metrics[5],\n",
    "        \"accuracy\": metrics[6],\n",
    "        \"flopstotal\": metrics[7],\n",
    "        \"energy_over_time\": metrics[8],\n",
    "        \"power_over_time\": metrics[9],\n",
    "        \"perplexity\": metrics[10]\n",
    "    }\n",
    "\n",
    "    shutdown_nvml()  # Ensure GPU monitoring is shut down\n",
    "    return arc_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categories = semanticdifferent # math computer_science health semanticdifferent\n",
    "#categories = [math, economics, computer_science, natural_sciences, health, humanities, sociology, engineering]\n",
    "categories = [humanities_reverse, economics1, health_1manupulated]\n",
    "#categories = [math,computer_science]\n",
    "\n",
    "#category_text = \"math\"\n",
    "\n",
    "# Bootstrapping iterations\n",
    "bootstrapping = 1\n",
    "# max new output tokens\n",
    "max_new_tokens = 1\n",
    "\n",
    "#initialize_nvml()\n",
    "\n",
    "#huggingface-cli login\n",
    "# HF Access Token\n",
    "access_token = \"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\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = [#'facebook/opt-125m',\n",
    "            \"meta-llama/Llama-3.1-8B\",  \n",
    "            \"meta-llama/Llama-3.2-1B\",\n",
    "            \"meta-llama/Llama-3.2-3B\",\n",
    "            \"tiiuae/falcon-7b\",\n",
    "            \"tiiuae/falcon-rw-1b\",\n",
    "            \"lmsys/vicuna-7b-v1.5\"\n",
    "            #\"ProbeMedicalYonseiMAILab/medllama3-v20\",\n",
    "            #\"NTQAI/Nxcode-CQ-7B-orpo\",\n",
    "            #\"MathLLMs/MathCoder-L-7B\"\n",
    "        ]\n",
    "#import os\n",
    "counter = 0\n",
    "Model_metrics_for_categories = []\n",
    "for models in model_name:\n",
    "# Load MMLU data\n",
    "    #print(i)\n",
    "    #counter +=1\n",
    "    model = AutoModelForCausalLM.from_pretrained(models, device_map=\"auto\", use_auth_token=access_token)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(models, use_auth_token=access_token)\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    #print(data_dict.keys())\n",
    "    allmetrics = []\n",
    "    for category in categories:\n",
    "        data_dict = load_mmlu_data(category)\n",
    "        initialize_nvml()\n",
    "        #print(models)\n",
    "        #model = AutoModelForCausalLM.from_pretrained(models, use_auth_token=access_token)\n",
    "\n",
    "        #model = AutoModelForCausalLM.from_pretrained(models, device_map=\"auto\", use_auth_token=access_token)\n",
    "        #tokenizer = AutoTokenizer.from_pretrained(models, use_auth_token=access_token)\n",
    "\n",
    "        #tokenizer.pad_token = tokenizer.eos_token\n",
    "        #tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "        flop_mmlu_metrics = collect_metrics_for_categories(data_dict, category, bootstrapping, model, tokenizer, max_new_tokens)\n",
    "        allmetrics.append(flop_mmlu_metrics)\n",
    "    \n",
    "    with open(f\"{models.replace('/','-').replace('.', '_')}_bootstrapping={bootstrapping}_ARC.json\", \"w\") as json_file:\n",
    "        json.dump(allmetrics, json_file)\n",
    "    del model\n",
    "    del tokenizer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    \n",
    "    #for i, metrics in enumerate(allmetrics):\n",
    "        # Create a filename for each model's metrics\n",
    "    Model_metrics_for_categories.append(allmetrics)\n",
    "'''\n",
    "counter = 0\n",
    "allmetrics = []\n",
    "for category in categories:\n",
    "# Load MMLU data\n",
    "    #print(i)\n",
    "    counter +=1\n",
    "    data_dict = load_mmlu_data(category)\n",
    "    print(data_dict.keys())\n",
    "    for models in model_name:\n",
    "        #model = AutoModelForCausalLM.from_pretrained(models, use_auth_token=access_token)\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(models, device_map=\"auto\", use_auth_token=access_token)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(models, use_auth_token=access_token)\n",
    "        flop_mmlu_metrics = collect_metrics_for_categories(data_dict, category, bootstrapping, model, tokenizer, max_new_tokens)\n",
    "        allmetrics.append(flop_mmlu_metrics)\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
