{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import transformers\n",
    "import accelerate\n",
    "import vllm\n",
    "import bitsandbytes\n",
    "from vllm import LLM, SamplingParams\n",
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib\n",
    "from collections import Counter\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "#matplotlib.use('TkAgg')\n",
    "#from awq import AutoAWQForCausalLM\n",
    "#from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>How can I improve my time management skills?</td>\n",
       "      <td>generic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>What are the most effective ways to deal with ...</td>\n",
       "      <td>generic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>What are the main differences between Python a...</td>\n",
       "      <td>generic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>How can I increase my productivity while worki...</td>\n",
       "      <td>generic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Can you explain the basics of quantum computing?</td>\n",
       "      <td>generic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>76</td>\n",
       "      <td>Write a script for a YouTube video exploring t...</td>\n",
       "      <td>writing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>77</td>\n",
       "      <td>Compose an engaging travel blog post about a r...</td>\n",
       "      <td>writing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>78</td>\n",
       "      <td>Write a captivating movie review for a recentl...</td>\n",
       "      <td>writing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>79</td>\n",
       "      <td>Structure a podcast script for an episode disc...</td>\n",
       "      <td>writing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>80</td>\n",
       "      <td>Write a symphony concert review, discussing th...</td>\n",
       "      <td>writing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    question_id                                               text category\n",
       "0             1       How can I improve my time management skills?  generic\n",
       "1             2  What are the most effective ways to deal with ...  generic\n",
       "2             3  What are the main differences between Python a...  generic\n",
       "3             4  How can I increase my productivity while worki...  generic\n",
       "4             5   Can you explain the basics of quantum computing?  generic\n",
       "..          ...                                                ...      ...\n",
       "75           76  Write a script for a YouTube video exploring t...  writing\n",
       "76           77  Compose an engaging travel blog post about a r...  writing\n",
       "77           78  Write a captivating movie review for a recentl...  writing\n",
       "78           79  Structure a podcast script for an episode disc...  writing\n",
       "79           80  Write a symphony concert review, discussing th...  writing\n",
       "\n",
       "[80 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Multitask Benchmark datenset json\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "file_path = \"../projects/question.jsonl\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "    #for line in f:\n",
    "    #    data = json.loads(line)\n",
    "    #    print(data)\n",
    "df_mtconversation = pd.DataFrame(data)\n",
    "df_mtconversation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['generic' 'knowledge' 'roleplay' 'common-sense' 'fermi' 'counterfactual'\n",
      " 'coding' 'math' 'writing']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "file_path = \"../projects/question.jsonl\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "    #for line in f:\n",
    "    #    data = json.loads(line)\n",
    "    #    print(data)\n",
    "df_mtconversation = pd.DataFrame(data)\n",
    "df_mtconversation\n",
    "\n",
    "# Categories:\n",
    "print(df_mtconversation.category.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpu_power(gpu_index=1): #measuring for gpu1\n",
    "    \"\"\"Fetches the current power consumption of the GPU using nvidia-smi.\"\"\"\n",
    "    result = subprocess.run(['nvidia-smi', '--query-gpu=power.draw', '--format=csv,noheader,nounits', '-i',str(gpu_index)], \n",
    "                            stdout=subprocess.PIPE, text=True)\n",
    "    power = float(result.stdout.strip())  # Power in watts\n",
    "    return power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hier mit simplem MT_Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (NEW) Energy per flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing category: coding\n",
      "Processing category: math\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch\n",
    "import subprocess\n",
    "import pynvml\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Specify the GPU device you want to use\n",
    "device = \"cuda:0\"  # Change this to your preferred GPU\n",
    "\n",
    "# Load model and tokenizer on the specified device\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")\n",
    "\n",
    "# Initialize NVML for power measurement\n",
    "def initialize_nvml():\n",
    "    pynvml.nvmlInit()\n",
    "\n",
    "def shutdown_nvml():\n",
    "    pynvml.nvmlShutdown()\n",
    "\n",
    "def get_gpu_handle(gpu_index=0):\n",
    "    return pynvml.nvmlDeviceGetHandleByIndex(gpu_index)\n",
    "\n",
    "def measure_power_consumption(handle, duration_sec=1.0, interval_sec=0.1):\n",
    "    power_readings = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while (time.time() - start_time) < duration_sec:\n",
    "        power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0  # Convert from mW to W\n",
    "        power_readings.append(power)\n",
    "        time.sleep(interval_sec)\n",
    "    \n",
    "    return sum(power_readings) / len(power_readings) if power_readings else 0\n",
    "\n",
    "def measure_energy_during_inference(handle, inference_function, *args, **kwargs):\n",
    "    power_start = measure_power_consumption(handle, duration_sec=0.5)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    result = inference_function(*args, **kwargs)  \n",
    "    end_time = time.time()\n",
    "    \n",
    "    power_end = measure_power_consumption(handle, duration_sec=0.5)\n",
    "    \n",
    "    avg_power = (power_start + power_end) / 2\n",
    "    elapsed_time = end_time - start_time  \n",
    "    energy_consumed = avg_power * elapsed_time  \n",
    "    \n",
    "    return energy_consumed, elapsed_time, result\n",
    "\n",
    "def calculate_perplexity(model, input_text, tokenizer):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)  # Ensure input is on the same device\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "        perplexity = torch.exp(loss)\n",
    "    return perplexity.item()\n",
    "\n",
    "def run_experiment_for_texts(texts, bootstrapping, handle):\n",
    "    latencies = []\n",
    "    energy_per_token = []\n",
    "    throughputs = []\n",
    "    generated_texts = []\n",
    "    perplexities = []\n",
    "\n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\").to(device)  # Ensure input is on the same device\n",
    "        text_latencies = []\n",
    "        text_energy_per_token = []\n",
    "        text_throughput = []\n",
    "        text_generated = []\n",
    "        text_perplexities = []\n",
    "\n",
    "        for _ in range(bootstrapping):\n",
    "            energy_consumed, latency, output = measure_energy_during_inference(handle, model.generate, inputs['input_ids'], max_new_tokens=200)\n",
    "            text_latencies.append(latency)\n",
    "\n",
    "            output_tokens = output.size(-1)\n",
    "            energy_token = energy_consumed / output_tokens if output_tokens > 0 else 0\n",
    "            text_energy_per_token.append(energy_token)\n",
    "\n",
    "            throughput = output_tokens / latency\n",
    "            text_throughput.append(throughput)\n",
    "\n",
    "            perplexity = calculate_perplexity(model, text, tokenizer)\n",
    "            text_perplexities.append(perplexity)\n",
    "\n",
    "            generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            filtered_generated_text = generated_text.replace(text, \"\").strip()\n",
    "            text_generated.append(filtered_generated_text)\n",
    "\n",
    "        latencies.append(text_latencies)\n",
    "        energy_per_token.append(text_energy_per_token)\n",
    "        throughputs.append(text_throughput)\n",
    "        generated_texts.append(text_generated)\n",
    "        perplexities.append(text_perplexities)\n",
    "\n",
    "    return latencies, energy_per_token, throughputs, generated_texts, perplexities\n",
    "\n",
    "def collect_metrics_for_categories(df, categories, bootstrapping):\n",
    "    category_metrics = {}\n",
    "    handle = get_gpu_handle(gpu_index=0)\n",
    "\n",
    "    for category in categories:\n",
    "        print(f\"Processing category: {category}\")\n",
    "        texts = filter_texts_by_category(df, category)\n",
    "        latencies, energy_per_token, throughputs, generated_texts, perplexities = run_experiment_for_texts(texts, bootstrapping, handle)\n",
    "\n",
    "        category_metrics[category] = {\n",
    "            \"latencies\": latencies,\n",
    "            \"energy_per_token\": energy_per_token,\n",
    "            \"throughput\": throughputs,\n",
    "            \"generated_texts\": generated_texts,\n",
    "            \"perplexities\": perplexities\n",
    "        }\n",
    "\n",
    "    shutdown_nvml()  \n",
    "    return category_metrics\n",
    "\n",
    "def filter_texts_by_category(df, category):\n",
    "    return df[df['category'] == category]['text'].values\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Example Usage\n",
    "file_path = \"../projects/question.jsonl\"\n",
    "bootstrapping = 2  \n",
    "df_mtconversation = load_dataset(file_path)\n",
    "\n",
    "categories = [  'coding', 'math']\n",
    "\n",
    "initialize_nvml()\n",
    "\n",
    "metrics = collect_metrics_for_categories(df_mtconversation, categories, bootstrapping)\n",
    "\n",
    "# (Optionally, you can visualize the collected metrics here)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def plot_metrics(metrics, categories):\n",
    "    num_categories = len(categories)\n",
    "\n",
    "    # Prepare the data for plotting\n",
    "    energy_per_token = []\n",
    "    avg_latencies = []\n",
    "    avg_perplexities = []\n",
    "\n",
    "    for category in categories:\n",
    "        if category in metrics:\n",
    "            energy_per_token.append(np.mean(metrics[category][\"energy_per_token\"]))\n",
    "            avg_latencies.append(np.mean(metrics[category][\"latencies\"]))\n",
    "            avg_perplexities.append(np.mean(metrics[category][\"perplexities\"]))\n",
    "        else:\n",
    "            energy_per_token.append(0)\n",
    "            avg_latencies.append(0)\n",
    "            avg_perplexities.append(0)\n",
    "\n",
    "    x = np.arange(num_categories)  # the label locations\n",
    "    width = 0.25  # the width of the bars\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Plot energy consumption\n",
    "    bars1 = ax1.bar(x - width, energy_per_token, width, label='Energy per Token (Joules)', color='b')\n",
    "    ax1.set_ylabel('Energy per Token (Joules)', color='b')\n",
    "    ax1.tick_params(axis='y', labelcolor='b')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(categories)\n",
    "    \n",
    "    # Create a second y-axis for latencies\n",
    "    ax2 = ax1.twinx()\n",
    "    bars2 = ax2.bar(x, avg_latencies, width, label='Average Latency (s)', color='g')\n",
    "    ax2.set_ylabel('Average Latency (s)', color='g')\n",
    "    ax2.tick_params(axis='y', labelcolor='g')\n",
    "\n",
    "    # Create a third y-axis for perplexities\n",
    "    ax3 = ax1.twinx()\n",
    "    bars3 = ax3.bar(x + width, avg_perplexities, width, label='Average Perplexity', color='r')\n",
    "    ax3.spines['right'].set_position(('outward', 60))  # move the third y-axis to the right\n",
    "    ax3.set_ylabel('Average Perplexity', color='r')\n",
    "    ax3.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "    # Adding titles and legend\n",
    "    fig.suptitle('Metrics Comparison Across Task Categories', fontsize=16)\n",
    "    fig.legend(loc='upper right', bbox_to_anchor=(0.85, 0.85))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the plotting function\n",
    "plot_metrics(metrics, categories)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MT_Bench mit quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    }
   ],
   "source": [
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",  # Specify the quantization type\n",
    "    bnb_4bit_use_double_quant=True,  # Use double quantization if needed\n",
    "    bnb_4bit_compute_dtype=torch.float16  # Specify computation dtype\n",
    ")\n",
    "device = \"cuda:0\" \n",
    "\n",
    "# Configure 4-bit quantization\n",
    "#quant_config = { \"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\" }\n",
    "# Load the model and tokenizer with quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\",\n",
    "                                             #\"tiiuae/falcon-mamba-7b\",\n",
    "                                             quantization_config=quant_config,\n",
    "                                             )\n",
    "tokenizer = AutoTokenizer.from_pretrained (\"facebook/opt-125m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ebergy per Token next to perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing category: knowledge\n",
      "Processing category: common-sense\n",
      "Processing category: coding\n",
      "Processing category: math\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch\n",
    "import subprocess\n",
    "import pynvml\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Specify the GPU device you want to use\n",
    "device = \"cuda:0\"  # Change this to your preferred GPU\n",
    "\n",
    "# Load model and tokenizer on the specified device\n",
    "#model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\").to(device)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")\n",
    "\n",
    "# Initialize NVML for power measurement\n",
    "def initialize_nvml():\n",
    "    pynvml.nvmlInit()\n",
    "\n",
    "def shutdown_nvml():\n",
    "    pynvml.nvmlShutdown()\n",
    "\n",
    "def get_gpu_handle(gpu_index=0):\n",
    "    return pynvml.nvmlDeviceGetHandleByIndex(gpu_index)\n",
    "\n",
    "def measure_power_consumption(handle, duration_sec=1.0, interval_sec=0.1):\n",
    "    power_readings = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while (time.time() - start_time) < duration_sec:\n",
    "        power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0  # Convert from mW to W\n",
    "        power_readings.append(power)\n",
    "        time.sleep(interval_sec)\n",
    "    \n",
    "    return sum(power_readings) / len(power_readings) if power_readings else 0\n",
    "\n",
    "def measure_energy_during_inference(handle, inference_function, *args, **kwargs):\n",
    "    power_start = measure_power_consumption(handle, duration_sec=0.5)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    result = inference_function(*args, **kwargs)  \n",
    "    end_time = time.time()\n",
    "    \n",
    "    power_end = measure_power_consumption(handle, duration_sec=0.5)\n",
    "    \n",
    "    avg_power = (power_start + power_end) / 2\n",
    "    elapsed_time = end_time - start_time  \n",
    "    energy_consumed = avg_power * elapsed_time  \n",
    "    \n",
    "    return energy_consumed, elapsed_time, result\n",
    "\n",
    "def calculate_perplexity(model, input_text, tokenizer):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)  # Ensure input is on the same device\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "        perplexity = torch.exp(loss)\n",
    "    return perplexity.item()\n",
    "\n",
    "def run_experiment_for_texts(texts, bootstrapping, handle):\n",
    "    latencies = []\n",
    "    energy_per_token = []\n",
    "    throughputs = []\n",
    "    generated_texts = []\n",
    "    perplexities = []\n",
    "\n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\").to(device)  # Ensure input is on the same device\n",
    "        text_latencies = []\n",
    "        text_energy_per_token = []\n",
    "        text_throughput = []\n",
    "        text_generated = []\n",
    "        text_perplexities = []\n",
    "\n",
    "        for _ in range(bootstrapping):\n",
    "            energy_consumed, latency, output = measure_energy_during_inference(handle, model.generate, inputs['input_ids'], max_new_tokens=200)\n",
    "            text_latencies.append(latency)\n",
    "\n",
    "            output_tokens = output.size(-1)\n",
    "            energy_token = energy_consumed / output_tokens if output_tokens > 0 else 0\n",
    "            text_energy_per_token.append(energy_token)\n",
    "\n",
    "            throughput = output_tokens / latency\n",
    "            text_throughput.append(throughput)\n",
    "\n",
    "            perplexity = calculate_perplexity(model, text, tokenizer)\n",
    "            text_perplexities.append(perplexity)\n",
    "\n",
    "            generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            filtered_generated_text = generated_text.replace(text, \"\").strip()\n",
    "            text_generated.append(filtered_generated_text)\n",
    "\n",
    "        latencies.append(text_latencies)\n",
    "        energy_per_token.append(text_energy_per_token)\n",
    "        throughputs.append(text_throughput)\n",
    "        generated_texts.append(text_generated)\n",
    "        perplexities.append(text_perplexities)\n",
    "\n",
    "    return latencies, energy_per_token, throughputs, generated_texts, perplexities\n",
    "\n",
    "def collect_metrics_for_categories(df, categories, bootstrapping):\n",
    "    category_metrics = {}\n",
    "    handle = get_gpu_handle(gpu_index=0)\n",
    "\n",
    "    for category in categories:\n",
    "        print(f\"Processing category: {category}\")\n",
    "        texts = filter_texts_by_category(df, category)\n",
    "        latencies, energy_per_token, throughputs, generated_texts, perplexities = run_experiment_for_texts(texts, bootstrapping, handle)\n",
    "\n",
    "        category_metrics[category] = {\n",
    "            \"latencies\": latencies,\n",
    "            \"energy_per_token\": energy_per_token,\n",
    "            \"throughput\": throughputs,\n",
    "            \"generated_texts\": generated_texts,\n",
    "            \"perplexities\": perplexities\n",
    "        }\n",
    "\n",
    "    shutdown_nvml()  \n",
    "    return category_metrics\n",
    "\n",
    "def filter_texts_by_category(df, category):\n",
    "    return df[df['category'] == category]['text'].values\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Example Usage\n",
    "file_path = \"../projects/question.jsonl\"\n",
    "bootstrapping = 2  \n",
    "df_mtconversation = load_dataset(file_path)\n",
    "\n",
    "categories = [ 'knowledge', 'common-sense', 'coding', 'math']\n",
    "\n",
    "initialize_nvml()\n",
    "\n",
    "metrics = collect_metrics_for_categories(df_mtconversation, categories, bootstrapping)\n",
    "\n",
    "# (Optionally, you can visualize the collected metrics here)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def plot_metrics(metrics, categories):\n",
    "    num_categories = len(categories)\n",
    "\n",
    "    # Prepare the data for plotting\n",
    "    energy_per_floost = []\n",
    "    energy_per_token = []\n",
    "    avg_latencies = []\n",
    "    avg_perplexities = []\n",
    "\n",
    "    for category in categories:\n",
    "        if category in metrics:\n",
    "            energy_per_token.append(np.mean(metrics[category][\"energy_per_token\"]))\n",
    "            avg_latencies.append(np.mean(metrics[category][\"latencies\"]))\n",
    "            avg_perplexities.append(np.mean(metrics[category][\"perplexities\"]))\n",
    "        else:\n",
    "            energy_per_token.append(0)\n",
    "            avg_latencies.append(0)\n",
    "            avg_perplexities.append(0)\n",
    "\n",
    "    x = np.arange(num_categories)  # the label locations\n",
    "    width = 0.25  # the width of the bars\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Plot energy consumption\n",
    "    bars1 = ax1.bar(x - width, energy_per_token, width, label='Energy per Token (Joules)', color='b')\n",
    "    ax1.set_ylabel('Energy per Token (Joules)', color='b')\n",
    "    ax1.tick_params(axis='y', labelcolor='b')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(categories)\n",
    "    \n",
    "    # Create a second y-axis for latencies\n",
    "    ax2 = ax1.twinx()\n",
    "    bars2 = ax2.bar(x, avg_latencies, width, label='Average Latency (s)', color='g')\n",
    "    ax2.set_ylabel('Average Latency (s)', color='g')\n",
    "    ax2.tick_params(axis='y', labelcolor='g')\n",
    "\n",
    "    # Create a third y-axis for perplexities\n",
    "    ax3 = ax1.twinx()\n",
    "    bars3 = ax3.bar(x + width, avg_perplexities, width, label='Average Perplexity', color='r')\n",
    "    ax3.spines['right'].set_position(('outward', 60))  # move the third y-axis to the right\n",
    "    ax3.set_ylabel('Average Perplexity', color='r')\n",
    "    ax3.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "    # Adding titles and legend\n",
    "    fig.suptitle('Metrics Comparison Across Task Categories', fontsize=16)\n",
    "    fig.legend(loc='upper right', bbox_to_anchor=(0.85, 0.85))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the plotting function\n",
    "plot_metrics(metrics, categories)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# energy_per_flops - 20.10.24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hier: That one is working (takes a lot of time)\n",
    "\n",
    "### Reasons might be: the duration_sec in measure_power_consumption (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing category: knowledge\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        17.10%     341.811ms        22.17%     443.284ms      30.784us     209.022ms        55.49%     209.022ms      14.515us         14400     38220.595  \n",
      "                                               aten::mm         0.26%       5.172ms         0.36%       7.179ms      35.894us      77.595ms        20.60%      77.595ms     387.977us           200     17374.003  \n",
      "                                              aten::bmm         5.42%     108.357ms         7.07%     141.308ms      29.439us      12.545ms         3.33%      12.545ms       2.613us          4800       949.248  \n",
      "                                              aten::add         5.88%     117.519ms         8.54%     170.703ms      21.333us      12.191ms         3.24%      12.191ms       1.524us          8002         8.029  \n",
      "                                              aten::mul         2.37%      47.402ms         3.32%      66.284ms      22.072us       3.857ms         1.02%       3.857ms       1.284us          3003         2.099  \n",
      "                                            aten::empty         4.95%      98.966ms         4.95%      98.966ms       5.248us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.32%       6.365ms         5.26%     105.114ms      24.956us       0.000us         0.00%       1.717ms       0.408us          4212            --  \n",
      "                                         aten::_to_copy         0.78%      15.601ms         4.94%      98.748ms      35.204us       0.000us         0.00%       1.717ms       0.612us          2805            --  \n",
      "                                    aten::empty_strided         1.09%      21.731ms         1.09%      21.731ms       7.229us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.46%      29.181ms         3.99%      79.811ms      21.782us       2.977ms         0.79%       2.977ms       0.813us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.999s\n",
      "Self CUDA time total: 376.700ms\n",
      "\n",
      "output: tensor([[    2,  2264,    32,   103,   801,  8819,     9,   634,    10,   881,\n",
      "            12,  3698,  4136,  7304,  4411,    10, 31460,  7304,    15,   258,\n",
      "             5,  1737,     8,  1050,   474,   116, 50118, 50118,   133,  4136,\n",
      "          7304,    16,    10, 31460,  7304,     6,    53,    24,    16,    45,\n",
      "         31460,     4,    20,  4136,  7304,    16,    10, 31460,  7304,     6,\n",
      "            53,    24,    16,    45, 31460,     4,    20,  4136,  7304,    16,\n",
      "            10, 31460,  7304,     6,    53,    24,    16,    45, 31460,     4,\n",
      "            20,  4136,  7304,    16,    10, 31460,  7304,     6,    53,    24,\n",
      "            16,    45, 31460,     4,    20,  4136,  7304,    16,    10, 31460,\n",
      "          7304,     6,    53,    24,    16,    45, 31460,     4,    20,  4136,\n",
      "          7304,    16,    10, 31460,  7304,     6,    53,    24,    16,    45,\n",
      "         31460,     4,    20,  4136,  7304,    16,    10, 31460,  7304,     6,\n",
      "            53,    24,    16,    45, 31460,     4,    20,  4136,  7304,    16,\n",
      "            10, 31460,  7304,     6,    53,    24,    16,    45, 31460,     4,\n",
      "            20,  4136,  7304,    16,    10, 31460,  7304,     6,    53,    24,\n",
      "            16,    45, 31460,     4,    20,  4136,  7304,    16,    10, 31460,\n",
      "          7304,     6,    53,    24,    16,    45, 31460,     4,    20,  4136,\n",
      "          7304,    16,    10, 31460,  7304,     6,    53,    24,    16,    45,\n",
      "         31460,     4,    20,  4136,  7304,    16,    10, 31460,  7304,     6,\n",
      "            53,    24,    16,    45, 31460,     4,    20,  4136,  7304,    16,\n",
      "            10, 31460,  7304,     6,    53,    24,    16,    45, 31460,     4,\n",
      "            20,  4136,  7304,    16,    10, 31460,  7304,     6,    53,    24,\n",
      "            16,    45, 31460,     4,    20,  4136]], device='cuda:0')\n",
      "text_energy_per_token: [3.429999543463445]\n",
      "output_tokens: 226\n",
      "flop: 56553974230\n",
      "energy_consumed:  775.1798968227386\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        17.04%     338.692ms        22.12%     439.666ms      30.532us     209.002ms        55.49%     209.002ms      14.514us         14400     38220.595  \n",
      "                                               aten::mm         0.26%       5.150ms         0.36%       7.159ms      35.795us      77.591ms        20.60%      77.591ms     387.955us           200     17374.003  \n",
      "                                              aten::bmm         5.44%     108.088ms         7.10%     141.017ms      29.379us      12.556ms         3.33%      12.556ms       2.616us          4800       949.248  \n",
      "                                              aten::add         5.89%     117.108ms         8.55%     169.954ms      21.239us      12.186ms         3.24%      12.186ms       1.523us          8002         8.029  \n",
      "                                              aten::mul         2.38%      47.226ms         3.33%      66.173ms      22.036us       3.852ms         1.02%       3.852ms       1.283us          3003         2.099  \n",
      "                                            aten::empty         4.92%      97.833ms         4.92%      97.833ms       5.188us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.32%       6.436ms         5.24%     104.207ms      24.740us       0.000us         0.00%       1.721ms       0.409us          4212            --  \n",
      "                                         aten::_to_copy         0.78%      15.445ms         4.92%      97.771ms      34.856us       0.000us         0.00%       1.721ms       0.613us          2805            --  \n",
      "                                    aten::empty_strided         1.08%      21.501ms         1.08%      21.501ms       7.153us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.46%      28.951ms         3.97%      78.933ms      21.543us       2.989ms         0.79%       2.989ms       0.816us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.987s\n",
      "Self CUDA time total: 376.675ms\n",
      "\n",
      "output: tensor([[    2,  2264,    32,   103,   801,  8819,     9,   634,    10,   881,\n",
      "            12,  3698,  4136,  7304,  4411,    10, 31460,  7304,    15,   258,\n",
      "             5,  1737,     8,  1050,   474,   116, 50118, 50118,   133,  4136,\n",
      "          7304,    16,    10, 31460,  7304,     6,    53,    24,    16,    45,\n",
      "         31460,     4,    20,  4136,  7304,    16,    10, 31460,  7304,     6,\n",
      "            53,    24,    16,    45, 31460,     4,    20,  4136,  7304,    16,\n",
      "            10, 31460,  7304,     6,    53,    24,    16,    45, 31460,     4,\n",
      "            20,  4136,  7304,    16,    10, 31460,  7304,     6,    53,    24,\n",
      "            16,    45, 31460,     4,    20,  4136,  7304,    16,    10, 31460,\n",
      "          7304,     6,    53,    24,    16,    45, 31460,     4,    20,  4136,\n",
      "          7304,    16,    10, 31460,  7304,     6,    53,    24,    16,    45,\n",
      "         31460,     4,    20,  4136,  7304,    16,    10, 31460,  7304,     6,\n",
      "            53,    24,    16,    45, 31460,     4,    20,  4136,  7304,    16,\n",
      "            10, 31460,  7304,     6,    53,    24,    16,    45, 31460,     4,\n",
      "            20,  4136,  7304,    16,    10, 31460,  7304,     6,    53,    24,\n",
      "            16,    45, 31460,     4,    20,  4136,  7304,    16,    10, 31460,\n",
      "          7304,     6,    53,    24,    16,    45, 31460,     4,    20,  4136,\n",
      "          7304,    16,    10, 31460,  7304,     6,    53,    24,    16,    45,\n",
      "         31460,     4,    20,  4136,  7304,    16,    10, 31460,  7304,     6,\n",
      "            53,    24,    16,    45, 31460,     4,    20,  4136,  7304,    16,\n",
      "            10, 31460,  7304,     6,    53,    24,    16,    45, 31460,     4,\n",
      "            20,  4136,  7304,    16,    10, 31460,  7304,     6,    53,    24,\n",
      "            16,    45, 31460,     4,    20,  4136]], device='cuda:0')\n",
      "text_energy_per_token: [3.429999543463445, 2.819025070667583]\n",
      "output_tokens: 226\n",
      "flop: 56553974230\n",
      "energy_consumed:  637.0996659708737\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        17.14%     340.028ms        22.21%     440.600ms      30.597us     208.834ms        55.59%     208.834ms      14.502us         14400     36521.902  \n",
      "                                               aten::mm         0.26%       5.132ms         0.35%       6.856ms      34.281us      77.601ms        20.66%      77.601ms     388.006us           200     16601.825  \n",
      "                                              aten::bmm         5.44%     107.898ms         7.09%     140.597ms      29.291us      12.197ms         3.25%      12.197ms       2.541us          4800       860.406  \n",
      "                                              aten::add         5.90%     117.017ms         8.58%     170.210ms      21.271us      12.175ms         3.24%      12.175ms       1.521us          8002         7.490  \n",
      "                                              aten::mul         2.36%      46.914ms         3.31%      65.684ms      21.873us       3.856ms         1.03%       3.856ms       1.284us          3003         2.005  \n",
      "                                            aten::empty         4.90%      97.187ms         4.90%      97.187ms       5.154us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.32%       6.420ms         5.24%     103.872ms      24.661us       0.000us         0.00%       1.712ms       0.406us          4212            --  \n",
      "                                         aten::_to_copy         0.78%      15.454ms         4.91%      97.451ms      34.742us       0.000us         0.00%       1.712ms       0.610us          2805            --  \n",
      "                                    aten::empty_strided         1.09%      21.576ms         1.09%      21.576ms       7.178us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.45%      28.667ms         3.94%      78.077ms      21.309us       2.979ms         0.79%       2.979ms       0.813us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.984s\n",
      "Self CUDA time total: 375.639ms\n",
      "\n",
      "output: tensor([[    2,  2264,  2433,    74,    47,  1701,    77, 15293,    41, 10510,\n",
      "             8,  6500,   285,  4264,   467,   116, 50118, 50118,   133,   412,\n",
      "             9,   764,  2659,    34,    10,   251,   750,     9,  1976,  4555,\n",
      "             6,  6500,     6,     8,  1522,   285,  4264,     4,   166,    32,\n",
      "          2021,     7,  1976,     5,   275,   678,  4264,  1735,    13,    70,\n",
      "            84,  1196,     4, 50118, 50118,  2264,  2433,    74,    47,  1701,\n",
      "            77, 15293,    41, 10510,     8,  6500,   285,  4264,   467,   116,\n",
      "         50118, 50118,   133,   412,     9,   764,  2659,    34,    10,   251,\n",
      "           750,     9,  1976,  4555,     6,  6500,     6,     8,  1522,   285,\n",
      "          4264,     4,   166,    32,  2021,     7,  1976,     5,   275,   678,\n",
      "          4264,  1735,    13,    70,    84,  1196,     4, 50118, 50118,  2264,\n",
      "          2433,    74,    47,  1701,    77, 15293,    41, 10510,     8,  6500,\n",
      "           285,  4264,   467,   116, 50118, 50118,   133,   412,     9,   764,\n",
      "          2659,    34,    10,   251,   750,     9,  1976,  4555,     6,  6500,\n",
      "             6,     8,  1522,   285,  4264,     4,   166,    32,  2021,     7,\n",
      "          1976,     5,   275,   678,  4264,  1735,    13,    70,    84,  1196,\n",
      "             4, 50118, 50118,  2264,  2433,    74,    47,  1701,    77, 15293,\n",
      "            41, 10510,     8,  6500,   285,  4264,   467,   116, 50118, 50118,\n",
      "           133,   412,     9,   764,  2659,    34,    10,   251,   750,     9,\n",
      "          1976,  4555,     6,  6500,     6,     8,  1522,   285,  4264,     4,\n",
      "           166,    32,  2021,     7,  1976,     5,   275,   678,  4264,  1735,\n",
      "            13,    70,    84,  1196,     4, 50118]], device='cuda:0')\n",
      "text_energy_per_token: [3.2955069858683483]\n",
      "output_tokens: 216\n",
      "flop: 53993627700\n",
      "energy_consumed:  711.8295089475632\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        17.12%     341.413ms        22.20%     442.693ms      30.743us     208.831ms        55.59%     208.831ms      14.502us         14400     36521.902  \n",
      "                                               aten::mm         0.26%       5.200ms         0.35%       6.948ms      34.738us      77.602ms        20.66%      77.602ms     388.008us           200     16601.825  \n",
      "                                              aten::bmm         5.43%     108.310ms         7.08%     141.087ms      29.393us      12.199ms         3.25%      12.199ms       2.541us          4800       860.406  \n",
      "                                              aten::add         5.91%     117.913ms         8.60%     171.394ms      21.419us      12.178ms         3.24%      12.178ms       1.522us          8002         7.490  \n",
      "                                              aten::mul         2.37%      47.235ms         3.32%      66.106ms      22.013us       3.853ms         1.03%       3.853ms       1.283us          3003         2.005  \n",
      "                                            aten::empty         4.91%      97.942ms         4.91%      97.942ms       5.194us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.32%       6.411ms         5.22%     104.132ms      24.723us       0.000us         0.00%       1.711ms       0.406us          4212            --  \n",
      "                                         aten::_to_copy         0.78%      15.560ms         4.90%      97.721ms      34.838us       0.000us         0.00%       1.711ms       0.610us          2805            --  \n",
      "                                    aten::empty_strided         1.08%      21.451ms         1.08%      21.451ms       7.136us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.45%      28.918ms         3.95%      78.741ms      21.490us       2.977ms         0.79%       2.977ms       0.813us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.994s\n",
      "Self CUDA time total: 375.656ms\n",
      "\n",
      "output: tensor([[    2,  2264,  2433,    74,    47,  1701,    77, 15293,    41, 10510,\n",
      "             8,  6500,   285,  4264,   467,   116, 50118, 50118,   133,   412,\n",
      "             9,   764,  2659,    34,    10,   251,   750,     9,  1976,  4555,\n",
      "             6,  6500,     6,     8,  1522,   285,  4264,     4,   166,    32,\n",
      "          2021,     7,  1976,     5,   275,   678,  4264,  1735,    13,    70,\n",
      "            84,  1196,     4, 50118, 50118,  2264,  2433,    74,    47,  1701,\n",
      "            77, 15293,    41, 10510,     8,  6500,   285,  4264,   467,   116,\n",
      "         50118, 50118,   133,   412,     9,   764,  2659,    34,    10,   251,\n",
      "           750,     9,  1976,  4555,     6,  6500,     6,     8,  1522,   285,\n",
      "          4264,     4,   166,    32,  2021,     7,  1976,     5,   275,   678,\n",
      "          4264,  1735,    13,    70,    84,  1196,     4, 50118, 50118,  2264,\n",
      "          2433,    74,    47,  1701,    77, 15293,    41, 10510,     8,  6500,\n",
      "           285,  4264,   467,   116, 50118, 50118,   133,   412,     9,   764,\n",
      "          2659,    34,    10,   251,   750,     9,  1976,  4555,     6,  6500,\n",
      "             6,     8,  1522,   285,  4264,     4,   166,    32,  2021,     7,\n",
      "          1976,     5,   275,   678,  4264,  1735,    13,    70,    84,  1196,\n",
      "             4, 50118, 50118,  2264,  2433,    74,    47,  1701,    77, 15293,\n",
      "            41, 10510,     8,  6500,   285,  4264,   467,   116, 50118, 50118,\n",
      "           133,   412,     9,   764,  2659,    34,    10,   251,   750,     9,\n",
      "          1976,  4555,     6,  6500,     6,     8,  1522,   285,  4264,     4,\n",
      "           166,    32,  2021,     7,  1976,     5,   275,   678,  4264,  1735,\n",
      "            13,    70,    84,  1196,     4, 50118]], device='cuda:0')\n",
      "text_energy_per_token: [3.2955069858683483, 3.1874626529282994]\n",
      "output_tokens: 216\n",
      "flop: 53993627700\n",
      "energy_consumed:  688.4919330325126\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        17.15%     341.844ms        22.22%     442.855ms      30.754us     208.760ms        55.60%     208.760ms      14.497us         14400     36352.033  \n",
      "                                               aten::mm         0.26%       5.126ms         0.34%       6.873ms      34.363us      77.594ms        20.66%      77.594ms     387.968us           200     16524.607  \n",
      "                                              aten::bmm         5.44%     108.503ms         7.09%     141.300ms      29.438us      12.173ms         3.24%      12.173ms       2.536us          4800       851.927  \n",
      "                                              aten::add         5.90%     117.708ms         8.57%     170.850ms      21.351us      12.180ms         3.24%      12.180ms       1.522us          8002         7.437  \n",
      "                                              aten::mul         2.36%      47.126ms         3.32%      66.261ms      22.065us       3.852ms         1.03%       3.852ms       1.283us          3003         1.996  \n",
      "                                            aten::empty         4.91%      97.909ms         4.91%      97.909ms       5.192us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.32%       6.373ms         5.23%     104.351ms      24.775us       0.000us         0.00%       1.711ms       0.406us          4212            --  \n",
      "                                         aten::_to_copy         0.78%      15.553ms         4.92%      97.978ms      34.930us       0.000us         0.00%       1.711ms       0.610us          2805            --  \n",
      "                                    aten::empty_strided         1.09%      21.760ms         1.09%      21.760ms       7.239us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.45%      28.823ms         3.94%      78.530ms      21.433us       2.979ms         0.79%       2.979ms       0.813us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.993s\n",
      "Self CUDA time total: 375.489ms\n",
      "\n",
      "output: tensor([[    2,  6179,    64,  3233, 16085,  2358,     8,  5775,  1986,     7,\n",
      "          5217,   776,  3872, 27366,   116, 50118, 50118,   133,  1948,    16,\n",
      "          2007,    35,     5,   168,    64,   304,  2358,     8,  5775,  1986,\n",
      "             7,  5217,   776,  3872, 27366,     4, 50118, 50118,   133,   168,\n",
      "            64,   304,  2358,     8,  5775,  1986,     7,  5217,   776,  3872,\n",
      "         27366,     4, 50118, 50118,   133,   168,    64,   304,  2358,     8,\n",
      "          5775,  1986,     7,  5217,   776,  3872, 27366,     4, 50118, 50118,\n",
      "           133,   168,    64,   304,  2358,     8,  5775,  1986,     7,  5217,\n",
      "           776,  3872, 27366,     4, 50118, 50118,   133,   168,    64,   304,\n",
      "          2358,     8,  5775,  1986,     7,  5217,   776,  3872, 27366,     4,\n",
      "         50118, 50118,   133,   168,    64,   304,  2358,     8,  5775,  1986,\n",
      "             7,  5217,   776,  3872, 27366,     4, 50118, 50118,   133,   168,\n",
      "            64,   304,  2358,     8,  5775,  1986,     7,  5217,   776,  3872,\n",
      "         27366,     4, 50118, 50118,   133,   168,    64,   304,  2358,     8,\n",
      "          5775,  1986,     7,  5217,   776,  3872, 27366,     4, 50118, 50118,\n",
      "           133,   168,    64,   304,  2358,     8,  5775,  1986,     7,  5217,\n",
      "           776,  3872, 27366,     4, 50118, 50118,   133,   168,    64,   304,\n",
      "          2358,     8,  5775,  1986,     7,  5217,   776,  3872, 27366,     4,\n",
      "         50118, 50118,   133,   168,    64,   304,  2358,     8,  5775,  1986,\n",
      "             7,  5217,   776,  3872, 27366,     4, 50118, 50118,   133,   168,\n",
      "            64,   304,  2358,     8,  5775,  1986,     7,  5217,   776,  3872,\n",
      "         27366,     4, 50118, 50118,   133]], device='cuda:0')\n",
      "text_energy_per_token: [3.181702285389567]\n",
      "output_tokens: 215\n",
      "flop: 53738000135\n",
      "energy_consumed:  684.065991358757\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        17.12%     340.563ms        22.20%     441.466ms      30.657us     208.782ms        55.59%     208.782ms      14.499us         14400     36352.033  \n",
      "                                               aten::mm         0.26%       5.153ms         0.35%       6.873ms      34.363us      77.601ms        20.66%      77.601ms     388.007us           200     16524.607  \n",
      "                                              aten::bmm         5.43%     108.009ms         7.09%     140.952ms      29.365us      12.157ms         3.24%      12.157ms       2.533us          4800       851.927  \n",
      "                                              aten::add         5.90%     117.324ms         8.57%     170.375ms      21.292us      12.180ms         3.24%      12.180ms       1.522us          8002         7.437  \n",
      "                                              aten::mul         2.37%      47.050ms         3.32%      65.946ms      21.960us       3.855ms         1.03%       3.855ms       1.284us          3003         1.996  \n",
      "                                            aten::empty         4.92%      97.845ms         4.92%      97.845ms       5.188us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.32%       6.393ms         5.23%     104.036ms      24.700us       0.000us         0.00%       1.705ms       0.405us          4212            --  \n",
      "                                         aten::_to_copy         0.78%      15.469ms         4.91%      97.643ms      34.810us       0.000us         0.00%       1.705ms       0.608us          2805            --  \n",
      "                                    aten::empty_strided         1.08%      21.429ms         1.08%      21.429ms       7.129us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.45%      28.894ms         3.95%      78.532ms      21.433us       2.971ms         0.79%       2.971ms       0.811us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.989s\n",
      "Self CUDA time total: 375.557ms\n",
      "\n",
      "output: tensor([[    2,  6179,    64,  3233, 16085,  2358,     8,  5775,  1986,     7,\n",
      "          5217,   776,  3872, 27366,   116, 50118, 50118,   133,  1948,    16,\n",
      "          2007,    35,     5,   168,    64,   304,  2358,     8,  5775,  1986,\n",
      "             7,  5217,   776,  3872, 27366,     4, 50118, 50118,   133,   168,\n",
      "            64,   304,  2358,     8,  5775,  1986,     7,  5217,   776,  3872,\n",
      "         27366,     4, 50118, 50118,   133,   168,    64,   304,  2358,     8,\n",
      "          5775,  1986,     7,  5217,   776,  3872, 27366,     4, 50118, 50118,\n",
      "           133,   168,    64,   304,  2358,     8,  5775,  1986,     7,  5217,\n",
      "           776,  3872, 27366,     4, 50118, 50118,   133,   168,    64,   304,\n",
      "          2358,     8,  5775,  1986,     7,  5217,   776,  3872, 27366,     4,\n",
      "         50118, 50118,   133,   168,    64,   304,  2358,     8,  5775,  1986,\n",
      "             7,  5217,   776,  3872, 27366,     4, 50118, 50118,   133,   168,\n",
      "            64,   304,  2358,     8,  5775,  1986,     7,  5217,   776,  3872,\n",
      "         27366,     4, 50118, 50118,   133,   168,    64,   304,  2358,     8,\n",
      "          5775,  1986,     7,  5217,   776,  3872, 27366,     4, 50118, 50118,\n",
      "           133,   168,    64,   304,  2358,     8,  5775,  1986,     7,  5217,\n",
      "           776,  3872, 27366,     4, 50118, 50118,   133,   168,    64,   304,\n",
      "          2358,     8,  5775,  1986,     7,  5217,   776,  3872, 27366,     4,\n",
      "         50118, 50118,   133,   168,    64,   304,  2358,     8,  5775,  1986,\n",
      "             7,  5217,   776,  3872, 27366,     4, 50118, 50118,   133,   168,\n",
      "            64,   304,  2358,     8,  5775,  1986,     7,  5217,   776,  3872,\n",
      "         27366,     4, 50118, 50118,   133]], device='cuda:0')\n",
      "text_energy_per_token: [3.181702285389567, 3.24291599695993]\n",
      "output_tokens: 215\n",
      "flop: 53738000135\n",
      "energy_consumed:  697.2269393463849\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        17.12%     339.169ms        22.17%     439.081ms      30.492us     208.598ms        55.51%     208.598ms      14.486us         14400     37031.510  \n",
      "                                               aten::mm         0.27%       5.343ms         0.36%       7.069ms      35.343us      77.594ms        20.65%      77.594ms     387.970us           200     16833.479  \n",
      "                                              aten::bmm         5.44%     107.787ms         7.10%     140.571ms      29.286us      12.381ms         3.29%      12.381ms       2.579us          4800       886.284  \n",
      "                                              aten::add         5.89%     116.745ms         8.58%     169.927ms      21.236us      12.192ms         3.24%      12.192ms       1.524us          8002         7.648  \n",
      "                                              aten::mul         2.37%      47.027ms         3.32%      65.751ms      21.895us       3.852ms         1.03%       3.852ms       1.283us          3003         2.033  \n",
      "                                            aten::empty         4.91%      97.335ms         4.91%      97.335ms       5.161us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.32%       6.415ms         5.26%     104.258ms      24.753us       0.000us         0.00%       1.716ms       0.407us          4212            --  \n",
      "                                         aten::_to_copy         0.78%      15.515ms         4.94%      97.843ms      34.882us       0.000us         0.00%       1.716ms       0.612us          2805            --  \n",
      "                                    aten::empty_strided         1.09%      21.540ms         1.09%      21.540ms       7.166us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.45%      28.803ms         3.97%      78.575ms      21.445us       2.984ms         0.79%       2.984ms       0.814us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.981s\n",
      "Self CUDA time total: 375.768ms\n",
      "\n",
      "output: tensor([[    2,  6179,   109,  2777,     8,  4106,  7926,  3327,     5,   169,\n",
      "            82,  8469,     8,  1026,  4158,    11, 30286, 17537,   116, 50118,\n",
      "         50118,   133,   892,     6,  1027,    11,     5,  8812,  3574, 32255,\n",
      "             6,   303,    14,  2777,     8,  4106,  7926,  3327,     5,   169,\n",
      "            82,  8469,     8,  1026,  4158,    11, 30286, 17537,     4, 50118,\n",
      "         50118,    17,    48, 46969,     8,  4106,  7926,  3327,     5,   169,\n",
      "            82,  8469,     8,  1026,  4158,    11, 30286, 17537,     6,    17,\n",
      "            46,   161,   892,  1029,    12, 11515,   925,     4,   871,   305,\n",
      "             4,   289,  3343,  2596,     6, 15221,     6,    10,  3097,     9,\n",
      "         16797,    23,     5,   589,     9,   886,     6,   764,  2659,     4,\n",
      "            44,    48, 46969,     8,  4106,  7926,  3327,     5,   169,    82,\n",
      "          8469,     8,  1026,  4158,    11, 30286, 17537,     4,    17,    46,\n",
      "         50118, 50118,   133,   892,     6,  1027,    11,     5,  8812,  3574,\n",
      "         32255,     6,   303,    14,  2777,     8,  4106,  7926,  3327,     5,\n",
      "           169,    82,  8469,     8,  1026,  4158,    11, 30286, 17537,     4,\n",
      "         50118, 50118,    17,    48, 46969,     8,  4106,  7926,  3327,     5,\n",
      "           169,    82,  8469,     8,  1026,  4158,    11, 30286, 17537,     6,\n",
      "            17,    46,   161,   892,  1029,    12, 11515,   925,     4,   871,\n",
      "           305,     4,   289,  3343,  2596,     6, 15221,     6,    10,  3097,\n",
      "             9, 16797,    23,     5,   589,     9,   886,     6,   764,  2659,\n",
      "             4,    44,    48, 46969,     8,  4106,  7926,  3327,     5,   169,\n",
      "            82,  8469,     8,  1026,  4158,    11, 30286, 17537,     4]],\n",
      "       device='cuda:0')\n",
      "text_energy_per_token: [3.4705019517634015]\n",
      "output_tokens: 219\n",
      "flop: 54760954491\n",
      "energy_consumed:  760.0399274361849\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        17.14%     336.297ms        22.18%     435.158ms      30.219us     208.611ms        55.52%     208.611ms      14.487us         14400     37031.510  \n",
      "                                               aten::mm         0.27%       5.313ms         0.36%       7.080ms      35.398us      77.581ms        20.65%      77.581ms     387.904us           200     16833.479  \n",
      "                                              aten::bmm         5.45%     106.993ms         7.11%     139.536ms      29.070us      12.392ms         3.30%      12.392ms       2.582us          4800       886.284  \n",
      "                                              aten::add         5.89%     115.530ms         8.56%     168.062ms      21.002us      12.167ms         3.24%      12.167ms       1.520us          8002         7.648  \n",
      "                                              aten::mul         2.36%      46.404ms         3.32%      65.069ms      21.668us       3.851ms         1.02%       3.851ms       1.282us          3003         2.033  \n",
      "                                            aten::empty         4.92%      96.525ms         4.92%      96.525ms       5.119us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.32%       6.352ms         5.22%     102.528ms      24.342us       0.000us         0.00%       1.715ms       0.407us          4212            --  \n",
      "                                         aten::_to_copy         0.78%      15.293ms         4.90%      96.177ms      34.288us       0.000us         0.00%       1.715ms       0.611us          2805            --  \n",
      "                                    aten::empty_strided         1.08%      21.230ms         1.08%      21.230ms       7.063us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.43%      28.142ms         3.92%      76.836ms      20.970us       2.982ms         0.79%       2.982ms       0.814us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.962s\n",
      "Self CUDA time total: 375.759ms\n",
      "\n",
      "output: tensor([[    2,  6179,   109,  2777,     8,  4106,  7926,  3327,     5,   169,\n",
      "            82,  8469,     8,  1026,  4158,    11, 30286, 17537,   116, 50118,\n",
      "         50118,   133,   892,     6,  1027,    11,     5,  8812,  3574, 32255,\n",
      "             6,   303,    14,  2777,     8,  4106,  7926,  3327,     5,   169,\n",
      "            82,  8469,     8,  1026,  4158,    11, 30286, 17537,     4, 50118,\n",
      "         50118,    17,    48, 46969,     8,  4106,  7926,  3327,     5,   169,\n",
      "            82,  8469,     8,  1026,  4158,    11, 30286, 17537,     6,    17,\n",
      "            46,   161,   892,  1029,    12, 11515,   925,     4,   871,   305,\n",
      "             4,   289,  3343,  2596,     6, 15221,     6,    10,  3097,     9,\n",
      "         16797,    23,     5,   589,     9,   886,     6,   764,  2659,     4,\n",
      "            44,    48, 46969,     8,  4106,  7926,  3327,     5,   169,    82,\n",
      "          8469,     8,  1026,  4158,    11, 30286, 17537,     4,    17,    46,\n",
      "         50118, 50118,   133,   892,     6,  1027,    11,     5,  8812,  3574,\n",
      "         32255,     6,   303,    14,  2777,     8,  4106,  7926,  3327,     5,\n",
      "           169,    82,  8469,     8,  1026,  4158,    11, 30286, 17537,     4,\n",
      "         50118, 50118,    17,    48, 46969,     8,  4106,  7926,  3327,     5,\n",
      "           169,    82,  8469,     8,  1026,  4158,    11, 30286, 17537,     6,\n",
      "            17,    46,   161,   892,  1029,    12, 11515,   925,     4,   871,\n",
      "           305,     4,   289,  3343,  2596,     6, 15221,     6,    10,  3097,\n",
      "             9, 16797,    23,     5,   589,     9,   886,     6,   764,  2659,\n",
      "             4,    44,    48, 46969,     8,  4106,  7926,  3327,     5,   169,\n",
      "            82,  8469,     8,  1026,  4158,    11, 30286, 17537,     4]],\n",
      "       device='cuda:0')\n",
      "text_energy_per_token: [3.4705019517634015, 3.1933930982578835]\n",
      "output_tokens: 219\n",
      "flop: 54760954491\n",
      "energy_consumed:  699.3530885184765\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        17.13%     340.663ms        22.17%     440.894ms      30.618us     208.703ms        55.50%     208.703ms      14.493us         14400     37371.249  \n",
      "                                               aten::mm         0.27%       5.369ms         0.36%       7.166ms      35.831us      77.596ms        20.64%      77.596ms     387.980us           200     16987.914  \n",
      "                                              aten::bmm         5.43%     108.007ms         7.08%     140.760ms      29.325us      12.427ms         3.30%      12.427ms       2.589us          4800       903.905  \n",
      "                                              aten::add         5.89%     117.144ms         8.57%     170.423ms      21.298us      12.190ms         3.24%      12.190ms       1.523us          8002         7.756  \n",
      "                                              aten::mul         2.36%      46.983ms         3.31%      65.854ms      21.930us       3.857ms         1.03%       3.857ms       1.284us          3003         2.052  \n",
      "                                            aten::empty         4.94%      98.194ms         4.94%      98.194ms       5.207us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.32%       6.408ms         5.22%     103.832ms      24.652us       0.000us         0.00%       1.714ms       0.407us          4212            --  \n",
      "                                         aten::_to_copy         0.78%      15.425ms         4.90%      97.424ms      34.732us       0.000us         0.00%       1.714ms       0.611us          2805            --  \n",
      "                                    aten::empty_strided         1.08%      21.526ms         1.08%      21.526ms       7.161us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.46%      28.966ms         3.95%      78.561ms      21.441us       2.982ms         0.79%       2.982ms       0.814us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.988s\n",
      "Self CUDA time total: 376.017ms\n",
      "\n",
      "output: tensor([[    2, 47066, 21700,    10,  5665,   147,  7350,  2316,   115,    28,\n",
      "           341,     7,  1477,     5,  1318,     8,  5838,     9,  3717,  2996,\n",
      "             4, 50118, 50118,  2264,    74,    47,   101,     7,   192,    11,\n",
      "             5,   499,   116, 50118, 50118,  2264,    74,    47,   101,     7,\n",
      "           192,    11,     5,   499,   116, 50118, 50118,  2264,    74,    47,\n",
      "           101,     7,   192,    11,     5,   499,   116, 50118, 50118,  2264,\n",
      "            74,    47,   101,     7,   192,    11,     5,   499,   116, 50118,\n",
      "         50118,  2264,    74,    47,   101,     7,   192,    11,     5,   499,\n",
      "           116, 50118, 50118,  2264,    74,    47,   101,     7,   192,    11,\n",
      "             5,   499,   116, 50118, 50118,  2264,    74,    47,   101,     7,\n",
      "           192,    11,     5,   499,   116, 50118, 50118,  2264,    74,    47,\n",
      "           101,     7,   192,    11,     5,   499,   116, 50118, 50118,  2264,\n",
      "            74,    47,   101,     7,   192,    11,     5,   499,   116, 50118,\n",
      "         50118,  2264,    74,    47,   101,     7,   192,    11,     5,   499,\n",
      "           116, 50118, 50118,  2264,    74,    47,   101,     7,   192,    11,\n",
      "             5,   499,   116, 50118, 50118,  2264,    74,    47,   101,     7,\n",
      "           192,    11,     5,   499,   116, 50118, 50118,  2264,    74,    47,\n",
      "           101,     7,   192,    11,     5,   499,   116, 50118, 50118,  2264,\n",
      "            74,    47,   101,     7,   192,    11,     5,   499,   116, 50118,\n",
      "         50118,  2264,    74,    47,   101,     7,   192,    11,     5,   499,\n",
      "           116, 50118, 50118,  2264,    74,    47,   101,     7,   192,    11,\n",
      "             5,   499,   116, 50118, 50118,  2264,    74,    47,   101,     7,\n",
      "           192]], device='cuda:0')\n",
      "text_energy_per_token: [3.2998690032566294]\n",
      "output_tokens: 221\n",
      "flop: 55272875765\n",
      "energy_consumed:  729.271049719715\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        17.14%     340.341ms        22.18%     440.364ms      30.581us     208.727ms        55.51%     208.727ms      14.495us         14400     37371.249  \n",
      "                                               aten::mm         0.27%       5.418ms         0.36%       7.177ms      35.883us      77.604ms        20.64%      77.604ms     388.019us           200     16987.914  \n",
      "                                              aten::bmm         5.46%     108.328ms         7.11%     141.136ms      29.403us      12.423ms         3.30%      12.423ms       2.588us          4800       903.905  \n",
      "                                              aten::add         5.90%     117.037ms         8.57%     170.140ms      21.262us      12.183ms         3.24%      12.183ms       1.523us          8002         7.756  \n",
      "                                              aten::mul         2.38%      47.201ms         3.33%      66.078ms      22.004us       3.850ms         1.02%       3.850ms       1.282us          3003         2.052  \n",
      "                                            aten::empty         4.93%      97.876ms         4.93%      97.876ms       5.190us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.32%       6.384ms         5.22%     103.604ms      24.597us       0.000us         0.00%       1.711ms       0.406us          4212            --  \n",
      "                                         aten::_to_copy         0.77%      15.371ms         4.90%      97.220ms      34.660us       0.000us         0.00%       1.711ms       0.610us          2805            --  \n",
      "                                    aten::empty_strided         1.08%      21.516ms         1.08%      21.516ms       7.158us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.45%      28.830ms         3.94%      78.249ms      21.356us       2.978ms         0.79%       2.978ms       0.813us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.985s\n",
      "Self CUDA time total: 376.046ms\n",
      "\n",
      "output: tensor([[    2, 47066, 21700,    10,  5665,   147,  7350,  2316,   115,    28,\n",
      "           341,     7,  1477,     5,  1318,     8,  5838,     9,  3717,  2996,\n",
      "             4, 50118, 50118,  2264,    74,    47,   101,     7,   192,    11,\n",
      "             5,   499,   116, 50118, 50118,  2264,    74,    47,   101,     7,\n",
      "           192,    11,     5,   499,   116, 50118, 50118,  2264,    74,    47,\n",
      "           101,     7,   192,    11,     5,   499,   116, 50118, 50118,  2264,\n",
      "            74,    47,   101,     7,   192,    11,     5,   499,   116, 50118,\n",
      "         50118,  2264,    74,    47,   101,     7,   192,    11,     5,   499,\n",
      "           116, 50118, 50118,  2264,    74,    47,   101,     7,   192,    11,\n",
      "             5,   499,   116, 50118, 50118,  2264,    74,    47,   101,     7,\n",
      "           192,    11,     5,   499,   116, 50118, 50118,  2264,    74,    47,\n",
      "           101,     7,   192,    11,     5,   499,   116, 50118, 50118,  2264,\n",
      "            74,    47,   101,     7,   192,    11,     5,   499,   116, 50118,\n",
      "         50118,  2264,    74,    47,   101,     7,   192,    11,     5,   499,\n",
      "           116, 50118, 50118,  2264,    74,    47,   101,     7,   192,    11,\n",
      "             5,   499,   116, 50118, 50118,  2264,    74,    47,   101,     7,\n",
      "           192,    11,     5,   499,   116, 50118, 50118,  2264,    74,    47,\n",
      "           101,     7,   192,    11,     5,   499,   116, 50118, 50118,  2264,\n",
      "            74,    47,   101,     7,   192,    11,     5,   499,   116, 50118,\n",
      "         50118,  2264,    74,    47,   101,     7,   192,    11,     5,   499,\n",
      "           116, 50118, 50118,  2264,    74,    47,   101,     7,   192,    11,\n",
      "             5,   499,   116, 50118, 50118,  2264,    74,    47,   101,     7,\n",
      "           192]], device='cuda:0')\n",
      "text_energy_per_token: [3.2998690032566294, 3.407900493657966]\n",
      "output_tokens: 221\n",
      "flop: 55272875765\n",
      "energy_consumed:  753.1460090984106\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        17.09%     338.226ms        22.17%     438.614ms      30.459us     208.995ms        55.48%     208.995ms      14.514us         14400     38220.595  \n",
      "                                               aten::mm         0.26%       5.125ms         0.36%       7.132ms      35.659us      77.611ms        20.60%      77.611ms     388.053us           200     17374.003  \n",
      "                                              aten::bmm         5.44%     107.634ms         7.10%     140.396ms      29.249us      12.546ms         3.33%      12.546ms       2.614us          4800       949.248  \n",
      "                                              aten::add         5.87%     116.246ms         8.53%     168.813ms      21.096us      12.185ms         3.23%      12.185ms       1.523us          8002         8.029  \n",
      "                                              aten::mul         2.36%      46.779ms         3.32%      65.664ms      21.866us       3.855ms         1.02%       3.855ms       1.284us          3003         2.099  \n",
      "                                            aten::empty         4.93%      97.592ms         4.93%      97.592ms       5.175us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.33%       6.446ms         5.21%     103.183ms      24.497us       0.000us         0.00%       1.716ms       0.407us          4212            --  \n",
      "                                         aten::_to_copy         0.78%      15.381ms         4.89%      96.738ms      34.488us       0.000us         0.00%       1.716ms       0.612us          2805            --  \n",
      "                                    aten::empty_strided         1.09%      21.478ms         1.09%      21.478ms       7.145us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.45%      28.672ms         3.93%      77.713ms      21.210us       2.983ms         0.79%       2.983ms       0.814us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.979s\n",
      "Self CUDA time total: 376.710ms\n",
      "\n",
      "output: tensor([[    2, 43043,  1851,     5,   609,     9, 10596,  5390,   634,  4307,\n",
      "          1729,  4454,    12, 36061,   466,   806,     6,     8,  2268,    63,\n",
      "           801,  2975,     8, 13557,  8819,     4, 50118, 50118,   133,  4307,\n",
      "          1729,  4454,    12, 36061,   466,   806,    16,    10,    92,   806,\n",
      "            14,    16,   145,  2226,    30,     5,   589,     9,   886,     6,\n",
      "         10817,     6,     8,    16,   145,   341,     7,  1045,    10,    92,\n",
      "         10596,  5390,   467,     4,    20,   806,    16,   145,   341,     7,\n",
      "          1045,    10,    92, 10596,  5390,   467,    14,    16,   145,   341,\n",
      "             7,  1045,    10,    92, 10596,  5390,   467,    14,    16,   145,\n",
      "           341,     7,  1045,    10,    92, 10596,  5390,   467,    14,    16,\n",
      "           145,   341,     7,  1045,    10,    92, 10596,  5390,   467,    14,\n",
      "            16,   145,   341,     7,  1045,    10,    92, 10596,  5390,   467,\n",
      "            14,    16,   145,   341,     7,  1045,    10,    92, 10596,  5390,\n",
      "           467,    14,    16,   145,   341,     7,  1045,    10,    92, 10596,\n",
      "          5390,   467,    14,    16,   145,   341,     7,  1045,    10,    92,\n",
      "         10596,  5390,   467,    14,    16,   145,   341,     7,  1045,    10,\n",
      "            92, 10596,  5390,   467,    14,    16,   145,   341,     7,  1045,\n",
      "            10,    92, 10596,  5390,   467,    14,    16,   145,   341,     7,\n",
      "          1045,    10,    92, 10596,  5390,   467,    14,    16,   145,   341,\n",
      "             7,  1045,    10,    92, 10596,  5390,   467,    14,    16,   145,\n",
      "           341,     7,  1045,    10,    92, 10596,  5390,   467,    14,    16,\n",
      "           145,   341,     7,  1045,    10,    92, 10596,  5390,   467,    14,\n",
      "            16,   145,   341,     7,  1045,    10]], device='cuda:0')\n",
      "text_energy_per_token: [3.3286781921137747]\n",
      "output_tokens: 226\n",
      "flop: 56553974230\n",
      "energy_consumed:  752.2812714177131\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        17.13%     338.541ms        22.19%     438.357ms      30.441us     209.048ms        55.49%     209.048ms      14.517us         14400     38220.595  \n",
      "                                               aten::mm         0.26%       5.106ms         0.36%       7.066ms      35.328us      77.607ms        20.60%      77.607ms     388.036us           200     17374.003  \n",
      "                                              aten::bmm         5.43%     107.235ms         7.07%     139.765ms      29.118us      12.555ms         3.33%      12.555ms       2.616us          4800       949.248  \n",
      "                                              aten::add         5.90%     116.640ms         8.55%     168.976ms      21.117us      12.212ms         3.24%      12.212ms       1.526us          8002         8.029  \n",
      "                                              aten::mul         2.37%      46.758ms         3.32%      65.504ms      21.813us       3.854ms         1.02%       3.854ms       1.283us          3003         2.099  \n",
      "                                            aten::empty         4.93%      97.367ms         4.93%      97.367ms       5.163us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.32%       6.408ms         5.24%     103.501ms      24.573us       0.000us         0.00%       1.718ms       0.408us          4212            --  \n",
      "                                         aten::_to_copy         0.78%      15.470ms         4.91%      97.093ms      34.614us       0.000us         0.00%       1.718ms       0.612us          2805            --  \n",
      "                                    aten::empty_strided         1.09%      21.448ms         1.09%      21.448ms       7.135us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.45%      28.673ms         3.94%      77.877ms      21.255us       2.986ms         0.79%       2.986ms       0.815us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.976s\n",
      "Self CUDA time total: 376.761ms\n",
      "\n",
      "output: tensor([[    2, 43043,  1851,     5,   609,     9, 10596,  5390,   634,  4307,\n",
      "          1729,  4454,    12, 36061,   466,   806,     6,     8,  2268,    63,\n",
      "           801,  2975,     8, 13557,  8819,     4, 50118, 50118,   133,  4307,\n",
      "          1729,  4454,    12, 36061,   466,   806,    16,    10,    92,   806,\n",
      "            14,    16,   145,  2226,    30,     5,   589,     9,   886,     6,\n",
      "         10817,     6,     8,    16,   145,   341,     7,  1045,    10,    92,\n",
      "         10596,  5390,   467,     4,    20,   806,    16,   145,   341,     7,\n",
      "          1045,    10,    92, 10596,  5390,   467,    14,    16,   145,   341,\n",
      "             7,  1045,    10,    92, 10596,  5390,   467,    14,    16,   145,\n",
      "           341,     7,  1045,    10,    92, 10596,  5390,   467,    14,    16,\n",
      "           145,   341,     7,  1045,    10,    92, 10596,  5390,   467,    14,\n",
      "            16,   145,   341,     7,  1045,    10,    92, 10596,  5390,   467,\n",
      "            14,    16,   145,   341,     7,  1045,    10,    92, 10596,  5390,\n",
      "           467,    14,    16,   145,   341,     7,  1045,    10,    92, 10596,\n",
      "          5390,   467,    14,    16,   145,   341,     7,  1045,    10,    92,\n",
      "         10596,  5390,   467,    14,    16,   145,   341,     7,  1045,    10,\n",
      "            92, 10596,  5390,   467,    14,    16,   145,   341,     7,  1045,\n",
      "            10,    92, 10596,  5390,   467,    14,    16,   145,   341,     7,\n",
      "          1045,    10,    92, 10596,  5390,   467,    14,    16,   145,   341,\n",
      "             7,  1045,    10,    92, 10596,  5390,   467,    14,    16,   145,\n",
      "           341,     7,  1045,    10,    92, 10596,  5390,   467,    14,    16,\n",
      "           145,   341,     7,  1045,    10,    92, 10596,  5390,   467,    14,\n",
      "            16,   145,   341,     7,  1045,    10]], device='cuda:0')\n",
      "text_energy_per_token: [3.3286781921137747, 3.099901904168382]\n",
      "output_tokens: 226\n",
      "flop: 56553974230\n",
      "energy_consumed:  700.5778303420543\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        17.14%     339.397ms        22.17%     438.987ms      30.485us     208.634ms        55.51%     208.634ms      14.488us         14400     37201.379  \n",
      "                                               aten::mm         0.27%       5.415ms         0.36%       7.160ms      35.801us      77.594ms        20.64%      77.594ms     387.971us           200     16910.696  \n",
      "                                              aten::bmm         5.45%     107.861ms         7.09%     140.468ms      29.264us      12.419ms         3.30%      12.419ms       2.587us          4800       895.058  \n",
      "                                              aten::add         5.89%     116.626ms         8.57%     169.778ms      21.217us      12.192ms         3.24%      12.192ms       1.524us          8002         7.702  \n",
      "                                              aten::mul         2.36%      46.825ms         3.31%      65.617ms      21.851us       3.854ms         1.03%       3.854ms       1.283us          3003         2.043  \n",
      "                                            aten::empty         4.94%      97.807ms         4.94%      97.807ms       5.187us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.33%       6.503ms         5.23%     103.525ms      24.579us       0.000us         0.00%       1.716ms       0.407us          4212            --  \n",
      "                                         aten::_to_copy         0.78%      15.362ms         4.90%      97.023ms      34.589us       0.000us         0.00%       1.716ms       0.612us          2805            --  \n",
      "                                    aten::empty_strided         1.09%      21.559ms         1.09%      21.559ms       7.172us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.45%      28.685ms         3.94%      77.975ms      21.281us       2.981ms         0.79%       2.981ms       0.813us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.980s\n",
      "Self CUDA time total: 375.869ms\n",
      "\n",
      "output: tensor([[    2,  6179,   109, 32509,   173,     7,  1744,  2172,     8,  1822,\n",
      "            31, 19166,  6357,     6,     8,    99,    16, 19400, 17381,   116,\n",
      "         50118,   133,   315,   532,    34,    57,    11,     5, 11201,     9,\n",
      "            10,   720, 23387, 14414,    13,     5,   375,   367,   107,     4,\n",
      "            20, 23387, 14414,    34,    57,    10,   720, 23387, 14414,    13,\n",
      "             5,   375,   367,   107,     6,     8,     5,   315,   532,    34,\n",
      "            57,    11,     5, 11201,     9,    10,   720, 23387, 14414,    13,\n",
      "             5,   375,   367,   107,     4,    20, 23387, 14414,    34,    57,\n",
      "            10,   720, 23387, 14414,    13,     5,   375,   367,   107,     6,\n",
      "             8,     5,   315,   532,    34,    57,    11,     5, 11201,     9,\n",
      "            10,   720, 23387, 14414,    13,     5,   375,   367,   107,     4,\n",
      "         50118,   133,   315,   532,    34,    57,    11,     5, 11201,     9,\n",
      "            10,   720, 23387, 14414,    13,     5,   375,   367,   107,     4,\n",
      "            20, 23387, 14414,    34,    57,    10,   720, 23387, 14414,    13,\n",
      "             5,   375,   367,   107,     6,     8,     5,   315,   532,    34,\n",
      "            57,    11,     5, 11201,     9,    10,   720, 23387, 14414,    13,\n",
      "             5,   375,   367,   107,     4, 50118,   133,   315,   532,    34,\n",
      "            57,    11,     5, 11201,     9,    10,   720, 23387, 14414,    13,\n",
      "             5,   375,   367,   107,     4,    20, 23387, 14414,    34,    57,\n",
      "            10,   720, 23387, 14414,    13,     5,   375,   367,   107,     6,\n",
      "             8,     5,   315,   532,    34,    57,    11,     5, 11201,     9,\n",
      "            10,   720, 23387, 14414,    13,     5,   375,   367,   107,     4]],\n",
      "       device='cuda:0')\n",
      "text_energy_per_token: [3.626892507538254]\n",
      "output_tokens: 220\n",
      "flop: 55016878120\n",
      "energy_consumed:  797.9163516584158\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        17.08%     335.369ms        22.12%     434.174ms      30.151us     208.654ms        55.51%     208.654ms      14.490us         14400     37201.379  \n",
      "                                               aten::mm         0.27%       5.334ms         0.36%       7.047ms      35.237us      77.594ms        20.64%      77.594ms     387.972us           200     16910.696  \n",
      "                                              aten::bmm         5.46%     107.134ms         7.12%     139.710ms      29.106us      12.406ms         3.30%      12.406ms       2.585us          4800       895.058  \n",
      "                                              aten::add         5.89%     115.665ms         8.56%     168.114ms      21.009us      12.182ms         3.24%      12.182ms       1.522us          8002         7.702  \n",
      "                                              aten::mul         2.37%      46.497ms         3.32%      65.139ms      21.691us       3.855ms         1.03%       3.855ms       1.284us          3003         2.043  \n",
      "                                            aten::empty         4.91%      96.337ms         4.91%      96.337ms       5.109us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.33%       6.393ms         5.26%     103.245ms      24.512us       0.000us         0.00%       1.711ms       0.406us          4212            --  \n",
      "                                         aten::_to_copy         0.78%      15.379ms         4.93%      96.851ms      34.528us       0.000us         0.00%       1.711ms       0.610us          2805            --  \n",
      "                                    aten::empty_strided         1.09%      21.416ms         1.09%      21.416ms       7.124us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.45%      28.388ms         3.94%      77.358ms      21.113us       2.979ms         0.79%       2.979ms       0.813us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.963s\n",
      "Self CUDA time total: 375.861ms\n",
      "\n",
      "output: tensor([[    2,  6179,   109, 32509,   173,     7,  1744,  2172,     8,  1822,\n",
      "            31, 19166,  6357,     6,     8,    99,    16, 19400, 17381,   116,\n",
      "         50118,   133,   315,   532,    34,    57,    11,     5, 11201,     9,\n",
      "            10,   720, 23387, 14414,    13,     5,   375,   367,   107,     4,\n",
      "            20, 23387, 14414,    34,    57,    10,   720, 23387, 14414,    13,\n",
      "             5,   375,   367,   107,     6,     8,     5,   315,   532,    34,\n",
      "            57,    11,     5, 11201,     9,    10,   720, 23387, 14414,    13,\n",
      "             5,   375,   367,   107,     4,    20, 23387, 14414,    34,    57,\n",
      "            10,   720, 23387, 14414,    13,     5,   375,   367,   107,     6,\n",
      "             8,     5,   315,   532,    34,    57,    11,     5, 11201,     9,\n",
      "            10,   720, 23387, 14414,    13,     5,   375,   367,   107,     4,\n",
      "         50118,   133,   315,   532,    34,    57,    11,     5, 11201,     9,\n",
      "            10,   720, 23387, 14414,    13,     5,   375,   367,   107,     4,\n",
      "            20, 23387, 14414,    34,    57,    10,   720, 23387, 14414,    13,\n",
      "             5,   375,   367,   107,     6,     8,     5,   315,   532,    34,\n",
      "            57,    11,     5, 11201,     9,    10,   720, 23387, 14414,    13,\n",
      "             5,   375,   367,   107,     4, 50118,   133,   315,   532,    34,\n",
      "            57,    11,     5, 11201,     9,    10,   720, 23387, 14414,    13,\n",
      "             5,   375,   367,   107,     4,    20, 23387, 14414,    34,    57,\n",
      "            10,   720, 23387, 14414,    13,     5,   375,   367,   107,     6,\n",
      "             8,     5,   315,   532,    34,    57,    11,     5, 11201,     9,\n",
      "            10,   720, 23387, 14414,    13,     5,   375,   367,   107,     4]],\n",
      "       device='cuda:0')\n",
      "text_energy_per_token: [3.626892507538254, 3.261767074310238]\n",
      "output_tokens: 220\n",
      "flop: 55016878120\n",
      "energy_consumed:  717.5887563482523\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        17.11%     340.920ms        22.18%     441.875ms      30.686us     209.040ms        55.47%     209.040ms      14.517us         14400     38390.465  \n",
      "                                               aten::mm         0.26%       5.136ms         0.36%       7.104ms      35.520us      77.604ms        20.59%      77.604ms     388.018us           200     17451.221  \n",
      "                                              aten::bmm         5.42%     108.012ms         7.07%     140.837ms      29.341us      12.576ms         3.34%      12.576ms       2.620us          4800       958.538  \n",
      "                                              aten::add         5.90%     117.494ms         8.55%     170.418ms      21.297us      12.185ms         3.23%      12.185ms       1.523us          8002         8.084  \n",
      "                                              aten::mul         2.36%      47.089ms         3.31%      65.947ms      21.960us       3.853ms         1.02%       3.853ms       1.283us          3003         2.109  \n",
      "                                            aten::empty         4.93%      98.292ms         4.93%      98.292ms       5.212us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.33%       6.552ms         5.25%     104.544ms      24.821us       0.000us         0.00%       1.719ms       0.408us          4212            --  \n",
      "                                         aten::_to_copy         0.78%      15.504ms         4.92%      97.992ms      34.935us       0.000us         0.00%       1.719ms       0.613us          2805            --  \n",
      "                                    aten::empty_strided         1.09%      21.759ms         1.09%      21.759ms       7.239us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.46%      29.181ms         3.97%      79.130ms      21.597us       2.989ms         0.79%       2.989ms       0.816us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.993s\n",
      "Self CUDA time total: 376.849ms\n",
      "\n",
      "output: tensor([[    2,  6179,   109,   592,   433,  4818,  2712,     5,   169,    82,\n",
      "         14623,     8,   458,   340,     6,     8,    99,    32,     5,   801,\n",
      "          8819,    13,     5,  2504,     9, 23038,   116, 50118, 50118,   133,\n",
      "           592,   433,  4818,    14,  2712,     5,   169,    82, 14623,     8,\n",
      "           458,   340,     6,     8,    99,    32,     5,   801,  8819,    13,\n",
      "             5,  2504,     9, 23038,   116, 50118, 50118,   133,   592,   433,\n",
      "          4818,    14,  2712,     5,   169,    82, 14623,     8,   458,   340,\n",
      "             6,     8,    99,    32,     5,   801,  8819,    13,     5,  2504,\n",
      "             9, 23038,   116, 50118, 50118,   133,   592,   433,  4818,    14,\n",
      "          2712,     5,   169,    82, 14623,     8,   458,   340,     6,     8,\n",
      "            99,    32,     5,   801,  8819,    13,     5,  2504,     9, 23038,\n",
      "           116, 50118, 50118,   133,   592,   433,  4818,    14,  2712,     5,\n",
      "           169,    82, 14623,     8,   458,   340,     6,     8,    99,    32,\n",
      "             5,   801,  8819,    13,     5,  2504,     9, 23038,   116, 50118,\n",
      "         50118,   133,   592,   433,  4818,    14,  2712,     5,   169,    82,\n",
      "         14623,     8,   458,   340,     6,     8,    99,    32,     5,   801,\n",
      "          8819,    13,     5,  2504,     9, 23038,   116, 50118, 50118,   133,\n",
      "           592,   433,  4818,    14,  2712,     5,   169,    82, 14623,     8,\n",
      "           458,   340,     6,     8,    99,    32,     5,   801,  8819,    13,\n",
      "             5,  2504,     9, 23038,   116, 50118, 50118,   133,   592,   433,\n",
      "          4818,    14,  2712,     5,   169,    82, 14623,     8,   458,   340,\n",
      "             6,     8,    99,    32,     5,   801,  8819,    13,     5,  2504,\n",
      "             9, 23038,   116, 50118, 50118,   133,   592]], device='cuda:0')\n",
      "text_energy_per_token: [3.5054251353835215]\n",
      "output_tokens: 227\n",
      "flop: 56810415971\n",
      "energy_consumed:  795.7315057320594\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        17.11%     338.548ms        22.15%     438.250ms      30.434us     209.085ms        55.49%     209.085ms      14.520us         14400     38390.465  \n",
      "                                               aten::mm         0.26%       5.150ms         0.36%       7.141ms      35.703us      77.597ms        20.59%      77.597ms     387.986us           200     17451.221  \n",
      "                                              aten::bmm         5.43%     107.460ms         7.08%     140.030ms      29.173us      12.569ms         3.34%      12.569ms       2.619us          4800       958.538  \n",
      "                                              aten::add         5.91%     116.873ms         8.57%     169.544ms      21.188us      12.181ms         3.23%      12.181ms       1.522us          8002         8.084  \n",
      "                                              aten::mul         2.36%      46.754ms         3.31%      65.528ms      21.821us       3.850ms         1.02%       3.850ms       1.282us          3003         2.109  \n",
      "                                            aten::empty         4.93%      97.612ms         4.93%      97.612ms       5.176us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.32%       6.406ms         5.24%     103.587ms      24.593us       0.000us         0.00%       1.715ms       0.407us          4212            --  \n",
      "                                         aten::_to_copy         0.78%      15.384ms         4.91%      97.181ms      34.646us       0.000us         0.00%       1.715ms       0.611us          2805            --  \n",
      "                                    aten::empty_strided         1.08%      21.421ms         1.08%      21.421ms       7.126us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.45%      28.615ms         3.94%      78.015ms      21.292us       2.984ms         0.79%       2.984ms       0.814us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.978s\n",
      "Self CUDA time total: 376.829ms\n",
      "\n",
      "output: tensor([[    2,  6179,   109,   592,   433,  4818,  2712,     5,   169,    82,\n",
      "         14623,     8,   458,   340,     6,     8,    99,    32,     5,   801,\n",
      "          8819,    13,     5,  2504,     9, 23038,   116, 50118, 50118,   133,\n",
      "           592,   433,  4818,    14,  2712,     5,   169,    82, 14623,     8,\n",
      "           458,   340,     6,     8,    99,    32,     5,   801,  8819,    13,\n",
      "             5,  2504,     9, 23038,   116, 50118, 50118,   133,   592,   433,\n",
      "          4818,    14,  2712,     5,   169,    82, 14623,     8,   458,   340,\n",
      "             6,     8,    99,    32,     5,   801,  8819,    13,     5,  2504,\n",
      "             9, 23038,   116, 50118, 50118,   133,   592,   433,  4818,    14,\n",
      "          2712,     5,   169,    82, 14623,     8,   458,   340,     6,     8,\n",
      "            99,    32,     5,   801,  8819,    13,     5,  2504,     9, 23038,\n",
      "           116, 50118, 50118,   133,   592,   433,  4818,    14,  2712,     5,\n",
      "           169,    82, 14623,     8,   458,   340,     6,     8,    99,    32,\n",
      "             5,   801,  8819,    13,     5,  2504,     9, 23038,   116, 50118,\n",
      "         50118,   133,   592,   433,  4818,    14,  2712,     5,   169,    82,\n",
      "         14623,     8,   458,   340,     6,     8,    99,    32,     5,   801,\n",
      "          8819,    13,     5,  2504,     9, 23038,   116, 50118, 50118,   133,\n",
      "           592,   433,  4818,    14,  2712,     5,   169,    82, 14623,     8,\n",
      "           458,   340,     6,     8,    99,    32,     5,   801,  8819,    13,\n",
      "             5,  2504,     9, 23038,   116, 50118, 50118,   133,   592,   433,\n",
      "          4818,    14,  2712,     5,   169,    82, 14623,     8,   458,   340,\n",
      "             6,     8,    99,    32,     5,   801,  8819,    13,     5,  2504,\n",
      "             9, 23038,   116, 50118, 50118,   133,   592]], device='cuda:0')\n",
      "text_energy_per_token: [3.5054251353835215, 3.2209519055408524]\n",
      "output_tokens: 227\n",
      "flop: 56810415971\n",
      "energy_consumed:  731.1560825577735\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        17.08%     339.523ms        22.14%     440.105ms      30.563us     209.106ms        55.47%     209.106ms      14.521us         14400     38560.334  \n",
      "                                               aten::mm         0.26%       5.182ms         0.36%       7.197ms      35.983us      77.614ms        20.59%      77.614ms     388.069us           200     17528.439  \n",
      "                                              aten::bmm         5.43%     107.985ms         7.08%     140.817ms      29.337us      12.578ms         3.34%      12.578ms       2.620us          4800       967.901  \n",
      "                                              aten::add         5.88%     116.933ms         8.53%     169.567ms      21.191us      12.196ms         3.24%      12.196ms       1.524us          8002         8.140  \n",
      "                                              aten::mul         2.36%      46.970ms         3.31%      65.881ms      21.938us       3.855ms         1.02%       3.855ms       1.284us          3003         2.118  \n",
      "                                            aten::empty         4.93%      98.076ms         4.93%      98.076ms       5.201us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.33%       6.476ms         5.24%     104.249ms      24.750us       0.000us         0.00%       1.715ms       0.407us          4212            --  \n",
      "                                         aten::_to_copy         0.78%      15.486ms         4.92%      97.772ms      34.856us       0.000us         0.00%       1.715ms       0.611us          2805            --  \n",
      "                                    aten::empty_strided         1.09%      21.691ms         1.09%      21.691ms       7.216us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.45%      28.882ms         3.96%      78.633ms      21.461us       2.986ms         0.79%       2.986ms       0.815us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.988s\n",
      "Self CUDA time total: 376.968ms\n",
      "\n",
      "output: tensor([[    2,  6179,   109,  4106,     6,   592,     6,     8,   776,  2433,\n",
      "          2712,    82,    18,   689,  5717,     6,     8,   141,    64,    42,\n",
      "          2655,    28,   341,     7,  3720, 12732, 22669,   116, 50118, 50118,\n",
      "           133,   892,    21,  2964,    30,     5,   589,     9,   886,     6,\n",
      "         10817,     6,     8,     5,   589,     9,   886,     6, 10817,     6,\n",
      "             8,    21,  1027,    11,     5,  8812, 20056,     4, 50118, 50118,\n",
      "           133,   892,    21,  2964,    30,     5,   589,     9,   886,     6,\n",
      "         10817,     6,     8,     5,   589,     9,   886,     6, 10817,     6,\n",
      "             8,    21,  1027,    11,     5,  8812, 20056,     4, 50118, 50118,\n",
      "           133,   892,    21,  2964,    30,     5,   589,     9,   886,     6,\n",
      "         10817,     6,     8,     5,   589,     9,   886,     6, 10817,     6,\n",
      "             8,    21,  1027,    11,     5,  8812, 20056,     4, 50118, 50118,\n",
      "           133,   892,    21,  2964,    30,     5,   589,     9,   886,     6,\n",
      "         10817,     6,     8,     5,   589,     9,   886,     6, 10817,     6,\n",
      "             8,    21,  1027,    11,     5,  8812, 20056,     4, 50118, 50118,\n",
      "           133,   892,    21,  2964,    30,     5,   589,     9,   886,     6,\n",
      "         10817,     6,     8,     5,   589,     9,   886,     6, 10817,     6,\n",
      "             8,    21,  1027,    11,     5,  8812, 20056,     4, 50118, 50118,\n",
      "           133,   892,    21,  2964,    30,     5,   589,     9,   886,     6,\n",
      "         10817,     6,     8,     5,   589,     9,   886,     6, 10817,     6,\n",
      "             8,    21,  1027,    11,     5,  8812, 20056,     4, 50118, 50118,\n",
      "           133,   892,    21,  2964,    30,     5,   589,     9,   886,     6,\n",
      "         10817,     6,     8,     5,   589,     9,   886,     6]],\n",
      "       device='cuda:0')\n",
      "text_energy_per_token: [3.226557815039785]\n",
      "output_tokens: 228\n",
      "flop: 57066931728\n",
      "energy_consumed:  735.655181829071\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        17.08%     337.989ms        22.15%     438.403ms      30.445us     209.119ms        55.48%     209.119ms      14.522us         14400     38560.334  \n",
      "                                               aten::mm         0.26%       5.081ms         0.36%       7.100ms      35.500us      77.606ms        20.59%      77.606ms     388.029us           200     17528.439  \n",
      "                                              aten::bmm         5.42%     107.342ms         7.07%     139.994ms      29.165us      12.580ms         3.34%      12.580ms       2.621us          4800       967.901  \n",
      "                                              aten::add         5.89%     116.625ms         8.54%     169.108ms      21.133us      12.187ms         3.23%      12.187ms       1.523us          8002         8.140  \n",
      "                                              aten::mul         2.37%      46.845ms         3.32%      65.751ms      21.895us       3.853ms         1.02%       3.853ms       1.283us          3003         2.118  \n",
      "                                            aten::empty         4.93%      97.633ms         4.93%      97.633ms       5.177us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.32%       6.422ms         5.25%     103.908ms      24.669us       0.000us         0.00%       1.708ms       0.405us          4212            --  \n",
      "                                         aten::_to_copy         0.78%      15.416ms         4.93%      97.486ms      34.754us       0.000us         0.00%       1.708ms       0.609us          2805            --  \n",
      "                                    aten::empty_strided         1.09%      21.591ms         1.09%      21.591ms       7.183us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.46%      28.993ms         3.97%      78.655ms      21.467us       2.974ms         0.79%       2.974ms       0.812us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.979s\n",
      "Self CUDA time total: 376.914ms\n",
      "\n",
      "output: tensor([[    2,  6179,   109,  4106,     6,   592,     6,     8,   776,  2433,\n",
      "          2712,    82,    18,   689,  5717,     6,     8,   141,    64,    42,\n",
      "          2655,    28,   341,     7,  3720, 12732, 22669,   116, 50118, 50118,\n",
      "           133,   892,    21,  2964,    30,     5,   589,     9,   886,     6,\n",
      "         10817,     6,     8,     5,   589,     9,   886,     6, 10817,     6,\n",
      "             8,    21,  1027,    11,     5,  8812, 20056,     4, 50118, 50118,\n",
      "           133,   892,    21,  2964,    30,     5,   589,     9,   886,     6,\n",
      "         10817,     6,     8,     5,   589,     9,   886,     6, 10817,     6,\n",
      "             8,    21,  1027,    11,     5,  8812, 20056,     4, 50118, 50118,\n",
      "           133,   892,    21,  2964,    30,     5,   589,     9,   886,     6,\n",
      "         10817,     6,     8,     5,   589,     9,   886,     6, 10817,     6,\n",
      "             8,    21,  1027,    11,     5,  8812, 20056,     4, 50118, 50118,\n",
      "           133,   892,    21,  2964,    30,     5,   589,     9,   886,     6,\n",
      "         10817,     6,     8,     5,   589,     9,   886,     6, 10817,     6,\n",
      "             8,    21,  1027,    11,     5,  8812, 20056,     4, 50118, 50118,\n",
      "           133,   892,    21,  2964,    30,     5,   589,     9,   886,     6,\n",
      "         10817,     6,     8,     5,   589,     9,   886,     6, 10817,     6,\n",
      "             8,    21,  1027,    11,     5,  8812, 20056,     4, 50118, 50118,\n",
      "           133,   892,    21,  2964,    30,     5,   589,     9,   886,     6,\n",
      "         10817,     6,     8,     5,   589,     9,   886,     6, 10817,     6,\n",
      "             8,    21,  1027,    11,     5,  8812, 20056,     4, 50118, 50118,\n",
      "           133,   892,    21,  2964,    30,     5,   589,     9,   886,     6,\n",
      "         10817,     6,     8,     5,   589,     9,   886,     6]],\n",
      "       device='cuda:0')\n",
      "text_energy_per_token: [3.226557815039785, 3.1683891216635707]\n",
      "output_tokens: 228\n",
      "flop: 57066931728\n",
      "energy_consumed:  722.3927197392941\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        17.15%     338.702ms        22.20%     438.355ms      30.441us     208.651ms        55.51%     208.651ms      14.490us         14400     37201.379  \n",
      "                                               aten::mm         0.27%       5.388ms         0.36%       7.140ms      35.700us      77.598ms        20.64%      77.598ms     387.992us           200     16910.696  \n",
      "                                              aten::bmm         5.46%     107.744ms         7.11%     140.431ms      29.256us      12.408ms         3.30%      12.408ms       2.585us          4800       895.058  \n",
      "                                              aten::add         5.89%     116.251ms         8.56%     168.993ms      21.119us      12.184ms         3.24%      12.184ms       1.523us          8002         7.702  \n",
      "                                              aten::mul         2.37%      46.803ms         3.32%      65.505ms      21.813us       3.855ms         1.03%       3.855ms       1.284us          3003         2.043  \n",
      "                                            aten::empty         4.90%      96.870ms         4.90%      96.870ms       5.137us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.32%       6.387ms         5.24%     103.533ms      24.580us       0.000us         0.00%       1.713ms       0.407us          4212            --  \n",
      "                                         aten::_to_copy         0.78%      15.324ms         4.92%      97.146ms      34.633us       0.000us         0.00%       1.713ms       0.611us          2805            --  \n",
      "                                    aten::empty_strided         1.09%      21.458ms         1.09%      21.458ms       7.139us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.44%      28.515ms         3.94%      77.808ms      21.236us       2.981ms         0.79%       2.981ms       0.813us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.975s\n",
      "Self CUDA time total: 375.872ms\n",
      "\n",
      "output: tensor([[    2, 43043,  1851,     5,   609,     9,  1632,  4230,     8,   141,\n",
      "            24, 17992,     7,     5, 10795,     8, 14082,     9,  4707,     4,\n",
      "         50118, 50118, 43043,  1851,     5,   609,     9,  1632,  4230,     8,\n",
      "           141,    24, 17992,     7,     5, 10795,     8, 14082,     9,  4707,\n",
      "             4, 50118, 50118, 43043,  1851,     5,   609,     9,  1632,  4230,\n",
      "             8,   141,    24, 17992,     7,     5, 10795,     8, 14082,     9,\n",
      "          4707,     4, 50118, 50118, 43043,  1851,     5,   609,     9,  1632,\n",
      "          4230,     8,   141,    24, 17992,     7,     5, 10795,     8, 14082,\n",
      "             9,  4707,     4, 50118, 50118, 43043,  1851,     5,   609,     9,\n",
      "          1632,  4230,     8,   141,    24, 17992,     7,     5, 10795,     8,\n",
      "         14082,     9,  4707,     4, 50118, 50118, 43043,  1851,     5,   609,\n",
      "             9,  1632,  4230,     8,   141,    24, 17992,     7,     5, 10795,\n",
      "             8, 14082,     9,  4707,     4, 50118, 50118, 43043,  1851,     5,\n",
      "           609,     9,  1632,  4230,     8,   141,    24, 17992,     7,     5,\n",
      "         10795,     8, 14082,     9,  4707,     4, 50118, 50118, 43043,  1851,\n",
      "             5,   609,     9,  1632,  4230,     8,   141,    24, 17992,     7,\n",
      "             5, 10795,     8, 14082,     9,  4707,     4, 50118, 50118, 43043,\n",
      "          1851,     5,   609,     9,  1632,  4230,     8,   141,    24, 17992,\n",
      "             7,     5, 10795,     8, 14082,     9,  4707,     4, 50118, 50118,\n",
      "         43043,  1851,     5,   609,     9,  1632,  4230,     8,   141,    24,\n",
      "         17992,     7,     5, 10795,     8, 14082,     9,  4707,     4, 50118,\n",
      "         50118, 43043,  1851,     5,   609,     9,  1632,  4230,     8,   141]],\n",
      "       device='cuda:0')\n",
      "text_energy_per_token: [3.3962486248640587]\n",
      "output_tokens: 220\n",
      "flop: 55016878120\n",
      "energy_consumed:  747.174697470093\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        17.12%     339.039ms        22.17%     438.971ms      30.484us     208.610ms        55.51%     208.610ms      14.487us         14400     37201.379  \n",
      "                                               aten::mm         0.27%       5.391ms         0.36%       7.151ms      35.757us      77.595ms        20.65%      77.595ms     387.974us           200     16910.696  \n",
      "                                              aten::bmm         5.45%     107.964ms         7.10%     140.567ms      29.285us      12.408ms         3.30%      12.408ms       2.585us          4800       895.058  \n",
      "                                              aten::add         5.89%     116.617ms         8.56%     169.505ms      21.183us      12.174ms         3.24%      12.174ms       1.521us          8002         7.702  \n",
      "                                              aten::mul         2.37%      46.939ms         3.32%      65.808ms      21.914us       3.851ms         1.02%       3.851ms       1.282us          3003         2.043  \n",
      "                                            aten::empty         4.90%      97.091ms         4.90%      97.091ms       5.149us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.32%       6.352ms         5.24%     103.852ms      24.656us       0.000us         0.00%       1.715ms       0.407us          4212            --  \n",
      "                                         aten::_to_copy         0.78%      15.455ms         4.92%      97.500ms      34.759us       0.000us         0.00%       1.715ms       0.611us          2805            --  \n",
      "                                    aten::empty_strided         1.09%      21.485ms         1.09%      21.485ms       7.147us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.45%      28.801ms         3.95%      78.203ms      21.344us       2.980ms         0.79%       2.980ms       0.813us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.980s\n",
      "Self CUDA time total: 375.814ms\n",
      "\n",
      "output: tensor([[    2, 43043,  1851,     5,   609,     9,  1632,  4230,     8,   141,\n",
      "            24, 17992,     7,     5, 10795,     8, 14082,     9,  4707,     4,\n",
      "         50118, 50118, 43043,  1851,     5,   609,     9,  1632,  4230,     8,\n",
      "           141,    24, 17992,     7,     5, 10795,     8, 14082,     9,  4707,\n",
      "             4, 50118, 50118, 43043,  1851,     5,   609,     9,  1632,  4230,\n",
      "             8,   141,    24, 17992,     7,     5, 10795,     8, 14082,     9,\n",
      "          4707,     4, 50118, 50118, 43043,  1851,     5,   609,     9,  1632,\n",
      "          4230,     8,   141,    24, 17992,     7,     5, 10795,     8, 14082,\n",
      "             9,  4707,     4, 50118, 50118, 43043,  1851,     5,   609,     9,\n",
      "          1632,  4230,     8,   141,    24, 17992,     7,     5, 10795,     8,\n",
      "         14082,     9,  4707,     4, 50118, 50118, 43043,  1851,     5,   609,\n",
      "             9,  1632,  4230,     8,   141,    24, 17992,     7,     5, 10795,\n",
      "             8, 14082,     9,  4707,     4, 50118, 50118, 43043,  1851,     5,\n",
      "           609,     9,  1632,  4230,     8,   141,    24, 17992,     7,     5,\n",
      "         10795,     8, 14082,     9,  4707,     4, 50118, 50118, 43043,  1851,\n",
      "             5,   609,     9,  1632,  4230,     8,   141,    24, 17992,     7,\n",
      "             5, 10795,     8, 14082,     9,  4707,     4, 50118, 50118, 43043,\n",
      "          1851,     5,   609,     9,  1632,  4230,     8,   141,    24, 17992,\n",
      "             7,     5, 10795,     8, 14082,     9,  4707,     4, 50118, 50118,\n",
      "         43043,  1851,     5,   609,     9,  1632,  4230,     8,   141,    24,\n",
      "         17992,     7,     5, 10795,     8, 14082,     9,  4707,     4, 50118,\n",
      "         50118, 43043,  1851,     5,   609,     9,  1632,  4230,     8,   141]],\n",
      "       device='cuda:0')\n",
      "text_energy_per_token: [3.3962486248640587, 3.3135943751784893]\n",
      "output_tokens: 220\n",
      "flop: 55016878120\n",
      "energy_consumed:  728.9907625392676\n",
      "Processing category: common-sense\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        17.12%     341.362ms        22.16%     441.908ms      30.688us     208.917ms        55.48%     208.917ms      14.508us         14400     38050.726  \n",
      "                                               aten::mm         0.27%       5.413ms         0.36%       7.183ms      35.917us      77.601ms        20.61%      77.601ms     388.007us           200     17296.785  \n",
      "                                              aten::bmm         5.45%     108.615ms         7.10%     141.616ms      29.503us      12.526ms         3.33%      12.526ms       2.610us          4800       940.032  \n",
      "                                              aten::add         5.90%     117.680ms         8.59%     171.242ms      21.400us      12.195ms         3.24%      12.195ms       1.524us          8002         7.973  \n",
      "                                              aten::mul         2.37%      47.240ms         3.32%      66.199ms      22.044us       3.853ms         1.02%       3.853ms       1.283us          3003         2.090  \n",
      "                                            aten::empty         4.91%      97.975ms         4.91%      97.975ms       5.195us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.32%       6.425ms         5.23%     104.252ms      24.751us       0.000us         0.00%       1.717ms       0.408us          4212            --  \n",
      "                                         aten::_to_copy         0.78%      15.507ms         4.91%      97.827ms      34.876us       0.000us         0.00%       1.717ms       0.612us          2805            --  \n",
      "                                    aten::empty_strided         1.08%      21.567ms         1.08%      21.567ms       7.175us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.45%      29.005ms         3.95%      78.815ms      21.511us       2.985ms         0.79%       2.985ms       0.815us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.994s\n",
      "Self CUDA time total: 376.568ms\n",
      "\n",
      "output: tensor([[    2,  6179,    64,    47,  3094,   114,    10,  2391,    16,  1406,\n",
      "           566,  8803,    50,  4412, 21538,  6349,     6,     8,   596,   429,\n",
      "            42,   335,    28,  5616,   116, 50118, 50118,   133,  1049,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349,     4, 50118, 50118,   133,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349,     4, 50118, 50118,   133,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349,     4, 50118, 50118,   133,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349,     4, 50118, 50118,   133,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349,     4, 50118, 50118,   133,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349,     4, 50118, 50118,   133,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349,     4, 50118, 50118,   133,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349,     4, 50118, 50118,   133,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349,     4, 50118, 50118,   133,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349]], device='cuda:0')\n",
      "text_energy_per_token: [3.339009918855349]\n",
      "output_tokens: 225\n",
      "flop: 56297606505\n",
      "energy_consumed:  751.2772317424535\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        17.11%     338.072ms        22.15%     437.770ms      30.401us     208.945ms        55.49%     208.945ms      14.510us         14400     38050.726  \n",
      "                                               aten::mm         0.27%       5.307ms         0.36%       7.018ms      35.091us      77.601ms        20.61%      77.601ms     388.003us           200     17296.785  \n",
      "                                              aten::bmm         5.46%     107.830ms         7.11%     140.551ms      29.282us      12.517ms         3.32%      12.517ms       2.608us          4800       940.032  \n",
      "                                              aten::add         5.89%     116.457ms         8.56%     169.214ms      21.146us      12.208ms         3.24%      12.208ms       1.526us          8002         7.973  \n",
      "                                              aten::mul         2.36%      46.731ms         3.32%      65.614ms      21.849us       3.854ms         1.02%       3.854ms       1.283us          3003         2.090  \n",
      "                                            aten::empty         4.92%      97.319ms         4.92%      97.319ms       5.161us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.32%       6.356ms         5.22%     103.243ms      24.512us       0.000us         0.00%       1.709ms       0.406us          4212            --  \n",
      "                                         aten::_to_copy         0.78%      15.377ms         4.90%      96.887ms      34.541us       0.000us         0.00%       1.709ms       0.609us          2805            --  \n",
      "                                    aten::empty_strided         1.08%      21.362ms         1.08%      21.362ms       7.106us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.46%      28.781ms         3.95%      78.008ms      21.290us       2.975ms         0.79%       2.975ms       0.812us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.976s\n",
      "Self CUDA time total: 376.565ms\n",
      "\n",
      "output: tensor([[    2,  6179,    64,    47,  3094,   114,    10,  2391,    16,  1406,\n",
      "           566,  8803,    50,  4412, 21538,  6349,     6,     8,   596,   429,\n",
      "            42,   335,    28,  5616,   116, 50118, 50118,   133,  1049,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349,     4, 50118, 50118,   133,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349,     4, 50118, 50118,   133,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349,     4, 50118, 50118,   133,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349,     4, 50118, 50118,   133,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349,     4, 50118, 50118,   133,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349,     4, 50118, 50118,   133,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349,     4, 50118, 50118,   133,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349,     4, 50118, 50118,   133,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349,     4, 50118, 50118,   133,  1219,\n",
      "            13,    42,    16,    14,     5,  8803,    32,    55,   533,     7,\n",
      "           825,    10,  2391,    87,  6349]], device='cuda:0')\n",
      "text_energy_per_token: [3.339009918855349, 3.161703645190769]\n",
      "output_tokens: 225\n",
      "flop: 56297606505\n",
      "energy_consumed:  711.383320167923\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total KFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.70%      35.287ms        21.95%      46.374ms      35.782us      19.402ms        56.48%      19.402ms      14.970us          1296   7304380.416  \n",
      "                                               aten::mm         0.25%     530.678us         0.34%     711.818us      39.545us       7.201ms        20.96%       7.201ms     400.081us            18   3320365.056  \n",
      "                                              aten::bmm         5.24%      11.082ms         6.82%      14.417ms      33.374us       1.080ms         3.15%       1.080ms       2.501us           432     46854.144  \n",
      "                                              aten::add         5.71%      12.074ms         8.27%      17.482ms      24.213us       1.106ms         3.22%       1.106ms       1.532us           722      1008.755  \n",
      "                                              aten::mul         2.35%       4.971ms         3.32%       7.011ms      25.683us     352.354us         1.03%     352.354us       1.291us           273       396.998  \n",
      "                                            aten::empty         5.11%      10.787ms         5.11%      10.787ms       6.164us       0.000us         0.00%       0.000us       0.000us          1750            --  \n",
      "                                               aten::to         0.31%     657.522us         5.62%      11.880ms      30.462us       0.000us         0.00%     157.000us       0.403us           390            --  \n",
      "                                         aten::_to_copy         0.79%       1.675ms         5.31%      11.223ms      43.668us       0.000us         0.00%     157.000us       0.611us           257            --  \n",
      "                                    aten::empty_strided         1.32%       2.796ms         1.32%       2.796ms      10.129us       0.000us         0.00%       0.000us       0.000us           276            --  \n",
      "                                            aten::copy_         1.91%       4.033ms         4.90%      10.350ms      26.675us     369.896us         1.08%     369.896us       0.953us           388            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 211.298ms\n",
      "Self CUDA time total: 34.351ms\n",
      "\n",
      "output: tensor([[    2,  2264,    32,   103, 12405, 14885,    14,  3608,   951,    16,\n",
      "         23748,     7,  1346,    10,  5674,    50,  1607,    77,    51,    32,\n",
      "           888, 10985,    50, 21969, 10312,   116, 50118,   100,   206,    24,\n",
      "            18,   142,    51,   214,    45,   269,   686,    99,    51,   214,\n",
      "          1686,    59,     4,     2]], device='cuda:0')\n",
      "text_energy_per_token: [1.888525618733059]\n",
      "output_tokens: 44\n",
      "flop: 10673005369\n",
      "energy_consumed:  83.0951272242546\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total KFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        16.72%      33.782ms        21.99%      44.420ms      34.274us      19.419ms        56.52%      19.419ms      14.984us          1296   7304380.416  \n",
      "                                               aten::mm         0.25%     497.414us         0.33%     667.647us      37.092us       7.199ms        20.96%       7.199ms     399.960us            18   3320365.056  \n",
      "                                              aten::bmm         5.23%      10.573ms         6.83%      13.797ms      31.938us       1.077ms         3.14%       1.077ms       2.493us           432     46854.144  \n",
      "                                              aten::add         5.74%      11.594ms         8.30%      16.773ms      23.231us       1.104ms         3.21%       1.104ms       1.529us           722      1008.755  \n",
      "                                              aten::mul         2.32%       4.691ms         3.30%       6.669ms      24.427us     351.605us         1.02%     351.605us       1.288us           273       396.998  \n",
      "                                            aten::empty         5.18%      10.455ms         5.18%      10.455ms       5.975us       0.000us         0.00%       0.000us       0.000us          1750            --  \n",
      "                                               aten::to         0.31%     620.783us         5.54%      11.196ms      28.707us       0.000us         0.00%     157.054us       0.403us           390            --  \n",
      "                                         aten::_to_copy         0.79%       1.596ms         5.23%      10.575ms      41.147us       0.000us         0.00%     157.054us       0.611us           257            --  \n",
      "                                    aten::empty_strided         1.30%       2.623ms         1.30%       2.623ms       9.502us       0.000us         0.00%       0.000us       0.000us           276            --  \n",
      "                                            aten::copy_         1.91%       3.868ms         4.84%       9.777ms      25.198us     369.981us         1.08%     369.981us       0.954us           388            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 202.021ms\n",
      "Self CUDA time total: 34.355ms\n",
      "\n",
      "output: tensor([[    2,  2264,    32,   103, 12405, 14885,    14,  3608,   951,    16,\n",
      "         23748,     7,  1346,    10,  5674,    50,  1607,    77,    51,    32,\n",
      "           888, 10985,    50, 21969, 10312,   116, 50118,   100,   206,    24,\n",
      "            18,   142,    51,   214,    45,   269,   686,    99,    51,   214,\n",
      "          1686,    59,     4,     2]], device='cuda:0')\n",
      "text_energy_per_token: [1.888525618733059, 2.0773342240832067]\n",
      "output_tokens: 44\n",
      "flop: 10673005369\n",
      "energy_consumed:  91.4027058596611\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        17.12%     339.250ms        22.14%     438.755ms      30.469us     208.910ms        55.48%     208.910ms      14.508us         14400     38050.726  \n",
      "                                               aten::mm         0.27%       5.339ms         0.36%       7.126ms      35.631us      77.615ms        20.61%      77.615ms     388.077us           200     17296.785  \n",
      "                                              aten::bmm         5.45%     108.072ms         7.11%     140.825ms      29.338us      12.527ms         3.33%      12.527ms       2.610us          4800       940.032  \n",
      "                                              aten::add         5.90%     116.841ms         8.58%     169.969ms      21.241us      12.189ms         3.24%      12.189ms       1.523us          8002         7.973  \n",
      "                                              aten::mul         2.37%      46.975ms         3.32%      65.892ms      21.942us       3.856ms         1.02%       3.856ms       1.284us          3003         2.090  \n",
      "                                            aten::empty         4.93%      97.695ms         4.93%      97.695ms       5.181us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.32%       6.382ms         5.27%     104.446ms      24.797us       0.000us         0.00%       1.715ms       0.407us          4212            --  \n",
      "                                         aten::_to_copy         0.78%      15.379ms         4.95%      98.063ms      34.960us       0.000us         0.00%       1.715ms       0.611us          2805            --  \n",
      "                                    aten::empty_strided         1.09%      21.529ms         1.09%      21.529ms       7.162us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.46%      28.887ms         3.98%      78.841ms      21.518us       2.985ms         0.79%       2.985ms       0.815us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.982s\n",
      "Self CUDA time total: 376.549ms\n",
      "\n",
      "output: tensor([[    2,  7608,   429,   951,  2807,     7,   304,    10,  2225,  5456,\n",
      "            50,  1394,    13,  9969,  1386,     9, 13304,    15,    10, 11921,\n",
      "          2187,    50,  4368,  1553,   116, 50118, 10105,    24,    18,  3013,\n",
      "             7,   304,    10,  4368,  1553,     4, 50118,   100,   437,    45,\n",
      "           584,    24,    18,  3013,     6,    38,   437,   584,    24,    18,\n",
      "          3013,     7,   304,    10,  2225,  5456,     4, 50118,   100,   437,\n",
      "           584,    24,    18,  3013,     7,   304,    10,  2225,  5456,     4,\n",
      "         50118,   100,   437,   584,    24,    18,  3013,     7,   304,    10,\n",
      "          2225,  5456,     4, 50118,   100,   437,   584,    24,    18,  3013,\n",
      "             7,   304,    10,  2225,  5456,     4, 50118,   100,   437,   584,\n",
      "            24,    18,  3013,     7,   304,    10,  2225,  5456,     4, 50118,\n",
      "           100,   437,   584,    24,    18,  3013,     7,   304,    10,  2225,\n",
      "          5456,     4, 50118,   100,   437,   584,    24,    18,  3013,     7,\n",
      "           304,    10,  2225,  5456,     4, 50118,   100,   437,   584,    24,\n",
      "            18,  3013,     7,   304,    10,  2225,  5456,     4, 50118,   100,\n",
      "           437,   584,    24,    18,  3013,     7,   304,    10,  2225,  5456,\n",
      "             4, 50118,   100,   437,   584,    24,    18,  3013,     7,   304,\n",
      "            10,  2225,  5456,     4, 50118,   100,   437,   584,    24,    18,\n",
      "          3013,     7,   304,    10,  2225,  5456,     4, 50118,   100,   437,\n",
      "           584,    24,    18,  3013,     7,   304,    10,  2225,  5456,     4,\n",
      "         50118,   100,   437,   584,    24,    18,  3013,     7,   304,    10,\n",
      "          2225,  5456,     4, 50118,   100,   437,   584,    24,    18,  3013,\n",
      "             7,   304,    10,  2225,  5456]], device='cuda:0')\n",
      "text_energy_per_token: [3.8262610950684013]\n",
      "output_tokens: 225\n",
      "flop: 56297606505\n",
      "energy_consumed:  860.9087463903903\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        17.10%     340.237ms        22.14%     440.586ms      30.596us     208.911ms        55.48%     208.911ms      14.508us         14400     38050.726  \n",
      "                                               aten::mm         0.27%       5.356ms         0.36%       7.117ms      35.586us      77.609ms        20.61%      77.609ms     388.045us           200     17296.785  \n",
      "                                              aten::bmm         5.45%     108.526ms         7.11%     141.517ms      29.483us      12.538ms         3.33%      12.538ms       2.612us          4800       940.032  \n",
      "                                              aten::add         5.91%     117.536ms         8.58%     170.809ms      21.346us      12.203ms         3.24%      12.203ms       1.525us          8002         7.973  \n",
      "                                              aten::mul         2.37%      47.229ms         3.32%      66.017ms      21.984us       3.854ms         1.02%       3.854ms       1.283us          3003         2.090  \n",
      "                                            aten::empty         4.92%      97.941ms         4.92%      97.941ms       5.194us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.33%       6.470ms         5.28%     105.002ms      24.929us       0.000us         0.00%       1.711ms       0.406us          4212            --  \n",
      "                                         aten::_to_copy         0.78%      15.545ms         4.95%      98.532ms      35.127us       0.000us         0.00%       1.711ms       0.610us          2805            --  \n",
      "                                    aten::empty_strided         1.09%      21.705ms         1.09%      21.705ms       7.220us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.46%      29.046ms         3.98%      79.128ms      21.596us       2.977ms         0.79%       2.977ms       0.813us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.990s\n",
      "Self CUDA time total: 376.531ms\n",
      "\n",
      "output: tensor([[    2,  7608,   429,   951,  2807,     7,   304,    10,  2225,  5456,\n",
      "            50,  1394,    13,  9969,  1386,     9, 13304,    15,    10, 11921,\n",
      "          2187,    50,  4368,  1553,   116, 50118, 10105,    24,    18,  3013,\n",
      "             7,   304,    10,  4368,  1553,     4, 50118,   100,   437,    45,\n",
      "           584,    24,    18,  3013,     6,    38,   437,   584,    24,    18,\n",
      "          3013,     7,   304,    10,  2225,  5456,     4, 50118,   100,   437,\n",
      "           584,    24,    18,  3013,     7,   304,    10,  2225,  5456,     4,\n",
      "         50118,   100,   437,   584,    24,    18,  3013,     7,   304,    10,\n",
      "          2225,  5456,     4, 50118,   100,   437,   584,    24,    18,  3013,\n",
      "             7,   304,    10,  2225,  5456,     4, 50118,   100,   437,   584,\n",
      "            24,    18,  3013,     7,   304,    10,  2225,  5456,     4, 50118,\n",
      "           100,   437,   584,    24,    18,  3013,     7,   304,    10,  2225,\n",
      "          5456,     4, 50118,   100,   437,   584,    24,    18,  3013,     7,\n",
      "           304,    10,  2225,  5456,     4, 50118,   100,   437,   584,    24,\n",
      "            18,  3013,     7,   304,    10,  2225,  5456,     4, 50118,   100,\n",
      "           437,   584,    24,    18,  3013,     7,   304,    10,  2225,  5456,\n",
      "             4, 50118,   100,   437,   584,    24,    18,  3013,     7,   304,\n",
      "            10,  2225,  5456,     4, 50118,   100,   437,   584,    24,    18,\n",
      "          3013,     7,   304,    10,  2225,  5456,     4, 50118,   100,   437,\n",
      "           584,    24,    18,  3013,     7,   304,    10,  2225,  5456,     4,\n",
      "         50118,   100,   437,   584,    24,    18,  3013,     7,   304,    10,\n",
      "          2225,  5456,     4, 50118,   100,   437,   584,    24,    18,  3013,\n",
      "             7,   304,    10,  2225,  5456]], device='cuda:0')\n",
      "text_energy_per_token: [3.8262610950684013, 3.3657090079392322]\n",
      "output_tokens: 225\n",
      "flop: 56297606505\n",
      "energy_consumed:  757.2845267863272\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        17.10%     338.809ms        22.13%     438.451ms      30.448us     208.585ms        55.51%     208.585ms      14.485us         14400     37031.510  \n",
      "                                               aten::mm         0.27%       5.383ms         0.36%       7.145ms      35.725us      77.600ms        20.65%      77.600ms     388.000us           200     16833.479  \n",
      "                                              aten::bmm         5.46%     108.195ms         7.12%     140.984ms      29.372us      12.382ms         3.30%      12.382ms       2.579us          4800       886.284  \n",
      "                                              aten::add         5.89%     116.623ms         8.55%     169.398ms      21.169us      12.181ms         3.24%      12.181ms       1.522us          8002         7.648  \n",
      "                                              aten::mul         2.37%      46.964ms         3.32%      65.687ms      21.874us       3.858ms         1.03%       3.858ms       1.285us          3003         2.033  \n",
      "                                            aten::empty         4.91%      97.342ms         4.91%      97.342ms       5.162us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.32%       6.407ms         5.25%     104.113ms      24.718us       0.000us         0.00%       1.709ms       0.406us          4212            --  \n",
      "                                         aten::_to_copy         0.78%      15.412ms         4.93%      97.706ms      34.833us       0.000us         0.00%       1.709ms       0.609us          2805            --  \n",
      "                                    aten::empty_strided         1.08%      21.458ms         1.08%      21.458ms       7.139us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.45%      28.735ms         3.96%      78.500ms      21.425us       2.976ms         0.79%       2.976ms       0.812us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.981s\n",
      "Self CUDA time total: 375.751ms\n",
      "\n",
      "output: tensor([[    2,  6179,    64,    47,  3094,   114,    10,   621,    16, 14528,\n",
      "          2509,    11,    10,  1607,    50,  1622,   145, 24908,   116, 50118,\n",
      "           100,   206,    24,    18,    55,     9,    10,    22,   100,   437,\n",
      "            45,  2509,    11,    10,  1607,   113,   631,     4,  1437,    38,\n",
      "           437,    45,   686,   114,    24,    18,    95,   162,    50,   114,\n",
      "            24,    18,    95,     5,   169,    38,   437,   341,     7,    24,\n",
      "             4,  1437,    38,   437,    45,   686,   114,    24,    18,    95,\n",
      "           162,    50,   114,    24,    18,    95,     5,   169,    38,   437,\n",
      "           341,     7,    24,     4,  1437,    38,   437,    45,   686,   114,\n",
      "            24,    18,    95,   162,    50,   114,    24,    18,    95,     5,\n",
      "           169,    38,   437,   341,     7,    24,     4,  1437,    38,   437,\n",
      "            45,   686,   114,    24,    18,    95,   162,    50,   114,    24,\n",
      "            18,    95,     5,   169,    38,   437,   341,     7,    24,     4,\n",
      "          1437,    38,   437,    45,   686,   114,    24,    18,    95,   162,\n",
      "            50,   114,    24,    18,    95,     5,   169,    38,   437,   341,\n",
      "             7,    24,     4,  1437,    38,   437,    45,   686,   114,    24,\n",
      "            18,    95,   162,    50,   114,    24,    18,    95,     5,   169,\n",
      "            38,   437,   341,     7,    24,     4,  1437,    38,   437,    45,\n",
      "           686,   114,    24,    18,    95,   162,    50,   114,    24,    18,\n",
      "            95,     5,   169,    38,   437,   341,     7,    24,     4, 50118,\n",
      "           100,   206,    24,    18,    55,     9,    10,    22,   100,   437,\n",
      "            45,  2509,    11,    10,  1607,   113,   631,     4,  1437]],\n",
      "       device='cuda:0')\n",
      "text_energy_per_token: [3.1671517511305742]\n",
      "output_tokens: 219\n",
      "flop: 54760954491\n",
      "energy_consumed:  693.6062334975958\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        17.14%     342.035ms        22.18%     442.545ms      30.732us     208.635ms        55.52%     208.635ms      14.489us         14400     37031.510  \n",
      "                                               aten::mm         0.27%       5.469ms         0.36%       7.216ms      36.081us      77.599ms        20.65%      77.599ms     387.994us           200     16833.479  \n",
      "                                              aten::bmm         5.44%     108.613ms         7.10%     141.693ms      29.519us      12.371ms         3.29%      12.371ms       2.577us          4800       886.284  \n",
      "                                              aten::add         5.89%     117.566ms         8.57%     170.978ms      21.367us      12.182ms         3.24%      12.182ms       1.522us          8002         7.648  \n",
      "                                              aten::mul         2.36%      47.094ms         3.31%      65.943ms      21.959us       3.855ms         1.03%       3.855ms       1.284us          3003         2.033  \n",
      "                                            aten::empty         4.92%      98.225ms         4.92%      98.225ms       5.209us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.32%       6.388ms         5.22%     104.133ms      24.723us       0.000us         0.00%       1.711ms       0.406us          4212            --  \n",
      "                                         aten::_to_copy         0.78%      15.547ms         4.90%      97.745ms      34.847us       0.000us         0.00%       1.711ms       0.610us          2805            --  \n",
      "                                    aten::empty_strided         1.08%      21.607ms         1.08%      21.607ms       7.188us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.45%      28.936ms         3.93%      78.378ms      21.391us       2.976ms         0.79%       2.976ms       0.812us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.995s\n",
      "Self CUDA time total: 375.766ms\n",
      "\n",
      "output: tensor([[    2,  6179,    64,    47,  3094,   114,    10,   621,    16, 14528,\n",
      "          2509,    11,    10,  1607,    50,  1622,   145, 24908,   116, 50118,\n",
      "           100,   206,    24,    18,    55,     9,    10,    22,   100,   437,\n",
      "            45,  2509,    11,    10,  1607,   113,   631,     4,  1437,    38,\n",
      "           437,    45,   686,   114,    24,    18,    95,   162,    50,   114,\n",
      "            24,    18,    95,     5,   169,    38,   437,   341,     7,    24,\n",
      "             4,  1437,    38,   437,    45,   686,   114,    24,    18,    95,\n",
      "           162,    50,   114,    24,    18,    95,     5,   169,    38,   437,\n",
      "           341,     7,    24,     4,  1437,    38,   437,    45,   686,   114,\n",
      "            24,    18,    95,   162,    50,   114,    24,    18,    95,     5,\n",
      "           169,    38,   437,   341,     7,    24,     4,  1437,    38,   437,\n",
      "            45,   686,   114,    24,    18,    95,   162,    50,   114,    24,\n",
      "            18,    95,     5,   169,    38,   437,   341,     7,    24,     4,\n",
      "          1437,    38,   437,    45,   686,   114,    24,    18,    95,   162,\n",
      "            50,   114,    24,    18,    95,     5,   169,    38,   437,   341,\n",
      "             7,    24,     4,  1437,    38,   437,    45,   686,   114,    24,\n",
      "            18,    95,   162,    50,   114,    24,    18,    95,     5,   169,\n",
      "            38,   437,   341,     7,    24,     4,  1437,    38,   437,    45,\n",
      "           686,   114,    24,    18,    95,   162,    50,   114,    24,    18,\n",
      "            95,     5,   169,    38,   437,   341,     7,    24,     4, 50118,\n",
      "           100,   206,    24,    18,    55,     9,    10,    22,   100,   437,\n",
      "            45,  2509,    11,    10,  1607,   113,   631,     4,  1437]],\n",
      "       device='cuda:0')\n",
      "text_energy_per_token: [3.1671517511305742, 3.2456229633043883]\n",
      "output_tokens: 219\n",
      "flop: 54760954491\n",
      "energy_consumed:  710.791428963661\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        17.12%     339.211ms        22.17%     439.293ms      30.506us     209.123ms        55.46%     209.123ms      14.522us         14400     38730.203  \n",
      "                                               aten::mm         0.26%       5.084ms         0.36%       7.079ms      35.397us      77.593ms        20.58%      77.593ms     387.964us           200     17605.657  \n",
      "                                              aten::bmm         5.43%     107.644ms         7.08%     140.212ms      29.211us      12.607ms         3.34%      12.607ms       2.626us          4800       977.338  \n",
      "                                              aten::add         5.90%     116.854ms         8.55%     169.393ms      21.169us      12.190ms         3.23%      12.190ms       1.523us          8002         8.196  \n",
      "                                              aten::mul         2.37%      46.891ms         3.32%      65.738ms      21.891us       3.857ms         1.02%       3.857ms       1.284us          3003         2.127  \n",
      "                                            aten::empty         4.93%      97.630ms         4.93%      97.630ms       5.177us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.33%       6.478ms         5.24%     103.778ms      24.639us       0.000us         0.00%       1.721ms       0.409us          4212            --  \n",
      "                                         aten::_to_copy         0.78%      15.435ms         4.91%      97.300ms      34.688us       0.000us         0.00%       1.721ms       0.614us          2805            --  \n",
      "                                    aten::empty_strided         1.09%      21.524ms         1.09%      21.524ms       7.160us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.46%      28.839ms         3.95%      78.323ms      21.376us       2.988ms         0.79%       2.988ms       0.816us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.981s\n",
      "Self CUDA time total: 377.046ms\n",
      "\n",
      "output: tensor([[    2,  7608,   429,   951,  6573,     7,  2792,    23,    10,   650,\n",
      "             6,  8094,    12,  4447,   265,  1386,     9,    10,   739,  3206,\n",
      "          1400,     6,   190,   114,     5,   850,    32,   723,   116, 50118,\n",
      "         10105,    51,   214,    45,   164,     7,    28,   441,     7,  4960,\n",
      "             7,   907,     5,   276,  2682,    23,    10,   400,  1400,     4,\n",
      "         50118,   100,   437,    45,   584,    14,    51,   214,    45,   164,\n",
      "             7,    28,   441,     7,  4960,     7,   907,     5,   276,  2682,\n",
      "            23,    10,   400,  1400,     6,    53,    51,   214,    45,   164,\n",
      "             7,    28,   441,     7,  4960,     7,   907,     5,   276,  2682,\n",
      "            23,    10,   400,  1400,     4,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437]],\n",
      "       device='cuda:0')\n",
      "text_energy_per_token: [3.0937083484404995]\n",
      "output_tokens: 229\n",
      "flop: 57323521501\n",
      "energy_consumed:  708.4592117928744\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        17.09%     338.289ms        22.16%     438.519ms      30.453us     209.192ms        55.47%     209.192ms      14.527us         14400     38730.203  \n",
      "                                               aten::mm         0.26%       5.078ms         0.36%       7.061ms      35.304us      77.610ms        20.58%      77.610ms     388.048us           200     17605.657  \n",
      "                                              aten::bmm         5.43%     107.517ms         7.08%     140.145ms      29.197us      12.594ms         3.34%      12.594ms       2.624us          4800       977.338  \n",
      "                                              aten::add         5.90%     116.715ms         8.54%     169.074ms      21.129us      12.198ms         3.23%      12.198ms       1.524us          8002         8.196  \n",
      "                                              aten::mul         2.37%      46.858ms         3.32%      65.732ms      21.889us       3.856ms         1.02%       3.856ms       1.284us          3003         2.127  \n",
      "                                            aten::empty         4.94%      97.701ms         4.94%      97.701ms       5.181us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.33%       6.495ms         5.24%     103.726ms      24.626us       0.000us         0.00%       1.721ms       0.408us          4212            --  \n",
      "                                         aten::_to_copy         0.78%      15.428ms         4.91%      97.231ms      34.664us       0.000us         0.00%       1.721ms       0.613us          2805            --  \n",
      "                                    aten::empty_strided         1.08%      21.462ms         1.08%      21.462ms       7.140us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.45%      28.739ms         3.95%      78.207ms      21.345us       2.989ms         0.79%       2.989ms       0.816us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.979s\n",
      "Self CUDA time total: 377.122ms\n",
      "\n",
      "output: tensor([[    2,  7608,   429,   951,  6573,     7,  2792,    23,    10,   650,\n",
      "             6,  8094,    12,  4447,   265,  1386,     9,    10,   739,  3206,\n",
      "          1400,     6,   190,   114,     5,   850,    32,   723,   116, 50118,\n",
      "         10105,    51,   214,    45,   164,     7,    28,   441,     7,  4960,\n",
      "             7,   907,     5,   276,  2682,    23,    10,   400,  1400,     4,\n",
      "         50118,   100,   437,    45,   584,    14,    51,   214,    45,   164,\n",
      "             7,    28,   441,     7,  4960,     7,   907,     5,   276,  2682,\n",
      "            23,    10,   400,  1400,     6,    53,    51,   214,    45,   164,\n",
      "             7,    28,   441,     7,  4960,     7,   907,     5,   276,  2682,\n",
      "            23,    10,   400,  1400,     4,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
      "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437]],\n",
      "       device='cuda:0')\n",
      "text_energy_per_token: [3.0937083484404995, 3.374861836609153]\n",
      "output_tokens: 229\n",
      "flop: 57323521501\n",
      "energy_consumed:  772.8433605834961\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        17.10%     340.539ms        22.12%     440.579ms      30.596us     209.776ms        55.39%     209.776ms      14.568us         14400     39579.550  \n",
      "                                               aten::mm         0.27%       5.408ms         0.36%       7.143ms      35.717us      78.221ms        20.66%      78.221ms     391.103us           200     17991.746  \n",
      "                                              aten::bmm         5.46%     108.643ms         7.09%     141.127ms      29.401us      12.701ms         3.35%      12.701ms       2.646us          4800      1025.630  \n",
      "                                              aten::add         5.93%     118.146ms         8.58%     170.936ms      21.362us      12.211ms         3.22%      12.211ms       1.526us          8002         8.481  \n",
      "                                              aten::mul         2.37%      47.101ms         3.31%      66.010ms      21.981us       3.856ms         1.02%       3.856ms       1.284us          3003         2.174  \n",
      "                                            aten::empty         4.91%      97.801ms         4.91%      97.801ms       5.186us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.33%       6.503ms         5.24%     104.383ms      24.782us       0.000us         0.00%       1.719ms       0.408us          4212            --  \n",
      "                                         aten::_to_copy         0.78%      15.539ms         4.92%      97.880ms      34.895us       0.000us         0.00%       1.719ms       0.613us          2805            --  \n",
      "                                    aten::empty_strided         1.09%      21.670ms         1.09%      21.670ms       7.209us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.46%      29.014ms         3.95%      78.688ms      21.476us       2.985ms         0.79%       2.985ms       0.815us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.991s\n",
      "Self CUDA time total: 378.700ms\n",
      "\n",
      "output: tensor([[    2,  6179,    64,    47,  7118,     5, 10796,     9,    10,  1300,\n",
      "             9,   335,     6,   215,    25,    10,   340,  1566,    50,  5059,\n",
      "           618,     6,   396, 13304,  9382,    15,     5,  5070,     9,     5,\n",
      "          2730,    50, 10710,   116, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4]], device='cuda:0')\n",
      "text_energy_per_token: [3.252067513461806]\n",
      "output_tokens: 234\n",
      "flop: 58607580606\n",
      "energy_consumed:  760.9837981500625\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        17.10%     340.779ms        22.12%     440.776ms      30.609us     209.774ms        55.40%     209.774ms      14.568us         14400     39579.550  \n",
      "                                               aten::mm         0.27%       5.379ms         0.36%       7.144ms      35.720us      78.210ms        20.65%      78.210ms     391.050us           200     17991.746  \n",
      "                                              aten::bmm         5.46%     108.735ms         7.09%     141.356ms      29.449us      12.704ms         3.35%      12.704ms       2.647us          4800      1025.630  \n",
      "                                              aten::add         5.92%     117.977ms         8.56%     170.574ms      21.316us      12.207ms         3.22%      12.207ms       1.525us          8002         8.481  \n",
      "                                              aten::mul         2.37%      47.281ms         3.33%      66.339ms      22.091us       3.853ms         1.02%       3.853ms       1.283us          3003         2.174  \n",
      "                                            aten::empty         4.92%      97.991ms         4.92%      97.991ms       5.196us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.33%       6.508ms         5.23%     104.258ms      24.753us       0.000us         0.00%       1.721ms       0.409us          4212            --  \n",
      "                                         aten::_to_copy         0.78%      15.571ms         4.91%      97.750ms      34.848us       0.000us         0.00%       1.721ms       0.614us          2805            --  \n",
      "                                    aten::empty_strided         1.09%      21.623ms         1.09%      21.623ms       7.193us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.46%      29.006ms         3.94%      78.471ms      21.417us       2.994ms         0.79%       2.994ms       0.817us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.993s\n",
      "Self CUDA time total: 378.678ms\n",
      "\n",
      "output: tensor([[    2,  6179,    64,    47,  7118,     5, 10796,     9,    10,  1300,\n",
      "             9,   335,     6,   215,    25,    10,   340,  1566,    50,  5059,\n",
      "           618,     6,   396, 13304,  9382,    15,     5,  5070,     9,     5,\n",
      "          2730,    50, 10710,   116, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4, 50118, 50118,   133,  2730,    50, 10710,\n",
      "             9,     5,  1566,    50,  5059,   618,    16,    45,     5,  1300,\n",
      "             9,     5,   335,     4]], device='cuda:0')\n",
      "text_energy_per_token: [3.252067513461806, 3.364762766957079]\n",
      "output_tokens: 234\n",
      "flop: 58607580606\n",
      "energy_consumed:  787.3544874679565\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        17.07%     340.044ms        22.22%     442.586ms      30.735us     209.305ms        55.47%     209.305ms      14.535us         14400     39069.942  \n",
      "                                               aten::mm         0.26%       5.142ms         0.36%       7.160ms      35.801us      77.605ms        20.57%      77.605ms     388.023us           200     17760.092  \n",
      "                                              aten::bmm         5.43%     108.191ms         7.08%     141.146ms      29.405us      12.623ms         3.35%      12.623ms       2.630us          4800       996.434  \n",
      "                                              aten::add         5.88%     117.238ms         8.54%     170.180ms      21.267us      12.202ms         3.23%      12.202ms       1.525us          8002         8.309  \n",
      "                                              aten::mul         2.36%      47.054ms         3.31%      65.950ms      21.961us       3.855ms         1.02%       3.855ms       1.284us          3003         2.146  \n",
      "                                            aten::empty         4.92%      98.079ms         4.92%      98.079ms       5.201us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.32%       6.464ms         5.24%     104.297ms      24.762us       0.000us         0.00%       1.719ms       0.408us          4212            --  \n",
      "                                         aten::_to_copy         0.78%      15.523ms         4.91%      97.832ms      34.878us       0.000us         0.00%       1.719ms       0.613us          2805            --  \n",
      "                                    aten::empty_strided         1.09%      21.674ms         1.09%      21.674ms       7.210us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.46%      29.031ms         3.95%      78.788ms      21.503us       2.987ms         0.79%       2.987ms       0.815us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.992s\n",
      "Self CUDA time total: 377.345ms\n",
      "\n",
      "output: tensor([[    2,  7608,   109,   103,    82,  2254,     5, 15583,     9,   145,\n",
      "          8265,     6,   215,    25,    30,  2494,  8444,  4133,    50,   164,\n",
      "            15, 15950,  1029, 22125,     6,   150,   643,  1877,   209,  3734,\n",
      "           116, 50118,   100,   218,    75,   216,     6,    38,   348,   393,\n",
      "            57,  8265,     9,   932,     4,    38,   348,   393,    57,  8265,\n",
      "             9,   932,     4,    38,   348,   393,    57,  8265,     9,   932,\n",
      "             4,    38,   348,   393,    57,  8265,     9,   932,     4,    38,\n",
      "           348,   393,    57,  8265,     9,   932,     4,    38,   348,   393,\n",
      "            57,  8265,     9,   932,     4,    38,   348,   393,    57,  8265,\n",
      "             9,   932,     4,    38,   348,   393,    57,  8265,     9,   932,\n",
      "             4,    38,   348,   393,    57,  8265,     9,   932,     4,    38,\n",
      "           348,   393,    57,  8265,     9,   932,     4,    38,   348,   393,\n",
      "            57,  8265,     9,   932,     4,    38,   348,   393,    57,  8265,\n",
      "             9,   932,     4,    38,   348,   393,    57,  8265,     9,   932,\n",
      "             4,    38,   348,   393,    57,  8265,     9,   932,     4,    38,\n",
      "           348,   393,    57,  8265,     9,   932,     4,    38,   348,   393,\n",
      "            57,  8265,     9,   932,     4,    38,   348,   393,    57,  8265,\n",
      "             9,   932,     4,    38,   348,   393,    57,  8265,     9,   932,\n",
      "             4,    38,   348,   393,    57,  8265,     9,   932,     4,    38,\n",
      "           348,   393,    57,  8265,     9,   932,     4,    38,   348,   393,\n",
      "            57,  8265,     9,   932,     4,    38,   348,   393,    57,  8265,\n",
      "             9,   932,     4,    38,   348,   393,    57,  8265,     9,   932,\n",
      "             4,    38,   348,   393,    57,  8265,     9,   932,     4,    38,\n",
      "           348]], device='cuda:0')\n",
      "text_energy_per_token: [2.9349876963355324]\n",
      "output_tokens: 231\n",
      "flop: 57836923095\n",
      "energy_consumed:  677.9821578535079\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        17.11%     339.094ms        22.15%     439.024ms      30.488us     209.286ms        55.46%     209.286ms      14.534us         14400     39069.942  \n",
      "                                               aten::mm         0.26%       5.174ms         0.36%       7.170ms      35.850us      77.607ms        20.57%      77.607ms     388.035us           200     17760.092  \n",
      "                                              aten::bmm         5.43%     107.665ms         7.09%     140.450ms      29.260us      12.622ms         3.34%      12.622ms       2.630us          4800       996.434  \n",
      "                                              aten::add         5.88%     116.578ms         8.53%     169.160ms      21.140us      12.201ms         3.23%      12.201ms       1.525us          8002         8.309  \n",
      "                                              aten::mul         2.38%      47.105ms         3.33%      65.912ms      21.949us       3.852ms         1.02%       3.852ms       1.283us          3003         2.146  \n",
      "                                            aten::empty         4.93%      97.614ms         4.93%      97.614ms       5.176us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.32%       6.360ms         5.24%     103.866ms      24.660us       0.000us         0.00%       1.719ms       0.408us          4212            --  \n",
      "                                         aten::_to_copy         0.78%      15.532ms         4.92%      97.506ms      34.762us       0.000us         0.00%       1.719ms       0.613us          2805            --  \n",
      "                                    aten::empty_strided         1.08%      21.399ms         1.08%      21.399ms       7.119us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.45%      28.714ms         3.95%      78.271ms      21.362us       2.992ms         0.79%       2.992ms       0.817us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.982s\n",
      "Self CUDA time total: 377.367ms\n",
      "\n",
      "output: tensor([[    2,  7608,   109,   103,    82,  2254,     5, 15583,     9,   145,\n",
      "          8265,     6,   215,    25,    30,  2494,  8444,  4133,    50,   164,\n",
      "            15, 15950,  1029, 22125,     6,   150,   643,  1877,   209,  3734,\n",
      "           116, 50118,   100,   218,    75,   216,     6,    38,   348,   393,\n",
      "            57,  8265,     9,   932,     4,    38,   348,   393,    57,  8265,\n",
      "             9,   932,     4,    38,   348,   393,    57,  8265,     9,   932,\n",
      "             4,    38,   348,   393,    57,  8265,     9,   932,     4,    38,\n",
      "           348,   393,    57,  8265,     9,   932,     4,    38,   348,   393,\n",
      "            57,  8265,     9,   932,     4,    38,   348,   393,    57,  8265,\n",
      "             9,   932,     4,    38,   348,   393,    57,  8265,     9,   932,\n",
      "             4,    38,   348,   393,    57,  8265,     9,   932,     4,    38,\n",
      "           348,   393,    57,  8265,     9,   932,     4,    38,   348,   393,\n",
      "            57,  8265,     9,   932,     4,    38,   348,   393,    57,  8265,\n",
      "             9,   932,     4,    38,   348,   393,    57,  8265,     9,   932,\n",
      "             4,    38,   348,   393,    57,  8265,     9,   932,     4,    38,\n",
      "           348,   393,    57,  8265,     9,   932,     4,    38,   348,   393,\n",
      "            57,  8265,     9,   932,     4,    38,   348,   393,    57,  8265,\n",
      "             9,   932,     4,    38,   348,   393,    57,  8265,     9,   932,\n",
      "             4,    38,   348,   393,    57,  8265,     9,   932,     4,    38,\n",
      "           348,   393,    57,  8265,     9,   932,     4,    38,   348,   393,\n",
      "            57,  8265,     9,   932,     4,    38,   348,   393,    57,  8265,\n",
      "             9,   932,     4,    38,   348,   393,    57,  8265,     9,   932,\n",
      "             4,    38,   348,   393,    57,  8265,     9,   932,     4,    38,\n",
      "           348]], device='cuda:0')\n",
      "text_energy_per_token: [2.9349876963355324, 3.2956073156992596]\n",
      "output_tokens: 231\n",
      "flop: 57836923095\n",
      "energy_consumed:  761.285289926529\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        17.10%     340.491ms        22.14%     440.858ms      30.615us     208.717ms        55.50%     208.717ms      14.494us         14400     37371.249  \n",
      "                                               aten::mm         0.27%       5.408ms         0.36%       7.187ms      35.936us      77.601ms        20.64%      77.601ms     388.006us           200     16987.914  \n",
      "                                              aten::bmm         5.44%     108.406ms         7.10%     141.447ms      29.468us      12.423ms         3.30%      12.423ms       2.588us          4800       903.905  \n",
      "                                              aten::add         5.90%     117.452ms         8.56%     170.537ms      21.312us      12.195ms         3.24%      12.195ms       1.524us          8002         7.756  \n",
      "                                              aten::mul         2.38%      47.318ms         3.32%      66.167ms      22.034us       3.855ms         1.03%       3.855ms       1.284us          3003         2.052  \n",
      "                                            aten::empty         4.92%      97.960ms         4.92%      97.960ms       5.195us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.32%       6.448ms         5.25%     104.558ms      24.824us       0.000us         0.00%       1.714ms       0.407us          4212            --  \n",
      "                                         aten::_to_copy         0.78%      15.479ms         4.93%      98.110ms      34.977us       0.000us         0.00%       1.714ms       0.611us          2805            --  \n",
      "                                    aten::empty_strided         1.09%      21.671ms         1.09%      21.671ms       7.209us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.45%      28.942ms         3.96%      78.799ms      21.506us       2.977ms         0.79%       2.977ms       0.813us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.991s\n",
      "Self CUDA time total: 376.062ms\n",
      "\n",
      "output: tensor([[    2,  6179,    64, 21981,     5,  3650,     9,    97,    82,    11,\n",
      "            10,   592,  1068,   694, 14885,    59,  4106, 14513,     8,  2113,\n",
      "           116, 50118, 50118,  1121,    42,  2225,     6,    52,  4830,     5,\n",
      "           774,     9,  4106, 14513,     8,  2113,    11,     5,  3650,     9,\n",
      "            97,    82,    11,    10,   592,  1068,     4,   166,   465,    14,\n",
      "          4106, 14513,     8,  2113,    32,    45,   129,   505,    13,     5,\n",
      "          3650,     9,    97,    82,     6,    53,    67,    13,     5,  3650,\n",
      "             9,    97,    82,    11,    10,   592,  1068,     4,   166,    67,\n",
      "           465,    14,  4106, 14513,     8,  2113,    32,    45,   129,   505,\n",
      "            13,     5,  3650,     9,    97,    82,     6,    53,    67,    13,\n",
      "             5,  3650,     9,    97,    82,    11,    10,   592,  1068,     4,\n",
      "         50118, 50118,   170,   465,    14,  4106, 14513,     8,  2113,    32,\n",
      "            45,   129,   505,    13,     5,  3650,     9,    97,    82,     6,\n",
      "            53,    67,    13,     5,  3650,     9,    97,    82,    11,    10,\n",
      "           592,  1068,     4,   166,    67,   465,    14,  4106, 14513,     8,\n",
      "          2113,    32,    45,   129,   505,    13,     5,  3650,     9,    97,\n",
      "            82,     6,    53,    67,    13,     5,  3650,     9,    97,    82,\n",
      "            11,    10,   592,  1068,     4, 50118, 50118,   170,   465,    14,\n",
      "          4106, 14513,     8,  2113,    32,    45,   129,   505,    13,     5,\n",
      "          3650,     9,    97,    82,     6,    53,    67,    13,     5,  3650,\n",
      "             9,    97,    82,    11,    10,   592,  1068,     4,   166,    67,\n",
      "           465,    14,  4106, 14513,     8,  2113,    32,    45,   129,   505,\n",
      "            13]], device='cuda:0')\n",
      "text_energy_per_token: [3.5640903686066023]\n",
      "output_tokens: 221\n",
      "flop: 55272875765\n",
      "energy_consumed:  787.6639714620591\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        17.12%     339.459ms        22.15%     439.153ms      30.497us     208.705ms        55.50%     208.705ms      14.493us         14400     37371.249  \n",
      "                                               aten::mm         0.27%       5.371ms         0.36%       7.138ms      35.689us      77.606ms        20.64%      77.606ms     388.028us           200     16987.914  \n",
      "                                              aten::bmm         5.45%     108.035ms         7.11%     140.913ms      29.357us      12.426ms         3.30%      12.426ms       2.589us          4800       903.905  \n",
      "                                              aten::add         5.89%     116.745ms         8.56%     169.718ms      21.209us      12.188ms         3.24%      12.188ms       1.523us          8002         7.756  \n",
      "                                              aten::mul         2.38%      47.138ms         3.33%      65.928ms      21.954us       3.853ms         1.02%       3.853ms       1.283us          3003         2.052  \n",
      "                                            aten::empty         4.92%      97.615ms         4.92%      97.615ms       5.176us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.32%       6.279ms         5.24%     103.797ms      24.643us       0.000us         0.00%       1.717ms       0.408us          4212            --  \n",
      "                                         aten::_to_copy         0.78%      15.491ms         4.92%      97.518ms      34.766us       0.000us         0.00%       1.717ms       0.612us          2805            --  \n",
      "                                    aten::empty_strided         1.08%      21.483ms         1.08%      21.483ms       7.147us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.45%      28.692ms         3.95%      78.357ms      21.386us       2.983ms         0.79%       2.983ms       0.814us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.983s\n",
      "Self CUDA time total: 376.027ms\n",
      "\n",
      "output: tensor([[    2,  6179,    64, 21981,     5,  3650,     9,    97,    82,    11,\n",
      "            10,   592,  1068,   694, 14885,    59,  4106, 14513,     8,  2113,\n",
      "           116, 50118, 50118,  1121,    42,  2225,     6,    52,  4830,     5,\n",
      "           774,     9,  4106, 14513,     8,  2113,    11,     5,  3650,     9,\n",
      "            97,    82,    11,    10,   592,  1068,     4,   166,   465,    14,\n",
      "          4106, 14513,     8,  2113,    32,    45,   129,   505,    13,     5,\n",
      "          3650,     9,    97,    82,     6,    53,    67,    13,     5,  3650,\n",
      "             9,    97,    82,    11,    10,   592,  1068,     4,   166,    67,\n",
      "           465,    14,  4106, 14513,     8,  2113,    32,    45,   129,   505,\n",
      "            13,     5,  3650,     9,    97,    82,     6,    53,    67,    13,\n",
      "             5,  3650,     9,    97,    82,    11,    10,   592,  1068,     4,\n",
      "         50118, 50118,   170,   465,    14,  4106, 14513,     8,  2113,    32,\n",
      "            45,   129,   505,    13,     5,  3650,     9,    97,    82,     6,\n",
      "            53,    67,    13,     5,  3650,     9,    97,    82,    11,    10,\n",
      "           592,  1068,     4,   166,    67,   465,    14,  4106, 14513,     8,\n",
      "          2113,    32,    45,   129,   505,    13,     5,  3650,     9,    97,\n",
      "            82,     6,    53,    67,    13,     5,  3650,     9,    97,    82,\n",
      "            11,    10,   592,  1068,     4, 50118, 50118,   170,   465,    14,\n",
      "          4106, 14513,     8,  2113,    32,    45,   129,   505,    13,     5,\n",
      "          3650,     9,    97,    82,     6,    53,    67,    13,     5,  3650,\n",
      "             9,    97,    82,    11,    10,   592,  1068,     4,   166,    67,\n",
      "           465,    14,  4106, 14513,     8,  2113,    32,    45,   129,   505,\n",
      "            13]], device='cuda:0')\n",
      "text_energy_per_token: [3.5640903686066023, 3.2050877896494456]\n",
      "output_tokens: 221\n",
      "flop: 55272875765\n",
      "energy_consumed:  708.3244015125275\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        17.14%     338.782ms        22.19%     438.616ms      30.459us     208.771ms        55.51%     208.771ms      14.498us         14400     37541.118  \n",
      "                                               aten::mm         0.27%       5.352ms         0.36%       7.079ms      35.397us      77.594ms        20.63%      77.594ms     387.968us           200     17065.132  \n",
      "                                              aten::bmm         5.45%     107.702ms         7.10%     140.326ms      29.235us      12.450ms         3.31%      12.450ms       2.594us          4800       912.826  \n",
      "                                              aten::add         5.90%     116.630ms         8.57%     169.361ms      21.165us      12.185ms         3.24%      12.185ms       1.523us          8002         7.810  \n",
      "                                              aten::mul         2.38%      46.959ms         3.33%      65.790ms      21.908us       3.851ms         1.02%       3.851ms       1.282us          3003         2.061  \n",
      "                                            aten::empty         4.94%      97.618ms         4.94%      97.618ms       5.176us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.32%       6.346ms         5.24%     103.603ms      24.597us       0.000us         0.00%       1.715ms       0.407us          4212            --  \n",
      "                                         aten::_to_copy         0.78%      15.385ms         4.92%      97.257ms      34.673us       0.000us         0.00%       1.715ms       0.611us          2805            --  \n",
      "                                    aten::empty_strided         1.09%      21.472ms         1.09%      21.472ms       7.143us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.46%      28.837ms         3.96%      78.251ms      21.357us       2.984ms         0.79%       2.984ms       0.814us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.977s\n",
      "Self CUDA time total: 376.128ms\n",
      "\n",
      "output: tensor([[    2,  8275,    52,    33,    10,  7654,  9061,     7,  5393,   980,\n",
      "             6,    50,   197,    52,  1056,    15, 15582,  3875,    18,  1272,\n",
      "            78,   116, 50118, 50118,   100,   437,    45,   686,    99,    47,\n",
      "          1266,    30,    22, 23242,  5137,   980,   845, 50118, 50118,   100,\n",
      "           437,    45,   686,    99,    47,  1266,    30,    22, 23242,  5137,\n",
      "           980,   845, 50118, 50118,   100,   437,    45,   686,    99,    47,\n",
      "          1266,    30,    22, 23242,  5137,   980,   845, 50118, 50118,   100,\n",
      "           437,    45,   686,    99,    47,  1266,    30,    22, 23242,  5137,\n",
      "           980,   845, 50118, 50118,   100,   437,    45,   686,    99,    47,\n",
      "          1266,    30,    22, 23242,  5137,   980,   845, 50118, 50118,   100,\n",
      "           437,    45,   686,    99,    47,  1266,    30,    22, 23242,  5137,\n",
      "           980,   845, 50118, 50118,   100,   437,    45,   686,    99,    47,\n",
      "          1266,    30,    22, 23242,  5137,   980,   845, 50118, 50118,   100,\n",
      "           437,    45,   686,    99,    47,  1266,    30,    22, 23242,  5137,\n",
      "           980,   845, 50118, 50118,   100,   437,    45,   686,    99,    47,\n",
      "          1266,    30,    22, 23242,  5137,   980,   845, 50118, 50118,   100,\n",
      "           437,    45,   686,    99,    47,  1266,    30,    22, 23242,  5137,\n",
      "           980,   845, 50118, 50118,   100,   437,    45,   686,    99,    47,\n",
      "          1266,    30,    22, 23242,  5137,   980,   845, 50118, 50118,   100,\n",
      "           437,    45,   686,    99,    47,  1266,    30,    22, 23242,  5137,\n",
      "           980,   845, 50118, 50118,   100,   437,    45,   686,    99,    47,\n",
      "          1266,    30,    22, 23242,  5137,   980,   845, 50118, 50118,   100,\n",
      "           437,    45]], device='cuda:0')\n",
      "text_energy_per_token: [3.4731643588233636]\n",
      "output_tokens: 222\n",
      "flop: 55528947426\n",
      "energy_consumed:  771.0424876587867\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        17.11%     341.052ms        22.15%     441.384ms      30.652us     208.759ms        55.50%     208.759ms      14.497us         14400     37541.118  \n",
      "                                               aten::mm         0.27%       5.461ms         0.36%       7.255ms      36.277us      77.601ms        20.63%      77.601ms     388.006us           200     17065.132  \n",
      "                                              aten::bmm         5.44%     108.488ms         7.09%     141.341ms      29.446us      12.448ms         3.31%      12.448ms       2.593us          4800       912.826  \n",
      "                                              aten::add         5.90%     117.528ms         8.57%     170.877ms      21.354us      12.171ms         3.24%      12.171ms       1.521us          8002         7.810  \n",
      "                                              aten::mul         2.37%      47.331ms         3.32%      66.220ms      22.051us       3.852ms         1.02%       3.852ms       1.283us          3003         2.061  \n",
      "                                            aten::empty         4.93%      98.170ms         4.93%      98.170ms       5.206us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.32%       6.477ms         5.25%     104.622ms      24.839us       0.000us         0.00%       1.713ms       0.407us          4212            --  \n",
      "                                         aten::_to_copy         0.78%      15.525ms         4.92%      98.145ms      34.989us       0.000us         0.00%       1.713ms       0.611us          2805            --  \n",
      "                                    aten::empty_strided         1.09%      21.729ms         1.09%      21.729ms       7.229us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.46%      29.147ms         3.96%      78.935ms      21.544us       2.980ms         0.79%       2.980ms       0.813us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.993s\n",
      "Self CUDA time total: 376.126ms\n",
      "\n",
      "output: tensor([[    2,  8275,    52,    33,    10,  7654,  9061,     7,  5393,   980,\n",
      "             6,    50,   197,    52,  1056,    15, 15582,  3875,    18,  1272,\n",
      "            78,   116, 50118, 50118,   100,   437,    45,   686,    99,    47,\n",
      "          1266,    30,    22, 23242,  5137,   980,   845, 50118, 50118,   100,\n",
      "           437,    45,   686,    99,    47,  1266,    30,    22, 23242,  5137,\n",
      "           980,   845, 50118, 50118,   100,   437,    45,   686,    99,    47,\n",
      "          1266,    30,    22, 23242,  5137,   980,   845, 50118, 50118,   100,\n",
      "           437,    45,   686,    99,    47,  1266,    30,    22, 23242,  5137,\n",
      "           980,   845, 50118, 50118,   100,   437,    45,   686,    99,    47,\n",
      "          1266,    30,    22, 23242,  5137,   980,   845, 50118, 50118,   100,\n",
      "           437,    45,   686,    99,    47,  1266,    30,    22, 23242,  5137,\n",
      "           980,   845, 50118, 50118,   100,   437,    45,   686,    99,    47,\n",
      "          1266,    30,    22, 23242,  5137,   980,   845, 50118, 50118,   100,\n",
      "           437,    45,   686,    99,    47,  1266,    30,    22, 23242,  5137,\n",
      "           980,   845, 50118, 50118,   100,   437,    45,   686,    99,    47,\n",
      "          1266,    30,    22, 23242,  5137,   980,   845, 50118, 50118,   100,\n",
      "           437,    45,   686,    99,    47,  1266,    30,    22, 23242,  5137,\n",
      "           980,   845, 50118, 50118,   100,   437,    45,   686,    99,    47,\n",
      "          1266,    30,    22, 23242,  5137,   980,   845, 50118, 50118,   100,\n",
      "           437,    45,   686,    99,    47,  1266,    30,    22, 23242,  5137,\n",
      "           980,   845, 50118, 50118,   100,   437,    45,   686,    99,    47,\n",
      "          1266,    30,    22, 23242,  5137,   980,   845, 50118, 50118,   100,\n",
      "           437,    45]], device='cuda:0')\n",
      "text_energy_per_token: [3.4731643588233636, 3.0547642764736946]\n",
      "output_tokens: 222\n",
      "flop: 55528947426\n",
      "energy_consumed:  678.1576693771602\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        17.13%     337.520ms        22.17%     436.661ms      30.324us     208.821ms        55.49%     208.821ms      14.501us         14400     37710.987  \n",
      "                                               aten::mm         0.27%       5.334ms         0.36%       7.092ms      35.458us      77.595ms        20.62%      77.595ms     387.975us           200     17142.350  \n",
      "                                              aten::bmm         5.45%     107.284ms         7.09%     139.744ms      29.113us      12.467ms         3.31%      12.467ms       2.597us          4800       921.821  \n",
      "                                              aten::add         5.88%     115.779ms         8.55%     168.451ms      21.051us      12.202ms         3.24%      12.202ms       1.525us          8002         7.864  \n",
      "                                              aten::mul         2.37%      46.661ms         3.32%      65.330ms      21.755us       3.852ms         1.02%       3.852ms       1.283us          3003         2.071  \n",
      "                                            aten::empty         4.92%      96.983ms         4.92%      96.983ms       5.143us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.32%       6.390ms         5.25%     103.329ms      24.532us       0.000us         0.00%       1.717ms       0.408us          4212            --  \n",
      "                                         aten::_to_copy         0.79%      15.473ms         4.92%      96.940ms      34.560us       0.000us         0.00%       1.717ms       0.612us          2805            --  \n",
      "                                    aten::empty_strided         1.09%      21.470ms         1.09%      21.470ms       7.142us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.45%      28.643ms         3.95%      77.775ms      21.227us       2.986ms         0.79%       2.986ms       0.815us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.970s\n",
      "Self CUDA time total: 376.298ms\n",
      "\n",
      "output: tensor([[    2,  1121,    10,   232,   147, 11767,    16,  1959,  3150, 18689,\n",
      "             6,    16,    24,    55,   505,     7, 21790,   633,  5012,    50,\n",
      "          9874,  2017,   116, 50118, 50118,   133,  1948,    16,  4420,     4,\n",
      "         50118, 50118,   133,   864,    16,    35,    16,    24,    55,   505,\n",
      "             7, 21790,   633,  5012,    50,  9874,  2017,   116, 50118, 50118,\n",
      "           133,  1948,    16,  4420,     4, 50118, 50118,   133,   864,    16,\n",
      "            35,    16,    24,    55,   505,     7, 21790,   633,  5012,    50,\n",
      "          9874,  2017,   116, 50118, 50118,   133,  1948,    16,  4420,     4,\n",
      "         50118, 50118,   133,   864,    16,    35,    16,    24,    55,   505,\n",
      "             7, 21790,   633,  5012,    50,  9874,  2017,   116, 50118, 50118,\n",
      "           133,  1948,    16,  4420,     4, 50118, 50118,   133,   864,    16,\n",
      "            35,    16,    24,    55,   505,     7, 21790,   633,  5012,    50,\n",
      "          9874,  2017,   116, 50118, 50118,   133,  1948,    16,  4420,     4,\n",
      "         50118, 50118,   133,   864,    16,    35,    16,    24,    55,   505,\n",
      "             7, 21790,   633,  5012,    50,  9874,  2017,   116, 50118, 50118,\n",
      "           133,  1948,    16,  4420,     4, 50118, 50118,   133,   864,    16,\n",
      "            35,    16,    24,    55,   505,     7, 21790,   633,  5012,    50,\n",
      "          9874,  2017,   116, 50118, 50118,   133,  1948,    16,  4420,     4,\n",
      "         50118, 50118,   133,   864,    16,    35,    16,    24,    55,   505,\n",
      "             7, 21790,   633,  5012,    50,  9874,  2017,   116, 50118, 50118,\n",
      "           133,  1948,    16,  4420,     4, 50118, 50118,   133,   864,    16,\n",
      "            35,    16,    24,    55,   505,     7, 21790,   633,  5012,    50,\n",
      "          9874,  2017,   116]], device='cuda:0')\n",
      "text_energy_per_token: [3.5399963206770177]\n",
      "output_tokens: 223\n",
      "flop: 55785093103\n",
      "energy_consumed:  789.4191795109749\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        17.10%     340.958ms        22.13%     441.369ms      30.651us     208.868ms        55.50%     208.868ms      14.505us         14400     37710.987  \n",
      "                                               aten::mm         0.27%       5.432ms         0.36%       7.239ms      36.196us      77.604ms        20.62%      77.604ms     388.018us           200     17142.350  \n",
      "                                              aten::bmm         5.46%     108.813ms         7.11%     141.895ms      29.561us      12.472ms         3.31%      12.472ms       2.598us          4800       921.821  \n",
      "                                              aten::add         5.88%     117.179ms         8.55%     170.554ms      21.314us      12.206ms         3.24%      12.206ms       1.525us          8002         7.864  \n",
      "                                              aten::mul         2.37%      47.251ms         3.32%      66.165ms      22.033us       3.855ms         1.02%       3.855ms       1.284us          3003         2.071  \n",
      "                                            aten::empty         4.92%      98.137ms         4.92%      98.137ms       5.204us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.33%       6.506ms         5.26%     104.904ms      24.906us       0.000us         0.00%       1.716ms       0.408us          4212            --  \n",
      "                                         aten::_to_copy         0.79%      15.683ms         4.93%      98.398ms      35.080us       0.000us         0.00%       1.716ms       0.612us          2805            --  \n",
      "                                    aten::empty_strided         1.09%      21.769ms         1.09%      21.769ms       7.242us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.46%      29.031ms         3.96%      79.032ms      21.570us       2.982ms         0.79%       2.982ms       0.814us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.994s\n",
      "Self CUDA time total: 376.335ms\n",
      "\n",
      "output: tensor([[    2,  1121,    10,   232,   147, 11767,    16,  1959,  3150, 18689,\n",
      "             6,    16,    24,    55,   505,     7, 21790,   633,  5012,    50,\n",
      "          9874,  2017,   116, 50118, 50118,   133,  1948,    16,  4420,     4,\n",
      "         50118, 50118,   133,   864,    16,    35,    16,    24,    55,   505,\n",
      "             7, 21790,   633,  5012,    50,  9874,  2017,   116, 50118, 50118,\n",
      "           133,  1948,    16,  4420,     4, 50118, 50118,   133,   864,    16,\n",
      "            35,    16,    24,    55,   505,     7, 21790,   633,  5012,    50,\n",
      "          9874,  2017,   116, 50118, 50118,   133,  1948,    16,  4420,     4,\n",
      "         50118, 50118,   133,   864,    16,    35,    16,    24,    55,   505,\n",
      "             7, 21790,   633,  5012,    50,  9874,  2017,   116, 50118, 50118,\n",
      "           133,  1948,    16,  4420,     4, 50118, 50118,   133,   864,    16,\n",
      "            35,    16,    24,    55,   505,     7, 21790,   633,  5012,    50,\n",
      "          9874,  2017,   116, 50118, 50118,   133,  1948,    16,  4420,     4,\n",
      "         50118, 50118,   133,   864,    16,    35,    16,    24,    55,   505,\n",
      "             7, 21790,   633,  5012,    50,  9874,  2017,   116, 50118, 50118,\n",
      "           133,  1948,    16,  4420,     4, 50118, 50118,   133,   864,    16,\n",
      "            35,    16,    24,    55,   505,     7, 21790,   633,  5012,    50,\n",
      "          9874,  2017,   116, 50118, 50118,   133,  1948,    16,  4420,     4,\n",
      "         50118, 50118,   133,   864,    16,    35,    16,    24,    55,   505,\n",
      "             7, 21790,   633,  5012,    50,  9874,  2017,   116, 50118, 50118,\n",
      "           133,  1948,    16,  4420,     4, 50118, 50118,   133,   864,    16,\n",
      "            35,    16,    24,    55,   505,     7, 21790,   633,  5012,    50,\n",
      "          9874,  2017,   116]], device='cuda:0')\n",
      "text_energy_per_token: [3.5399963206770177, 3.3223159240600775]\n",
      "output_tokens: 223\n",
      "flop: 55785093103\n",
      "energy_consumed:  740.8764510653973\n",
      "Processing category: coding\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        17.11%     339.237ms        22.16%     439.304ms      30.507us     209.144ms        55.48%     209.144ms      14.524us         14400     38560.334  \n",
      "                                               aten::mm         0.26%       5.113ms         0.36%       7.104ms      35.520us      77.612ms        20.59%      77.612ms     388.059us           200     17528.439  \n",
      "                                              aten::bmm         5.43%     107.590ms         7.08%     140.438ms      29.258us      12.579ms         3.34%      12.579ms       2.621us          4800       967.901  \n",
      "                                              aten::add         5.89%     116.837ms         8.55%     169.546ms      21.188us      12.195ms         3.23%      12.195ms       1.524us          8002         8.140  \n",
      "                                              aten::mul         2.36%      46.876ms         3.31%      65.716ms      21.884us       3.854ms         1.02%       3.854ms       1.283us          3003         2.118  \n",
      "                                            aten::empty         4.94%      97.913ms         4.94%      97.913ms       5.192us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.33%       6.489ms         5.27%     104.507ms      24.812us       0.000us         0.00%       1.720ms       0.408us          4212            --  \n",
      "                                         aten::_to_copy         0.79%      15.568ms         4.94%      98.018ms      34.944us       0.000us         0.00%       1.720ms       0.613us          2805            --  \n",
      "                                    aten::empty_strided         1.09%      21.584ms         1.09%      21.584ms       7.180us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.46%      28.888ms         3.97%      78.745ms      21.492us       2.989ms         0.79%       2.989ms       0.816us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.983s\n",
      "Self CUDA time total: 377.006ms\n",
      "\n",
      "output: tensor([[    2, 42627,    10,   230, 42964,   586,    14,  7005,    10,  2788,\n",
      "          2870,   516,    30,   516,     8,  3948,     5,   346,     9, 39036,\n",
      "             9,    10,  2167,  2136,    11,     5,  2870,     4, 50118, 50118,\n",
      "         44758,    10,   230, 42964,   586,    14,  7005,    10,  2788,  2870,\n",
      "           516,    30,   516,     8,  3948,     5,   346,     9, 39036,     9,\n",
      "            10,  2167,  2136,    11,     5,  2870,     4, 50118, 50118, 44758,\n",
      "            10,   230, 42964,   586,    14,  7005,    10,  2788,  2870,   516,\n",
      "            30,   516,     8,  3948,     5,   346,     9, 39036,     9,    10,\n",
      "          2167,  2136,    11,     5,  2870,     4, 50118, 50118, 44758,    10,\n",
      "           230, 42964,   586,    14,  7005,    10,  2788,  2870,   516,    30,\n",
      "           516,     8,  3948,     5,   346,     9, 39036,     9,    10,  2167,\n",
      "          2136,    11,     5,  2870,     4, 50118, 50118, 44758,    10,   230,\n",
      "         42964,   586,    14,  7005,    10,  2788,  2870,   516,    30,   516,\n",
      "             8,  3948,     5,   346,     9, 39036,     9,    10,  2167,  2136,\n",
      "            11,     5,  2870,     4, 50118, 50118, 44758,    10,   230, 42964,\n",
      "           586,    14,  7005,    10,  2788,  2870,   516,    30,   516,     8,\n",
      "          3948,     5,   346,     9, 39036,     9,    10,  2167,  2136,    11,\n",
      "             5,  2870,     4, 50118, 50118, 44758,    10,   230, 42964,   586,\n",
      "            14,  7005,    10,  2788,  2870,   516,    30,   516,     8,  3948,\n",
      "             5,   346,     9, 39036,     9,    10,  2167,  2136,    11,     5,\n",
      "          2870,     4, 50118, 50118, 44758,    10,   230, 42964,   586,    14,\n",
      "          7005,    10,  2788,  2870,   516,    30,   516,     8,  3948,     5,\n",
      "           346,     9, 39036,     9,    10,  2167,  2136,    11]],\n",
      "       device='cuda:0')\n",
      "text_energy_per_token: [3.0972573766595435]\n",
      "output_tokens: 228\n",
      "flop: 57066931728\n",
      "energy_consumed:  706.174681878376\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        17.11%     334.842ms        22.18%     434.053ms      30.143us     209.097ms        55.47%     209.097ms      14.521us         14400     38560.334  \n",
      "                                               aten::mm         0.26%       5.060ms         0.36%       7.058ms      35.290us      77.622ms        20.59%      77.622ms     388.111us           200     17528.439  \n",
      "                                              aten::bmm         5.43%     106.232ms         7.09%     138.673ms      28.890us      12.573ms         3.34%      12.573ms       2.619us          4800       967.901  \n",
      "                                              aten::add         5.89%     115.281ms         8.55%     167.239ms      20.900us      12.196ms         3.24%      12.196ms       1.524us          8002         8.140  \n",
      "                                              aten::mul         2.37%      46.456ms         3.32%      64.916ms      21.617us       3.853ms         1.02%       3.853ms       1.283us          3003         2.118  \n",
      "                                            aten::empty         4.91%      96.065ms         4.91%      96.065ms       5.094us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.33%       6.435ms         5.26%     102.885ms      24.427us       0.000us         0.00%       1.719ms       0.408us          4212            --  \n",
      "                                         aten::_to_copy         0.78%      15.312ms         4.93%      96.450ms      34.385us       0.000us         0.00%       1.719ms       0.613us          2805            --  \n",
      "                                    aten::empty_strided         1.08%      21.206ms         1.08%      21.206ms       7.055us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.44%      28.252ms         3.94%      77.161ms      21.059us       2.988ms         0.79%       2.988ms       0.815us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.957s\n",
      "Self CUDA time total: 376.973ms\n",
      "\n",
      "output: tensor([[    2, 42627,    10,   230, 42964,   586,    14,  7005,    10,  2788,\n",
      "          2870,   516,    30,   516,     8,  3948,     5,   346,     9, 39036,\n",
      "             9,    10,  2167,  2136,    11,     5,  2870,     4, 50118, 50118,\n",
      "         44758,    10,   230, 42964,   586,    14,  7005,    10,  2788,  2870,\n",
      "           516,    30,   516,     8,  3948,     5,   346,     9, 39036,     9,\n",
      "            10,  2167,  2136,    11,     5,  2870,     4, 50118, 50118, 44758,\n",
      "            10,   230, 42964,   586,    14,  7005,    10,  2788,  2870,   516,\n",
      "            30,   516,     8,  3948,     5,   346,     9, 39036,     9,    10,\n",
      "          2167,  2136,    11,     5,  2870,     4, 50118, 50118, 44758,    10,\n",
      "           230, 42964,   586,    14,  7005,    10,  2788,  2870,   516,    30,\n",
      "           516,     8,  3948,     5,   346,     9, 39036,     9,    10,  2167,\n",
      "          2136,    11,     5,  2870,     4, 50118, 50118, 44758,    10,   230,\n",
      "         42964,   586,    14,  7005,    10,  2788,  2870,   516,    30,   516,\n",
      "             8,  3948,     5,   346,     9, 39036,     9,    10,  2167,  2136,\n",
      "            11,     5,  2870,     4, 50118, 50118, 44758,    10,   230, 42964,\n",
      "           586,    14,  7005,    10,  2788,  2870,   516,    30,   516,     8,\n",
      "          3948,     5,   346,     9, 39036,     9,    10,  2167,  2136,    11,\n",
      "             5,  2870,     4, 50118, 50118, 44758,    10,   230, 42964,   586,\n",
      "            14,  7005,    10,  2788,  2870,   516,    30,   516,     8,  3948,\n",
      "             5,   346,     9, 39036,     9,    10,  2167,  2136,    11,     5,\n",
      "          2870,     4, 50118, 50118, 44758,    10,   230, 42964,   586,    14,\n",
      "          7005,    10,  2788,  2870,   516,    30,   516,     8,  3948,     5,\n",
      "           346,     9, 39036,     9,    10,  2167,  2136,    11]],\n",
      "       device='cuda:0')\n",
      "text_energy_per_token: [3.0972573766595435, 3.210786598021105]\n",
      "output_tokens: 228\n",
      "flop: 57066931728\n",
      "energy_consumed:  732.059344348812\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        17.11%     338.461ms        22.16%     438.273ms      30.436us     208.736ms        55.51%     208.736ms      14.496us         14400     37371.249  \n",
      "                                               aten::mm         0.27%       5.338ms         0.36%       7.130ms      35.651us      77.608ms        20.64%      77.608ms     388.041us           200     16987.914  \n",
      "                                              aten::bmm         5.44%     107.612ms         7.10%     140.366ms      29.243us      12.421ms         3.30%      12.421ms       2.588us          4800       903.905  \n",
      "                                              aten::add         5.89%     116.536ms         8.57%     169.414ms      21.171us      12.185ms         3.24%      12.185ms       1.523us          8002         7.756  \n",
      "                                              aten::mul         2.36%      46.752ms         3.31%      65.513ms      21.816us       3.852ms         1.02%       3.852ms       1.283us          3003         2.052  \n",
      "                                            aten::empty         4.93%      97.542ms         4.93%      97.542ms       5.172us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.33%       6.454ms         5.23%     103.375ms      24.543us       0.000us         0.00%       1.712ms       0.406us          4212            --  \n",
      "                                         aten::_to_copy         0.78%      15.387ms         4.90%      96.921ms      34.553us       0.000us         0.00%       1.712ms       0.610us          2805            --  \n",
      "                                    aten::empty_strided         1.09%      21.596ms         1.09%      21.596ms       7.184us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.46%      28.789ms         3.94%      77.996ms      21.287us       2.979ms         0.79%       2.979ms       0.813us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.978s\n",
      "Self CUDA time total: 376.045ms\n",
      "\n",
      "output: tensor([[    2, 20470, 40224,    10, 31886,  5043,     7,   465,     5,  6463,\n",
      "          1537, 49734,  4086,     9,    80,  8135, 22052,   634,  6878,  8326,\n",
      "             4, 50118, 50118, 44758,    10, 31886,  5043,     7,   465,     5,\n",
      "          6463,  1537, 49734,  4086,     9,    80,  8135, 22052,   634,  6878,\n",
      "          8326,     4, 50118, 50118, 44758,    10, 31886,  5043,     7,   465,\n",
      "             5,  6463,  1537, 49734,  4086,     9,    80,  8135, 22052,   634,\n",
      "          6878,  8326,     4, 50118, 50118, 44758,    10, 31886,  5043,     7,\n",
      "           465,     5,  6463,  1537, 49734,  4086,     9,    80,  8135, 22052,\n",
      "           634,  6878,  8326,     4, 50118, 50118, 44758,    10, 31886,  5043,\n",
      "             7,   465,     5,  6463,  1537, 49734,  4086,     9,    80,  8135,\n",
      "         22052,   634,  6878,  8326,     4, 50118, 50118, 44758,    10, 31886,\n",
      "          5043,     7,   465,     5,  6463,  1537, 49734,  4086,     9,    80,\n",
      "          8135, 22052,   634,  6878,  8326,     4, 50118, 50118, 44758,    10,\n",
      "         31886,  5043,     7,   465,     5,  6463,  1537, 49734,  4086,     9,\n",
      "            80,  8135, 22052,   634,  6878,  8326,     4, 50118, 50118, 44758,\n",
      "            10, 31886,  5043,     7,   465,     5,  6463,  1537, 49734,  4086,\n",
      "             9,    80,  8135, 22052,   634,  6878,  8326,     4, 50118, 50118,\n",
      "         44758,    10, 31886,  5043,     7,   465,     5,  6463,  1537, 49734,\n",
      "          4086,     9,    80,  8135, 22052,   634,  6878,  8326,     4, 50118,\n",
      "         50118, 44758,    10, 31886,  5043,     7,   465,     5,  6463,  1537,\n",
      "         49734,  4086,     9,    80,  8135, 22052,   634,  6878,  8326,     4,\n",
      "         50118, 50118, 44758,    10, 31886,  5043,     7,   465,     5,  6463,\n",
      "          1537]], device='cuda:0')\n",
      "text_energy_per_token: [3.1860690942272343]\n",
      "output_tokens: 221\n",
      "flop: 55272875765\n",
      "energy_consumed:  704.1212698242188\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        17.15%     340.712ms        22.19%     440.844ms      30.614us     208.697ms        55.50%     208.697ms      14.493us         14400     37371.249  \n",
      "                                               aten::mm         0.27%       5.437ms         0.36%       7.177ms      35.883us      77.598ms        20.64%      77.598ms     387.992us           200     16987.914  \n",
      "                                              aten::bmm         5.44%     108.035ms         7.09%     140.789ms      29.331us      12.425ms         3.30%      12.425ms       2.589us          4800       903.905  \n",
      "                                              aten::add         5.89%     116.927ms         8.56%     170.144ms      21.263us      12.181ms         3.24%      12.181ms       1.522us          8002         7.756  \n",
      "                                              aten::mul         2.37%      47.089ms         3.32%      65.900ms      21.945us       3.854ms         1.02%       3.854ms       1.283us          3003         2.052  \n",
      "                                            aten::empty         4.93%      97.958ms         4.93%      97.958ms       5.195us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.32%       6.406ms         5.23%     103.925ms      24.674us       0.000us         0.00%       1.713ms       0.407us          4212            --  \n",
      "                                         aten::_to_copy         0.78%      15.425ms         4.91%      97.519ms      34.766us       0.000us         0.00%       1.713ms       0.611us          2805            --  \n",
      "                                    aten::empty_strided         1.09%      21.599ms         1.09%      21.599ms       7.185us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.45%      28.896ms         3.95%      78.424ms      21.404us       2.979ms         0.79%       2.979ms       0.813us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.987s\n",
      "Self CUDA time total: 375.998ms\n",
      "\n",
      "output: tensor([[    2, 20470, 40224,    10, 31886,  5043,     7,   465,     5,  6463,\n",
      "          1537, 49734,  4086,     9,    80,  8135, 22052,   634,  6878,  8326,\n",
      "             4, 50118, 50118, 44758,    10, 31886,  5043,     7,   465,     5,\n",
      "          6463,  1537, 49734,  4086,     9,    80,  8135, 22052,   634,  6878,\n",
      "          8326,     4, 50118, 50118, 44758,    10, 31886,  5043,     7,   465,\n",
      "             5,  6463,  1537, 49734,  4086,     9,    80,  8135, 22052,   634,\n",
      "          6878,  8326,     4, 50118, 50118, 44758,    10, 31886,  5043,     7,\n",
      "           465,     5,  6463,  1537, 49734,  4086,     9,    80,  8135, 22052,\n",
      "           634,  6878,  8326,     4, 50118, 50118, 44758,    10, 31886,  5043,\n",
      "             7,   465,     5,  6463,  1537, 49734,  4086,     9,    80,  8135,\n",
      "         22052,   634,  6878,  8326,     4, 50118, 50118, 44758,    10, 31886,\n",
      "          5043,     7,   465,     5,  6463,  1537, 49734,  4086,     9,    80,\n",
      "          8135, 22052,   634,  6878,  8326,     4, 50118, 50118, 44758,    10,\n",
      "         31886,  5043,     7,   465,     5,  6463,  1537, 49734,  4086,     9,\n",
      "            80,  8135, 22052,   634,  6878,  8326,     4, 50118, 50118, 44758,\n",
      "            10, 31886,  5043,     7,   465,     5,  6463,  1537, 49734,  4086,\n",
      "             9,    80,  8135, 22052,   634,  6878,  8326,     4, 50118, 50118,\n",
      "         44758,    10, 31886,  5043,     7,   465,     5,  6463,  1537, 49734,\n",
      "          4086,     9,    80,  8135, 22052,   634,  6878,  8326,     4, 50118,\n",
      "         50118, 44758,    10, 31886,  5043,     7,   465,     5,  6463,  1537,\n",
      "         49734,  4086,     9,    80,  8135, 22052,   634,  6878,  8326,     4,\n",
      "         50118, 50118, 44758,    10, 31886,  5043,     7,   465,     5,  6463,\n",
      "          1537]], device='cuda:0')\n",
      "text_energy_per_token: [3.1860690942272343, 3.532212466278767]\n",
      "output_tokens: 221\n",
      "flop: 55272875765\n",
      "energy_consumed:  780.6189550476075\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        17.12%     340.438ms        22.16%     440.675ms      30.602us     208.391ms        55.56%     208.391ms      14.472us         14400     36182.163  \n",
      "                                               aten::mm         0.26%       5.152ms         0.35%       6.940ms      34.699us      77.584ms        20.69%      77.584ms     387.918us           200     16447.390  \n",
      "                                              aten::bmm         5.45%     108.444ms         7.10%     141.205ms      29.418us      12.153ms         3.24%      12.153ms       2.532us          4800       843.522  \n",
      "                                              aten::add         5.89%     117.226ms         8.55%     170.073ms      21.254us      12.179ms         3.25%      12.179ms       1.522us          8002         7.385  \n",
      "                                              aten::mul         2.37%      47.082ms         3.31%      65.885ms      21.940us       3.851ms         1.03%       3.851ms       1.282us          3003         1.986  \n",
      "                                            aten::empty         4.92%      97.925ms         4.92%      97.925ms       5.193us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.32%       6.448ms         5.25%     104.483ms      24.806us       0.000us         0.00%       1.709ms       0.406us          4212            --  \n",
      "                                         aten::_to_copy         0.79%      15.676ms         4.93%      98.035ms      34.950us       0.000us         0.00%       1.709ms       0.609us          2805            --  \n",
      "                                    aten::empty_strided         1.09%      21.637ms         1.09%      21.637ms       7.198us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.46%      28.983ms         3.96%      78.853ms      21.521us       2.978ms         0.79%       2.978ms       0.813us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.989s\n",
      "Self CUDA time total: 375.069ms\n",
      "\n",
      "output: tensor([[    2, 20470, 40224,    10,  1675,  8151,    11, 31886,     7, 28754,\n",
      "            41,  1047,  1100,     4, 50118, 50118,   713,    16,    10,   182,\n",
      "          1537,   936,    19,  1047,  8480,     4, 50118, 50118,   133,    78,\n",
      "           631,     7,   109,    16,     7,  1045,    10,    92,  1047,  1100,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758]], device='cuda:0')\n",
      "text_energy_per_token: [3.2076304572764958]\n",
      "output_tokens: 214\n",
      "flop: 53482446586\n",
      "energy_consumed:  686.4329178571701\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        17.12%     341.429ms        22.15%     441.725ms      30.675us     208.440ms        55.57%     208.440ms      14.475us         14400     36182.163  \n",
      "                                               aten::mm         0.26%       5.135ms         0.34%       6.854ms      34.272us      77.595ms        20.69%      77.595ms     387.973us           200     16447.390  \n",
      "                                              aten::bmm         5.44%     108.468ms         7.09%     141.293ms      29.436us      12.142ms         3.24%      12.142ms       2.530us          4800       843.522  \n",
      "                                              aten::add         5.90%     117.623ms         8.55%     170.522ms      21.310us      12.181ms         3.25%      12.181ms       1.522us          8002         7.385  \n",
      "                                              aten::mul         2.37%      47.162ms         3.31%      66.029ms      21.988us       3.854ms         1.03%       3.854ms       1.283us          3003         1.986  \n",
      "                                            aten::empty         4.93%      98.219ms         4.93%      98.219ms       5.208us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.32%       6.439ms         5.25%     104.671ms      24.851us       0.000us         0.00%       1.704ms       0.405us          4212            --  \n",
      "                                         aten::_to_copy         0.79%      15.670ms         4.93%      98.232ms      35.020us       0.000us         0.00%       1.704ms       0.608us          2805            --  \n",
      "                                    aten::empty_strided         1.09%      21.744ms         1.09%      21.744ms       7.233us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.46%      29.089ms         3.95%      78.840ms      21.517us       2.972ms         0.79%       2.972ms       0.811us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.994s\n",
      "Self CUDA time total: 375.101ms\n",
      "\n",
      "output: tensor([[    2, 20470, 40224,    10,  1675,  8151,    11, 31886,     7, 28754,\n",
      "            41,  1047,  1100,     4, 50118, 50118,   713,    16,    10,   182,\n",
      "          1537,   936,    19,  1047,  8480,     4, 50118, 50118,   133,    78,\n",
      "           631,     7,   109,    16,     7,  1045,    10,    92,  1047,  1100,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758,    10,    92,  1047,  1100,    11, 31886,\n",
      "             4, 50118, 50118, 44758]], device='cuda:0')\n",
      "text_energy_per_token: [3.2076304572764958, 3.647063103134387]\n",
      "output_tokens: 214\n",
      "flop: 53482446586\n",
      "energy_consumed:  780.4715040707588\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        17.12%     339.905ms        22.15%     439.964ms      30.553us     208.497ms        55.53%     208.497ms      14.479us         14400     36691.771  \n",
      "                                               aten::mm         0.27%       5.452ms         0.36%       7.206ms      36.029us      77.581ms        20.66%      77.581ms     387.906us           200     16679.043  \n",
      "                                              aten::bmm         5.44%     108.023ms         7.09%     140.814ms      29.336us      12.326ms         3.28%      12.326ms       2.568us          4800       868.958  \n",
      "                                              aten::add         5.88%     116.856ms         8.56%     170.081ms      21.255us      12.186ms         3.25%      12.186ms       1.523us          8002         7.542  \n",
      "                                              aten::mul         2.36%      46.769ms         3.30%      65.556ms      21.830us       3.851ms         1.03%       3.851ms       1.282us          3003         2.014  \n",
      "                                            aten::empty         4.93%      97.850ms         4.93%      97.850ms       5.189us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.32%       6.328ms         5.25%     104.314ms      24.766us       0.000us         0.00%       1.714ms       0.407us          4212            --  \n",
      "                                         aten::_to_copy         0.78%      15.571ms         4.93%      97.986ms      34.933us       0.000us         0.00%       1.714ms       0.611us          2805            --  \n",
      "                                    aten::empty_strided         1.09%      21.555ms         1.09%      21.555ms       7.171us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.46%      28.913ms         3.97%      78.839ms      21.517us       2.978ms         0.79%       2.978ms       0.813us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.986s\n",
      "Self CUDA time total: 375.487ms\n",
      "\n",
      "output: tensor([[    2, 45714,    10,   586,     7,   465,     5,   295,   212, 28174,\n",
      "           261, 29522,   346,   634,  6878,  8326,     4, 50118, 50118, 44758,\n",
      "            10,   586,     7,   465,     5,   295,   212, 28174,   261, 29522,\n",
      "           346,   634,  6878,  8326,     4, 50118, 50118, 44758,    10,   586,\n",
      "             7,   465,     5,   295,   212, 28174,   261, 29522,   346,   634,\n",
      "          6878,  8326,     4, 50118, 50118, 44758,    10,   586,     7,   465,\n",
      "             5,   295,   212, 28174,   261, 29522,   346,   634,  6878,  8326,\n",
      "             4, 50118, 50118, 44758,    10,   586,     7,   465,     5,   295,\n",
      "           212, 28174,   261, 29522,   346,   634,  6878,  8326,     4, 50118,\n",
      "         50118, 44758,    10,   586,     7,   465,     5,   295,   212, 28174,\n",
      "           261, 29522,   346,   634,  6878,  8326,     4, 50118, 50118, 44758,\n",
      "            10,   586,     7,   465,     5,   295,   212, 28174,   261, 29522,\n",
      "           346,   634,  6878,  8326,     4, 50118, 50118, 44758,    10,   586,\n",
      "             7,   465,     5,   295,   212, 28174,   261, 29522,   346,   634,\n",
      "          6878,  8326,     4, 50118, 50118, 44758,    10,   586,     7,   465,\n",
      "             5,   295,   212, 28174,   261, 29522,   346,   634,  6878,  8326,\n",
      "             4, 50118, 50118, 44758,    10,   586,     7,   465,     5,   295,\n",
      "           212, 28174,   261, 29522,   346,   634,  6878,  8326,     4, 50118,\n",
      "         50118, 44758,    10,   586,     7,   465,     5,   295,   212, 28174,\n",
      "           261, 29522,   346,   634,  6878,  8326,     4, 50118, 50118, 44758,\n",
      "            10,   586,     7,   465,     5,   295,   212, 28174,   261, 29522,\n",
      "           346,   634,  6878,  8326,     4, 50118, 50118]], device='cuda:0')\n",
      "text_energy_per_token: [3.140726270913419]\n",
      "output_tokens: 217\n",
      "flop: 54249329281\n",
      "energy_consumed:  681.537600788212\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        17.12%     340.777ms        22.17%     441.357ms      30.650us     208.539ms        55.53%     208.539ms      14.482us         14400     36691.771  \n",
      "                                               aten::mm         0.27%       5.370ms         0.36%       7.125ms      35.624us      77.596ms        20.66%      77.596ms     387.982us           200     16679.043  \n",
      "                                              aten::bmm         5.42%     107.920ms         7.07%     140.779ms      29.329us      12.323ms         3.28%      12.323ms       2.567us          4800       868.958  \n",
      "                                              aten::add         5.89%     117.207ms         8.56%     170.344ms      21.288us      12.187ms         3.25%      12.187ms       1.523us          8002         7.542  \n",
      "                                              aten::mul         2.36%      47.065ms         3.31%      65.808ms      21.914us       3.851ms         1.03%       3.851ms       1.283us          3003         2.014  \n",
      "                                            aten::empty         4.94%      98.320ms         4.94%      98.320ms       5.214us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.33%       6.517ms         5.24%     104.295ms      24.761us       0.000us         0.00%       1.715ms       0.407us          4212            --  \n",
      "                                         aten::_to_copy         0.78%      15.473ms         4.91%      97.778ms      34.858us       0.000us         0.00%       1.715ms       0.611us          2805            --  \n",
      "                                    aten::empty_strided         1.09%      21.659ms         1.09%      21.659ms       7.205us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.46%      29.021ms         3.95%      78.574ms      21.445us       2.979ms         0.79%       2.979ms       0.813us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.991s\n",
      "Self CUDA time total: 375.558ms\n",
      "\n",
      "output: tensor([[    2, 45714,    10,   586,     7,   465,     5,   295,   212, 28174,\n",
      "           261, 29522,   346,   634,  6878,  8326,     4, 50118, 50118, 44758,\n",
      "            10,   586,     7,   465,     5,   295,   212, 28174,   261, 29522,\n",
      "           346,   634,  6878,  8326,     4, 50118, 50118, 44758,    10,   586,\n",
      "             7,   465,     5,   295,   212, 28174,   261, 29522,   346,   634,\n",
      "          6878,  8326,     4, 50118, 50118, 44758,    10,   586,     7,   465,\n",
      "             5,   295,   212, 28174,   261, 29522,   346,   634,  6878,  8326,\n",
      "             4, 50118, 50118, 44758,    10,   586,     7,   465,     5,   295,\n",
      "           212, 28174,   261, 29522,   346,   634,  6878,  8326,     4, 50118,\n",
      "         50118, 44758,    10,   586,     7,   465,     5,   295,   212, 28174,\n",
      "           261, 29522,   346,   634,  6878,  8326,     4, 50118, 50118, 44758,\n",
      "            10,   586,     7,   465,     5,   295,   212, 28174,   261, 29522,\n",
      "           346,   634,  6878,  8326,     4, 50118, 50118, 44758,    10,   586,\n",
      "             7,   465,     5,   295,   212, 28174,   261, 29522,   346,   634,\n",
      "          6878,  8326,     4, 50118, 50118, 44758,    10,   586,     7,   465,\n",
      "             5,   295,   212, 28174,   261, 29522,   346,   634,  6878,  8326,\n",
      "             4, 50118, 50118, 44758,    10,   586,     7,   465,     5,   295,\n",
      "           212, 28174,   261, 29522,   346,   634,  6878,  8326,     4, 50118,\n",
      "         50118, 44758,    10,   586,     7,   465,     5,   295,   212, 28174,\n",
      "           261, 29522,   346,   634,  6878,  8326,     4, 50118, 50118, 44758,\n",
      "            10,   586,     7,   465,     5,   295,   212, 28174,   261, 29522,\n",
      "           346,   634,  6878,  8326,     4, 50118, 50118]], device='cuda:0')\n",
      "text_energy_per_token: [3.140726270913419, 3.4887562609887888]\n",
      "output_tokens: 217\n",
      "flop: 54249329281\n",
      "energy_consumed:  757.0601086345672\n",
      "prof keys flops table\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        17.14%     341.028ms        22.17%     441.260ms      30.643us     208.513ms        55.52%     208.513ms      14.480us         14400     36691.771  \n",
      "                                               aten::mm         0.27%       5.353ms         0.36%       7.113ms      35.566us      77.597ms        20.66%      77.597ms     387.984us           200     16679.043  \n",
      "                                              aten::bmm         5.44%     108.294ms         7.10%     141.224ms      29.422us      12.327ms         3.28%      12.327ms       2.568us          4800       868.958  \n",
      "                                              aten::add         5.91%     117.559ms         8.58%     170.683ms      21.330us      12.198ms         3.25%      12.198ms       1.524us          8002         7.542  \n",
      "                                              aten::mul         2.37%      47.120ms         3.32%      66.057ms      21.997us       3.853ms         1.03%       3.853ms       1.283us          3003         2.014  \n",
      "                                            aten::empty         4.92%      97.905ms         4.92%      97.905ms       5.192us       0.000us         0.00%       0.000us       0.000us         18858            --  \n",
      "                                               aten::to         0.32%       6.397ms         5.24%     104.265ms      24.754us       0.000us         0.00%       1.711ms       0.406us          4212            --  \n",
      "                                         aten::_to_copy         0.78%      15.471ms         4.92%      97.868ms      34.891us       0.000us         0.00%       1.711ms       0.610us          2805            --  \n",
      "                                    aten::empty_strided         1.09%      21.651ms         1.09%      21.651ms       7.203us       0.000us         0.00%       0.000us       0.000us          3006            --  \n",
      "                                            aten::copy_         1.45%      28.825ms         3.95%      78.622ms      21.458us       2.975ms         0.79%       2.975ms       0.812us          3664            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.990s\n",
      "Self CUDA time total: 375.540ms\n",
      "\n",
      "output: tensor([[    2, 20470, 40224,    10, 32771,  1707, 17194,     7,   465,    10,\n",
      "          2167,  7510,    11,    10, 24713,  8932,     4, 50118, 50118,   713,\n",
      "            16,    10,   182,  2007,     8,  1365,   169,     7,   465,    10,\n",
      "          1989,  7510,    11,    10, 24713,  8932,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4, 50118, 50118,   133,\n",
      "         17194,    16,    10, 32771,  1707, 17194,     4]], device='cuda:0')\n",
      "text_energy_per_token: [3.3668504452340615]\n",
      "output_tokens: 217\n",
      "flop: 54249329281\n",
      "energy_consumed:  730.6065466157913\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch\n",
    "import pynvml\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.profiler import profile, ProfilerActivity\n",
    "\n",
    "# Specify the GPU device you want to use\n",
    "device = \"cuda:0\"  # Change this to your preferred GPU\n",
    "\n",
    "# Initialize NVML for power measurement\n",
    "def initialize_nvml():\n",
    "    pynvml.nvmlInit()\n",
    "\n",
    "def shutdown_nvml():\n",
    "    pynvml.nvmlShutdown()\n",
    "\n",
    "def get_gpu_handle(gpu_index=0):\n",
    "    return pynvml.nvmlDeviceGetHandleByIndex(gpu_index)\n",
    "\n",
    "# Measure GPU power consumption over a period of time\n",
    "def measure_power_consumption(handle, duration_sec=1.0, interval_sec=0.1):\n",
    "    power_readings = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while (time.time() - start_time) < duration_sec:\n",
    "        power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0  # Convert from mW to W\n",
    "        power_readings.append(power)\n",
    "        time.sleep(interval_sec)\n",
    "    \n",
    "    return sum(power_readings) / len(power_readings) if power_readings else 0\n",
    "\n",
    "# Measure energy consumed during inference and FLOPs\n",
    "def measure_energy_during_inference(handle, inference_function, model, inputs, max_new_tokens=200):\n",
    "    power_start = measure_power_consumption(handle, duration_sec=0.5)\n",
    "    \n",
    "    # Start time for inference\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Measure FLOPs using PyTorch profiler\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], with_flops=True) as prof:\n",
    "        with torch.no_grad():\n",
    "            result = inference_function(inputs['input_ids'], max_new_tokens=max_new_tokens)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    power_end = measure_power_consumption(handle, duration_sec=0.5)\n",
    "    \n",
    "    avg_power = (power_start + power_end) / 2\n",
    "    elapsed_time = end_time - start_time\n",
    "    energy_consumed = avg_power * elapsed_time\n",
    "    print(\"prof keys flops table\")\n",
    "    print(prof.key_averages().table(sort_by=\"flops\", row_limit=10)) \n",
    "    # Calculate FLOPs\n",
    "    flops = sum([event.flops for event in prof.key_averages() if event.flops is not None])\n",
    "\n",
    "    return energy_consumed, elapsed_time, flops, result\n",
    "\n",
    "# Measure energy consumed during inference and FLOPs\n",
    "def NOTWORKING_measure_energy_during_inferenceWRONG_NOTWORKING(handle, inference_function, model, inputs, max_new_tokens=200):\n",
    "    # Measure initial power consumption\n",
    "    power_start = measure_power_consumption(handle, duration_sec=0.2)\n",
    "\n",
    "    # Start time for inference\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Measure FLOPs using PyTorch profiler\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], with_flops=True) as prof:\n",
    "        with torch.no_grad():\n",
    "            result = inference_function(model, inputs['input_ids'], max_new_tokens=max_new_tokens)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    # Measure final power consumption\n",
    "    power_end = measure_power_consumption(handle, duration_sec=0.2)\n",
    "\n",
    "    # Calculate average power and elapsed time\n",
    "    avg_power = (power_start + power_end) / 2\n",
    "    elapsed_time = end_time - start_time\n",
    "    energy_consumed = avg_power * elapsed_time\n",
    "    print(\"prof keys flops table\")\n",
    "    print(prof.key_averages().table(sort_by=\"flops\", row_limit=10))\n",
    "    # Calculate FLOPs\n",
    "    flops = sum(event.flops for event in prof.key_averages() if event.flops is not None)\n",
    "\n",
    "    return energy_consumed, elapsed_time, flops, result\n",
    "\n",
    "\n",
    "\n",
    "# Calculate perplexity for generated text\n",
    "def calculate_perplexity(model, input_text, tokenizer):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)  # Ensure input is on the same device\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "        perplexity = torch.exp(loss)\n",
    "    return perplexity.item()\n",
    "\n",
    "# Run the experiment for a list of texts\n",
    "def run_experiment_for_texts(texts, bootstrapping, handle, model, tokenizer):\n",
    "    latencies = []\n",
    "    energy_per_token = []\n",
    "    energy_per_flops = []\n",
    "    energy_per_task = []\n",
    "    throughputs = []\n",
    "    generated_texts = []\n",
    "    perplexities = []\n",
    "\n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\").to(device)  # Ensure input is on the same device\n",
    "        text_latencies = []\n",
    "        text_energy_per_token = []\n",
    "        text_energy_per_flops = []\n",
    "        text_energy_per_task = []\n",
    "        text_throughput = []\n",
    "        text_generated = []\n",
    "        text_perplexities = []\n",
    "\n",
    "        for _ in range(bootstrapping):\n",
    "            energy_consumed, latency, flops, output = measure_energy_during_inference(\n",
    "                handle, model.generate, model, inputs, max_new_tokens=200\n",
    "            )\n",
    "            text_latencies.append(latency)\n",
    "\n",
    "            print(\"output:\", output)\n",
    "            output_tokens = output.size(-1)\n",
    "            energy_token = energy_consumed / output_tokens if output_tokens > 0 else 0\n",
    "            text_energy_per_token.append(energy_token)\n",
    "\n",
    "            # Energy per FLOPs calculation\n",
    "\n",
    "            print(\"text_energy_per_token:\", text_energy_per_token)\n",
    "            print(\"output_tokens:\", output_tokens)\n",
    "            print(\"flop:\", flops)\n",
    "            print(\"energy_consumed: \",energy_consumed)\n",
    "            energy_flop = energy_consumed / flops #if flops > 0 else 0\n",
    "            text_energy_per_flops.append(energy_flop)\n",
    "\n",
    "            # Energy per task (full inference energy)\n",
    "            text_energy_per_task.append(energy_consumed)\n",
    "\n",
    "            throughput = output_tokens / latency\n",
    "            text_throughput.append(throughput)\n",
    "\n",
    "            perplexity = calculate_perplexity(model, text, tokenizer)\n",
    "            text_perplexities.append(perplexity)\n",
    "\n",
    "            generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            filtered_generated_text = generated_text.replace(text, \"\").strip()\n",
    "            text_generated.append(filtered_generated_text)\n",
    "\n",
    "        latencies.append(text_latencies)\n",
    "        energy_per_token.append(text_energy_per_token)\n",
    "        energy_per_flops.append(text_energy_per_flops)\n",
    "        energy_per_task.append(text_energy_per_task)\n",
    "        throughputs.append(text_throughput)\n",
    "        generated_texts.append(text_generated)\n",
    "        perplexities.append(text_perplexities)\n",
    "\n",
    "    return latencies, energy_per_token, energy_per_flops, energy_per_task, throughputs, generated_texts, perplexities\n",
    "\n",
    "# Collect metrics for each category\n",
    "def collect_metrics_for_categories(df, categories, bootstrapping, model, tokenizer):\n",
    "    category_metrics = {}\n",
    "    handle = get_gpu_handle(gpu_index=0)\n",
    "\n",
    "    for category in categories:\n",
    "        print(f\"Processing category: {category}\")\n",
    "        texts = filter_texts_by_category(df, category)\n",
    "        latencies, energy_per_token, energy_per_flops, energy_per_task, throughputs, generated_texts, perplexities = run_experiment_for_texts(\n",
    "            texts, bootstrapping, handle, model, tokenizer\n",
    "        )\n",
    "\n",
    "        category_metrics[category] = {\n",
    "            \"latencies\": latencies,\n",
    "            \"energy_per_token\": energy_per_token,\n",
    "            \"energy_per_flops\": energy_per_flops,\n",
    "            \"energy_per_task\": energy_per_task,\n",
    "            \"throughput\": throughputs,\n",
    "            \"generated_texts\": generated_texts,\n",
    "            \"perplexities\": perplexities\n",
    "        }\n",
    "\n",
    "    shutdown_nvml()  \n",
    "    return category_metrics\n",
    "\n",
    "def filter_texts_by_category(df, category):\n",
    "    return df[df['category'] == category]['text'].values\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Example Usage\n",
    "file_path = \"../projects/question.jsonl\"\n",
    "bootstrapping = 2 \n",
    "df_mtconversation = load_dataset(file_path)\n",
    "\n",
    "#categories = [ 'common-sense']\n",
    "categories = ['knowledge', 'common-sense', 'coding', 'math']\n",
    "\n",
    "initialize_nvml()\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")\n",
    "\n",
    "metrics = collect_metrics_for_categories(df_mtconversation, categories, bootstrapping, model, tokenizer)\n",
    "\n",
    "# (Optionally, you can visualize the collected metrics here)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x800 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to plot the metrics\n",
    "def plot_metrics(metrics, categories):\n",
    "    num_categories = len(categories)\n",
    "\n",
    "    # Prepare the data for plotting\n",
    "    avg_latencies = []\n",
    "    avg_perplexities = []\n",
    "    avg_energy_per_flops = []\n",
    "    avg_energy_per_token = []\n",
    "    avg_energy_per_task = []\n",
    "\n",
    "    for category in categories:\n",
    "        if category in metrics:\n",
    "            avg_latencies.append(np.mean(metrics[category][\"latencies\"]))\n",
    "            avg_perplexities.append(np.mean(metrics[category][\"perplexities\"]))\n",
    "            avg_energy_per_flops.append(np.mean(metrics[category][\"energy_per_flops\"]))\n",
    "            avg_energy_per_token.append(np.mean(metrics[category][\"energy_per_token\"]))\n",
    "            avg_energy_per_task.append(np.mean(metrics[category][\"energy_per_task\"]))\n",
    "        else:\n",
    "            avg_latencies.append(0)\n",
    "            avg_perplexities.append(0)\n",
    "            avg_energy_per_flops.append(0)\n",
    "            avg_energy_per_token.append(0)\n",
    "            avg_energy_per_task.append(0)\n",
    "\n",
    "    x = np.arange(num_categories)  # the label locations\n",
    "    width = 0.15  # the width of the bars\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "    # Plot latencies\n",
    "    bars1 = ax1.bar(x - 2*width, avg_latencies, width, label='Average Latency (s)', color='b')\n",
    "    ax1.set_ylabel('Average Latency (s)', color='b')\n",
    "    ax1.tick_params(axis='y', labelcolor='b')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(categories)\n",
    "\n",
    "    # Create a second y-axis for perplexities\n",
    "    ax2 = ax1.twinx()\n",
    "    bars2 = ax2.bar(x - width, avg_perplexities, width, label='Average Perplexity', color='g')\n",
    "    ax2.set_ylabel('Average Perplexity', color='g')\n",
    "    ax2.tick_params(axis='y', labelcolor='g')\n",
    "\n",
    "    # Create a third y-axis for energy per FLOPs\n",
    "    ax3 = ax1.twinx()\n",
    "    bars3 = ax3.bar(x, avg_energy_per_flops, width, label='Energy per FLOP (Joules)', color='r')\n",
    "    ax3.spines['right'].set_position(('outward', 60))  # move the third y-axis to the right\n",
    "    ax3.set_ylabel('Energy per FLOP (Joules)', color='r')\n",
    "    ax3.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "    # Create a fourth y-axis for energy per token\n",
    "    ax4 = ax1.twinx()\n",
    "    bars4 = ax4.bar(x + width, avg_energy_per_token, width, label='Energy per Token (Joules)', color='purple')\n",
    "    ax4.spines['right'].set_position(('outward', 120))  # move the fourth y-axis to the right\n",
    "    ax4.set_ylabel('Energy per Token (Joules)', color='purple')\n",
    "    ax4.tick_params(axis='y', labelcolor='purple')\n",
    "\n",
    "\n",
    "plot_metrics(metrics, categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# energy_per_flops\n",
    "\n",
    "# with nvidia nsights - nvtx (does not work yet): youtube:\n",
    "short video: https://www.youtube.com/watch?v=5Gxx59Q0g6o\n",
    "hands on : https://www.youtube.com/watch?v=3DAYN-onSzY\n",
    "\n",
    "\n",
    "multiple different videos available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 77\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Example usage with a loop over your text inputs\u001b[39;00m\n\u001b[1;32m     76\u001b[0m texts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample input 1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample input 2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample input 3\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# Replace with actual text inputs\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m()  \u001b[38;5;66;03m# Your model loading logic\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Running the experiment to collect metrics\u001b[39;00m\n\u001b[1;32m     80\u001b[0m latencies, energy_per_task, energy_per_flop \u001b[38;5;241m=\u001b[39m run_experiment_for_texts(texts, model, handle)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_model' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import nvtx\n",
    "import pynvml\n",
    "from torch.profiler import profile, ProfilerActivity\n",
    "\n",
    "# Initialize NVML for GPU power usage\n",
    "pynvml.nvmlInit()\n",
    "handle = pynvml.nvmlDeviceGetHandleByIndex(0)  # Assuming single GPU\n",
    "\n",
    "# Function to measure GPU energy consumption (in Joules)\n",
    "def get_gpu_energy(handle, duration_sec):\n",
    "    power_draw = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000  # Convert from mW to W\n",
    "    return power_draw * duration_sec  # Energy in Joules\n",
    "\n",
    "# Function to calculate FLOPs using PyTorch's profiler\n",
    "def calculate_flops(model, inputs):\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], profile_memory=True, record_shapes=True) as prof:\n",
    "        with torch.no_grad():\n",
    "            model(inputs)\n",
    "    # Sum the FLOPs from the profiler\n",
    "    flops = sum([event.cpu_time for event in prof.key_averages()])\n",
    "    return flops\n",
    "    \n",
    "def calculate_perplexity(model, input_text, tokenizer):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)  # Ensure input is on the same device\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "        perplexity = torch.exp(loss)\n",
    "    return perplexity.item()\n",
    "\n",
    "# Function to run the experiment for each text (inference)\n",
    "def run_experiment_for_texts(texts, model, handle):\n",
    "    latencies = []\n",
    "    energy_per_task = []\n",
    "    energy_per_flop = []\n",
    "    perplexities = []\n",
    "    \n",
    "    for text in texts:\n",
    "        input_tensor = preprocess_text(text)\n",
    "        \n",
    "        # Start energy and time measurement\n",
    "        start_time = time.time()\n",
    "        nvtx.range_push(\"Task Inference\")\n",
    "\n",
    "        # Measure initial GPU energy\n",
    "        initial_energy = get_gpu_energy(handle, 0)\n",
    "\n",
    "        # Run inference\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "\n",
    "        # End NVTX range\n",
    "        nvtx.range_pop()\n",
    "\n",
    "        # Measure final GPU energy\n",
    "        end_time = time.time()\n",
    "        final_energy = get_gpu_energy(handle, end_time - start_time)\n",
    "\n",
    "        # Calculate task energy consumption\n",
    "        task_energy = final_energy - initial_energy\n",
    "\n",
    "        # Measure latency\n",
    "        latency = end_time - start_time\n",
    "        latencies.append(latency)\n",
    "\n",
    "        # Calculate FLOPs for this task\n",
    "        task_flops = calculate_flops(model, input_tensor)\n",
    "\n",
    "        # Calculate Energy per Task (Joules per task)\n",
    "        energy_per_task.append(task_energy)\n",
    "\n",
    "        # Calculate Energy per FLOP (Joules per FLOP)\n",
    "        if task_flops > 0:\n",
    "            energy_per_flop.append(task_energy / task_flops)\n",
    "        else:\n",
    "            energy_per_flop.append(np.nan)  # Handle case where FLOP estimation is zero\n",
    "        \n",
    "    return latencies, energy_per_task, energy_per_flop\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# chat gpt:\n",
    "# Example usage with a loop over your text inputs\n",
    "texts = [\"Sample input 1\", \"Sample input 2\", \"Sample input 3\"]  # Replace with actual text inputs\n",
    "model = load_model()  # Your model loading logic\n",
    "\n",
    "# Running the experiment to collect metrics\n",
    "latencies, energy_per_task, energy_per_flop = run_experiment_for_texts(texts, model, handle)\n",
    "\n",
    "# Example output\n",
    "for i, text in enumerate(texts):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Latency: {latencies[i]:.4f} seconds\")\n",
    "    print(f\"Energy per Task: {energy_per_task[i]:.4f} Joules\")\n",
    "    print(f\"Energy per FLOP: {energy_per_flop[i]:.4e} Joules/FLOP\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Clean up NVML\n",
    "pynvml.nvmlShutdown()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MT_Bench with vLLM\n",
    "\n",
    "\n",
    "### Das funktioniert noch nicht mit vLLM!!!  out of memory error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Patrick/vllmenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-09-26 16:22:31,750\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import transformers\n",
    "import accelerate\n",
    "import vllm\n",
    "import bitsandbytes\n",
    "from vllm import LLM, SamplingParams\n",
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib\n",
    "from collections import Counter\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "#matplotlib.use('TkAgg')\n",
    "#from awq import AutoAWQForCausalLM\n",
    "#from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MMLU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Science, Technology, Engineering, Mathematics = stem\n",
    "stem = [\"clinical_knowledge\",\n",
    "\"medical_genetics\", \n",
    "\"high_school_physics\",\n",
    "\"virology\",\n",
    "\"high_school_biology\",\n",
    "\"abstract_algebra\",\n",
    "\"professional_medicine\",\n",
    "\"nutrition\",\n",
    "\"machine_learning\",\n",
    "\"anatomy\",\n",
    "\"college_medicine\",\n",
    "\"college_chemistry\",\n",
    "\"elementary_mathematics\",\n",
    "\"human_aging\",\n",
    "\"college_mathematics\",\n",
    "\"high_school_statistics\",\n",
    "\"high_school_mathematics\",\n",
    "\"high_school_computer_science\",\n",
    "\"conceptual_physics\",\n",
    "\"high_school_chemistry\",\n",
    "\"college_physics\",\n",
    "\"electrical_engineering\",\n",
    "\"astronomy\",\n",
    "\"college_biology\",\n",
    "\"computer_security\"]\n",
    "\n",
    "humanities= [\"high_school_european_history\",\n",
    "\"high_school_us_history\",\n",
    "\"high_school_world_history\",\n",
    "\"philosophy\",\n",
    "\"global_facts\",\n",
    "\"security_studies\",\n",
    "\"prehistory\",\n",
    "\"high_school_government_and_politics\",\n",
    "\"logical_fallacies\",\n",
    "\"international_law\",\n",
    "\"jurisprudence\",\n",
    "\"world_religions\",\n",
    "\"us_foreign_policy\",\n",
    "\"moral_scenarios\",\n",
    "\"moral_disputes\"\n",
    "]\n",
    "\n",
    "sociology = [\"sociology\",\n",
    "\"professional_psychology\",\n",
    "\"high_school_psychology\",\n",
    "\"human_sexuality\"]\n",
    "\n",
    "others = [\"business_ethics\",\n",
    "\"high_school_microeconomics\",\n",
    "\"econometrics\",\n",
    "\"professional_accounting\",\n",
    "\"public_relations\",\n",
    "\"marketing\",\n",
    "\"professional_law\",\n",
    "\"management\",\n",
    "\"miscellaneous\",\n",
    "\"high_school_macroeconomics\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "def convert_to_dataframe(categories):\n",
    "    category_dataframes = {}  # Dictionary to store DataFrames for each category\n",
    "    \n",
    "    for category in categories:\n",
    "        print(\"Loading Data for category: \", category)\n",
    "        \n",
    "        # Load the dataset for the given category\n",
    "        mmlu_dataset = load_dataset(\"lukaemon/mmlu\", category, split='validation', trust_remote_code=True)\n",
    "        \n",
    "        # Create a DataFrame for the current category\n",
    "        df_category = pd.DataFrame({\n",
    "            'input': mmlu_dataset['input'],  # The question or prompt\n",
    "            'A': mmlu_dataset['A'],          # Option A\n",
    "            'B': mmlu_dataset['B'],          # Option B\n",
    "            'C': mmlu_dataset['C'],          # Option C\n",
    "            'D': mmlu_dataset['D'],          # Option D\n",
    "            'target': mmlu_dataset['target'] # The correct answer (e.g., 'A', 'B', 'C', 'D')\n",
    "        })\n",
    "        \n",
    "        # Store the DataFrame in the dictionary, with the category as the key\n",
    "        category_dataframes[category] = df_category\n",
    "    \n",
    "    return category_dataframes\n",
    "\n",
    "# Access a DataFrame for a specific category\n",
    "#print(category_dfs.head())  # Example for checking 'high_school_biology'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "stem, humanities ..etc\n",
    "\"\"\"\n",
    "\n",
    "# Example list of categories\n",
    "categories = ['high_school_biology', 'abstract_algebra', 'professional_medicine', 'nutrition']\n",
    "\n",
    "# Call the function and get the dictionary of DataFrames\n",
    "category_dfs = convert_to_dataframe(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from datasets import load_dataset\n",
    "\n",
    "#cataegory = \"sociology\"\n",
    "# Load the MMLU dataset\n",
    "#def load_mmlu_dataset():\n",
    "#    mmlu_dataset = load_dataset(\"lukaemon/mmlu\",category, split='validation',trust_remote_code=True)\n",
    "#    return mmlu_dataset\n",
    "\n",
    "\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\", device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")\n",
    "\n",
    "def get_gpu_power(gpu_index=1): #measuring for gpu1\n",
    "    \"\"\"Fetches the current power consumption of the GPU using nvidia-smi.\"\"\"\n",
    "    result = subprocess.run(['nvidia-smi', '--query-gpu=power.draw', '--format=csv,noheader,nounits', '-i',str(gpu_index)], \n",
    "                            stdout=subprocess.PIPE, text=True)\n",
    "    power = float(result.stdout.strip())  # Power in watts\n",
    "    return power\n",
    "\n",
    "def convert_to_dataframe(categories):\n",
    "    category_dataframes = {}  # Dictionary to store DataFrames for each category\n",
    "    \n",
    "    for category in categories:\n",
    "        print(\"Loading Data for category: \", category)\n",
    "        \n",
    "        # Load the dataset for the given category\n",
    "        mmlu_dataset = load_dataset(\"lukaemon/mmlu\",category, split='validation',trust_remote_code=True)\n",
    "        #print(type(mmlu_dataset))\n",
    "        # Create a DataFrame for the current category\n",
    "        df_category = pd.DataFrame({\n",
    "            'input': mmlu_dataset['input'],  # The question or prompt\n",
    "            'A': mmlu_dataset['A'],          # Option A\n",
    "            'B': mmlu_dataset['B'],          # Option B\n",
    "            'C': mmlu_dataset['C'],          # Option C\n",
    "            'D': mmlu_dataset['D'],          # Option D\n",
    "            'target': mmlu_dataset['target'] # The correct answer (e.g., 'A', 'B', 'C', 'D')\n",
    "        })\n",
    "        \n",
    "        # Store the DataFrame in the dictionary, with the category as the key\n",
    "        category_dataframes[category] = df_category\n",
    "    print(\"loading_ data finish\")\n",
    "    return category_dataframes\n",
    "\n",
    "# Filter dataset by category\n",
    "#def filter_texts_by_category(df, category):\n",
    "#    return df[df['category'] == category]['text'].values\n",
    "\n",
    "# Filter dataset by subject category (e.g., 'high_school_biology', 'abstract_algebra')\n",
    "def filter_dict_by_category(df, category):\n",
    "    return df[category] \n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "# Run the bootstrapping experiment for multiple-choice questions in a given category\n",
    "def run_experiment_for_texts(datadictionary,categories, bootstrapping ):\n",
    "    category_latencies = []\n",
    "    category_energy_per_token = []\n",
    "    \n",
    "    \n",
    "    #print(categories)\n",
    "    category_accuracy= []\n",
    "\n",
    "\n",
    "\n",
    "    # task in one category\n",
    "    for category in categories:\n",
    "        data = datadictionary[category]#filter_dict_by_category(datadictionary, category)\n",
    "        question_text = data['input'].values\n",
    "        choices = [data['A'].values, data['B'].values, data['C'].values, data['D'].values]\n",
    "        correct_answer = data['target'].values\n",
    "        \n",
    "        task_accuracy= []\n",
    "        task_latencies = []\n",
    "        task_energy_per_token = []\n",
    "        #print(\" type question_text: \", question_text)\n",
    "        #print(\"len question_text: \", len(question_text))\n",
    "        print(\"processing category: \", category)\n",
    "\n",
    "\n",
    "        # Prompts of one tasks\n",
    "        for i, tasks in enumerate (question_text):\n",
    "\n",
    "\n",
    "\n",
    "            print(\"i : \", i)\n",
    "            # Concatenate question with options for LLM input\n",
    "            full_input = f\"Question: {question_text[i]}\\nA) {choices[0][i]}\\nB) ,{choices[1][i]}\\nC) ,{choices[2][i]}\\nD) ,{choices[3][i]}\"\n",
    "            #print(\"full input: \", full_input)\n",
    "            inputs = tokenizer(full_input, return_tensors=\"pt\").to(\"cuda\")  # Prepare input tensors\n",
    "\n",
    "            text_latencies = []\n",
    "            text_energy_per_token = []\n",
    "            correct_predictions = 0  # To calculate accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # Prompt Bootstrapping \n",
    "            for _ in range(bootstrapping):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                print(_)\n",
    "                power_start = get_gpu_power()  # Assuming a function to get GPU power\n",
    "                start_time = time.time()\n",
    "\n",
    "                # Generate the model's response\n",
    "                output = model.generate(inputs['input_ids'], max_new_tokens=200, do_sample=False)  # Adjust tokens if necessary\n",
    "\n",
    "                end_time = time.time()\n",
    "                power_end = get_gpu_power()\n",
    "\n",
    "                # Measure latency\n",
    "                latency = end_time - start_time\n",
    "                text_latencies.append(latency)\n",
    "\n",
    "                # Calculate energy consumption\n",
    "                avg_power = (power_start + power_end) / 2\n",
    "                energy = avg_power * latency\n",
    "\n",
    "                # Token count from output (assuming a tensor output)\n",
    "                output_tokens = output[0].shape[0]\n",
    "                energy_token = energy / output_tokens if output_tokens > 0 else 0\n",
    "                text_energy_per_token.append(energy_token)\n",
    "\n",
    "                # Decode the model's generated answer (you might need to adjust based on model output format)\n",
    "                generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "                # Check if the model's generated answer matches the correct answer\n",
    "                if correct_answer[i] in generated_text:\n",
    "                    correct_predictions += 1\n",
    "\n",
    "            task_latencies.append(np.mean(text_latencies))\n",
    "            task_energy_per_token.append(np.mean(text_energy_per_token))\n",
    "            accuracy = correct_predictions / bootstrapping \n",
    "            task_accuracy.append(accuracy)\n",
    "    category_accuracy.append(np.array(task_accuracy)/len(question_text))\n",
    "    return task_latencies, task_energy_per_token, category_accuracy\n",
    "\n",
    "\n",
    "# Store and collect metrics for each category\n",
    "# Collect metrics for each category\n",
    "def collect_metrics_for_categories(df, categories, bootstrapping):\n",
    "    category_metrics = {}\n",
    "\n",
    "    \"\"\"    for category in categories:\n",
    "            print(f\"Processing category: {category}\")\n",
    "            texts = filter_texts_by_category(df, category)\n",
    "\n",
    "            if texts.empty:\n",
    "                print(f\"No texts found for category {category}\")\n",
    "                continue\n",
    "    \"\"\"\n",
    "    latencies, energy_per_token, accuracy = run_experiment_for_texts(data_dict, categories, bootstrapping)\n",
    "\n",
    "    # Store metrics for each category\n",
    "    category_metrics[category] = {\n",
    "        \"latencies\": latencies,\n",
    "        \"energy_per_token\": energy_per_token,\n",
    "        \"accuracy\": accuracy\n",
    "    }\n",
    "\n",
    "    return category_metrics\n",
    "\n",
    "# Plot the energy consumption per token comparison\n",
    "def plot_energy_vs_latency(metrics, categories):\n",
    "    for category in categories:\n",
    "        category_data = metrics[category]\n",
    "        energy_per_token = category_data[\"energy_per_token\"]\n",
    "        latencies = category_data[\"latencies\"]\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "\n",
    "        # Plot energy per token\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot([sum(x)/len(x) for x in energy_per_token], marker='o', color='blue', label='Energy per Token (J)')\n",
    "        plt.title(f\"Energy per Token for {category}\")\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Energy (J)')\n",
    "        plt.legend()\n",
    "\n",
    "        # Plot latencies\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot([sum(x)/len(x) for x in latencies], marker='o', color='green', label='Latency (s)')\n",
    "        plt.title(f\"Latency for {category}\")\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Latency (s)')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    #plot_energy_vs_latency(metrics, categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Processing data\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "categories = stem\n",
    "categories = sociology  #sociology is shorter / less prompts\n",
    "#mmlu_dataset = load_mmlu_dataset()\n",
    "data_dict = convert_to_dataframe(categories)\n",
    "print(\"data_dict geladen\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run Experiments\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "bootstrapping = 3  # Number of iterations for each prompt\n",
    "\n",
    "# Collect metrics for each category\n",
    "metrics = collect_metrics_for_categories(data_dict, categories, bootstrapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BACK UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import pynvml\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch.profiler\n",
    "\n",
    "# Specify the GPU device\n",
    "device = \"cuda:0\"  # Change to the appropriate GPU if needed\n",
    "\n",
    "# Initialize NVML for power measurement\n",
    "def initialize_nvml():\n",
    "    pynvml.nvmlInit()\n",
    "\n",
    "def shutdown_nvml():\n",
    "    pynvml.nvmlShutdown()\n",
    "\n",
    "def get_gpu_handle(gpu_index=0):\n",
    "    return pynvml.nvmlDeviceGetHandleByIndex(gpu_index)\n",
    "\n",
    "# Measure GPU power consumption over a period of time\n",
    "def measure_power_consumption(handle, duration_sec=1.0, interval_sec=0.1):\n",
    "    power_readings = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    while (time.time() - start_time) < duration_sec:\n",
    "        power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0  # Convert from mW to W\n",
    "        power_readings.append(power)\n",
    "        time.sleep(interval_sec)\n",
    "    \n",
    "    return sum(power_readings) / len(power_readings) if power_readings else 0\n",
    "\n",
    "\n",
    "#inputs['input_ids'], max_new_tokens=200, do_sample=True\n",
    "# Measure energy consumed during inference\n",
    "def measure_energy_during_inference(handle, inference_function):\n",
    "    power_start = measure_power_consumption(handle, duration_sec=0.5)  # Measure power before inference\n",
    "    \n",
    "    start_time = time.time()  # Start time for inference\n",
    "    result = inference_function()  # Run inference\n",
    "    end_time = time.time()  # End time for inference\n",
    "    \n",
    "    power_end = measure_power_consumption(handle, duration_sec=0.5)  # Measure power after inference\n",
    "    \n",
    "    # Average the power readings before and after inference\n",
    "    avg_power = (power_start + power_end) / 2\n",
    "    elapsed_time = end_time - start_time  # Total inference time\n",
    "    energy_consumed = avg_power * elapsed_time  # Energy consumed in Joules\n",
    "\n",
    "    return energy_consumed, elapsed_time, result\n",
    "\n",
    "# Count FLOPs using PyTorch Profiler\n",
    "def count_flops(model, input_ids):\n",
    "    # Use torch.profiler to count FLOPs\n",
    "    with torch.profiler.profile(\n",
    "        activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],\n",
    "        with_stack=True,\n",
    "        record_shapes=True,\n",
    "    ) as prof:\n",
    "        with torch.no_grad():\n",
    "            model.generate(input_ids)\n",
    "\n",
    "    # Sum the FLOPs from profiler events\n",
    "    flops = sum(event.flops for event in prof.events() if event.flops is not None)\n",
    "    return flops\n",
    "\n",
    "# Calculate perplexity for generated text\n",
    "def calculate_perplexity(model, input_text, tokenizer):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)  # Tokenize input and move to device\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "        perplexity = torch.exp(loss)\n",
    "    return perplexity.item()\n",
    "\n",
    "# Main function to run experiments and measure energy & FLOPs\n",
    "def run_experiment_for_texts(texts, bootstrapping, handle, model, tokenizer):\n",
    "    latencies = []\n",
    "    energy_per_task = []\n",
    "    energy_per_flop = []\n",
    "    generated_texts = []\n",
    "    perplexities = []\n",
    "\n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\").to(device)  # Tokenize and move input to device\n",
    "        text_latencies = []\n",
    "        text_energy_per_task = []\n",
    "        text_energy_per_flop = []\n",
    "        text_generated = []\n",
    "        text_perplexities = []\n",
    "\n",
    "        for _ in range(bootstrapping):\n",
    "            # Count FLOPs for the input\n",
    "            flops = count_flops(model, inputs['input_ids'])\n",
    "\n",
    "            # Measure energy during inference\n",
    "            energy_consumed, latency, output = measure_energy_during_inference(\n",
    "                handle, model.generate(inputs['input_ids'], max_new_tokens=200, do_sample=True)\n",
    "            )\n",
    "            text_latencies.append(latency)\n",
    "\n",
    "            # Energy per Task\n",
    "            text_energy_per_task.append(energy_consumed)\n",
    "\n",
    "            # Energy per FLOP\n",
    "            energy_flop = energy_consumed / flops if flops > 0 else 0\n",
    "            text_energy_per_flop.append(energy_flop)\n",
    "\n",
    "            # Generate text\n",
    "            generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            filtered_generated_text = generated_text.replace(text, \"\").strip()\n",
    "            text_generated.append(filtered_generated_text)\n",
    "\n",
    "            # Calculate perplexity\n",
    "            perplexity = calculate_perplexity(model, text, tokenizer)\n",
    "            text_perplexities.append(perplexity)\n",
    "\n",
    "        latencies.append(text_latencies)\n",
    "        energy_per_task.append(text_energy_per_task)\n",
    "        energy_per_flop.append(text_energy_per_flop)\n",
    "        generated_texts.append(text_generated)\n",
    "        perplexities.append(text_perplexities)\n",
    "\n",
    "    return latencies, energy_per_task, energy_per_flop, generated_texts, perplexities\n",
    "\n",
    "# Function to collect metrics for multiple text categories\n",
    "def collect_metrics_for_categories(df, categories, bootstrapping, model, tokenizer):\n",
    "    category_metrics = {}\n",
    "    handle = get_gpu_handle(gpu_index=0)\n",
    "\n",
    "    for category in categories:\n",
    "        print(f\"Processing category: {category}\")\n",
    "        texts = filter_texts_by_category(df, category)\n",
    "        latencies, energy_per_task, energy_per_flop, generated_texts, perplexities = run_experiment_for_texts(\n",
    "            texts, bootstrapping, handle, model, tokenizer\n",
    "        )\n",
    "        \n",
    "\n",
    "        category_metrics[category] = {\n",
    "            \"latencies\": latencies,\n",
    "            \"energy_per_task\": energy_per_task,\n",
    "            \"energy_per_flop\": energy_per_flop,\n",
    "            \"generated_texts\": generated_texts,\n",
    "            \"perplexities\": perplexities,\n",
    "        }\n",
    "\n",
    "    shutdown_nvml()  # Close NVML after measurements\n",
    "    return category_metrics\n",
    "\n",
    "# Function to filter texts based on category\n",
    "def filter_texts_by_category(df, category):\n",
    "    return df[df['category'] == category]['text'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
