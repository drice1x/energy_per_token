{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MT_Bench dataset\n",
    "import numpy as np\n",
    "import transformers\n",
    "import accelerate\n",
    "#import vllm\n",
    "import bitsandbytes\n",
    "#from vllm import LLM, SamplingParams\n",
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib\n",
    "from collections import Counter\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Science, Technology, Engineering, Mathematics = stem\n",
    "stem = [\"clinical_knowledge\",\n",
    "\"medical_genetics\", \n",
    "\"high_school_physics\",\n",
    "\"virology\",\n",
    "\"high_school_biology\",\n",
    "\"abstract_algebra\",\n",
    "\"professional_medicine\",\n",
    "\"nutrition\",\n",
    "\"machine_learning\",\n",
    "\"anatomy\",\n",
    "\"college_medicine\",\n",
    "\"college_chemistry\",\n",
    "\"elementary_mathematics\",\n",
    "\"human_aging\",\n",
    "\"college_mathematics\",\n",
    "\"high_school_statistics\",\n",
    "\"high_school_mathematics\",\n",
    "\"high_school_computer_science\",\n",
    "\"conceptual_physics\",\n",
    "\"high_school_chemistry\",\n",
    "\"college_physics\",\n",
    "\"electrical_engineering\",\n",
    "\"astronomy\",\n",
    "\"college_biology\",\n",
    "\"computer_security\"]\n",
    "\n",
    "humanities= [\"high_school_european_history\",\n",
    "\"high_school_us_history\",\n",
    "\"high_school_world_history\",\n",
    "\"philosophy\",\n",
    "\"global_facts\",\n",
    "\"security_studies\",\n",
    "\"prehistory\",\n",
    "\"high_school_government_and_politics\",\n",
    "\"logical_fallacies\",\n",
    "\"international_law\",\n",
    "\"jurisprudence\",\n",
    "\"world_religions\",\n",
    "\"us_foreign_policy\",\n",
    "\"moral_scenarios\",\n",
    "\"moral_disputes\"\n",
    "]\n",
    "\n",
    "sociology = [\"sociology\",\n",
    "\"professional_psychology\",\n",
    "\"high_school_psychology\",\n",
    "\"human_sexuality\"]\n",
    "\n",
    "economics = [\"business_ethics\",\n",
    "\"high_school_microeconomics\",\n",
    "\"econometrics\",\n",
    "\"professional_accounting\",\n",
    "\"public_relations\",\n",
    "\"marketing\",\n",
    "\"professional_law\",\n",
    "\"management\",\n",
    "\"miscellaneous\",\n",
    "\"high_school_macroeconomics\"]\n",
    "\n",
    "math = [\"abstract_algebra\",\n",
    "\t\"college_mathematics\",\n",
    "\t\"elementary_mathematics\",\n",
    "\t\"high_school_mathematics\",\n",
    "\t\"high_school_statistics\"]\n",
    "\n",
    "math1 = [\"abstract_algebra\",\n",
    "\t\"college_mathematics\",\n",
    "#\t\"elementary_mathematics\",\n",
    "#\t\"high_school_mathematics\",\n",
    "\t\"high_school_statistics\"]\n",
    "\n",
    "computer_science = [\"college_computer_science\",\n",
    "\t\"computer_security\",\n",
    "\t\"high_school_computer_science\",\n",
    "\t\"machine_learning\"]\n",
    "\n",
    "engineering = [\"electrical_engineering\"]\n",
    "\n",
    "natural_sciences = [\"astronomy\",\"college_biology\", \"college_physics\",\n",
    "\t\t\t\t\t\"conceptual_physics\", \"high_school_biology\", \"high_school_chemistry\",\n",
    "\t\t\t\t\t\"high_school_physics\"]\n",
    "\n",
    "health = [#\"anatomy\",\n",
    "\t\"clinical_knowledge\",\n",
    "\t\"college_medicine\",\n",
    "\t\"human_aging\",\n",
    "\t\"medical_genetics\",\n",
    "\t\"nutrition\",\n",
    "\t\"professional_medicine\",\n",
    "\t#\"virology\"\n",
    "\t]\n",
    "\n",
    "\n",
    "\n",
    "semanticdifferent = [\"abstract_algebra\",\"college_mathematics\",\"college_computer_science\",\"computer_security\", \"anatomy\",\"virology\", \"professional_medicine\",\"econometrics\", \"management\",\"sociology\", \"high_school_world_history\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import threading\n",
    "import torch\n",
    "import pynvml\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.profiler import profile, ProfilerActivity\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import subprocess\n",
    "\n",
    "# Initialize NVML for power measurement\n",
    "def initialize_nvml():\n",
    "    pynvml.nvmlInit()\n",
    "\n",
    "def shutdown_nvml():\n",
    "    pynvml.nvmlShutdown()\n",
    "\n",
    "def get_gpu_handle(gpu_index=0):\n",
    "    return pynvml.nvmlDeviceGetHandleByIndex(gpu_index)\n",
    "\n",
    "def start_power_monitoring(handle, interval_sec=0.1):\n",
    "    power_readings = []\n",
    "    running = True\n",
    "\n",
    "    def monitor():\n",
    "        while running:\n",
    "            power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0  # Convert from mW to W\n",
    "            timestamp = time.time()\n",
    "            power_readings.append((timestamp, power))\n",
    "            time.sleep(interval_sec)\n",
    "\n",
    "    thread = threading.Thread(target=monitor)\n",
    "    thread.start()\n",
    "\n",
    "    def stop():\n",
    "        nonlocal running\n",
    "        running = False\n",
    "        thread.join()\n",
    "\n",
    "    return power_readings, stop\n",
    "\n",
    "\n",
    "# Map generated text to one of the options A, B, C, D\n",
    "def map_generated_text_to_option(generated_text):\n",
    "    valid_options = ['A', 'B', 'C', 'D']\n",
    "    generated_text = generated_text.strip().upper()\n",
    "    if generated_text in valid_options:\n",
    "        return generated_text\n",
    "    #else:\n",
    "        # Attempt to extract the option from the text\n",
    "        #for option in valid_options:\n",
    "            #if option in generated_text:\n",
    "                #return option\n",
    "        # If no valid option is found, return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def calculate_perplexity1(model, inputs):\n",
    "    #inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)  # Ensure input is on the same device\n",
    "    with torch.no_grad():\n",
    "        outputs = model( labels=inputs)\n",
    "        loss = outputs.loss\n",
    "        perplexity = torch.exp(loss)\n",
    "    return perplexity.item()\n",
    "\n",
    "\n",
    "def calculate_perplexity(model, inputs, attention_mask=None):\n",
    "    # Assume `inputs` is a tensor directly containing input_ids\n",
    "    input_ids = inputs  # Directly use inputs if it's a tensor\n",
    "    labels = input_ids.clone()  # Copy input_ids to use as labels\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Pass input_ids and optionally attention_mask to the model\n",
    "        if attention_mask is not None:\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        else:\n",
    "            outputs = model(input_ids=input_ids, labels=labels)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        perplexity = torch.exp(loss)\n",
    "    \n",
    "    return perplexity.item()\n",
    "\n",
    "\n",
    "\n",
    "# Measure energy consumed during inference and FLOPs\n",
    "def measure_energy_during_inference(handle, inference_function, model, inputs, max_new_tokens=1):\n",
    "    print(f\"tokens: {max_new_tokens}\")\n",
    "    \n",
    "    # Start power monitoring\n",
    "    power_readings, stop_monitoring = start_power_monitoring(handle, interval_sec=0.05)\n",
    "    \n",
    "    # Start time for inference\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Measure FLOPs using PyTorch profiler\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], with_flops=True, record_shapes=False) as prof:\n",
    "        with torch.no_grad():\n",
    "            result = inference_function(inputs['input_ids'], max_new_tokens=max_new_tokens, do_sample=False )#num_beams=1)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Stop power monitoring\n",
    "    stop_monitoring()\n",
    "\n",
    "    # Filter power readings during inference\n",
    "    power_during_inference = [p for t, p in power_readings if start_time <= t <= end_time]\n",
    "\n",
    "    # Calculate average power and energy consumed\n",
    "    if power_during_inference:\n",
    "        avg_power = sum(power_during_inference) / len(power_during_inference)\n",
    "        elapsed_time = end_time - start_time\n",
    "        energy_consumed = avg_power * elapsed_time\n",
    "    else:\n",
    "        avg_power = 0\n",
    "        energy_consumed = 0\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "    # Calculate FLOPs\n",
    "    flops = sum([event.flops for event in prof.key_averages() if event.flops is not None])\n",
    "\n",
    "    perplexity = calculate_perplexity(model, inputs['input_ids'])\n",
    "\n",
    "    return energy_consumed, elapsed_time, flops, result, power_during_inference, perplexity\n",
    "\n",
    "# Load the MMLU dataset for specified categories\n",
    "def load_mmlu_data(categories):\n",
    "    category_dataframes = {}  # Dictionary to store DataFrames for each category\n",
    "        \n",
    "    for category in categories:\n",
    "        print(\"Loading Data for category: \", category)\n",
    "            \n",
    "        # Load the dataset for the given category\n",
    "        mmlu_dataset = load_dataset(\"lukaemon/mmlu\", category, split='validation', trust_remote_code=True)\n",
    "        \n",
    "        # Create a DataFrame for the current category\n",
    "        df_category = pd.DataFrame({\n",
    "            'input': mmlu_dataset['input'],  # The question or prompt\n",
    "            'A': mmlu_dataset['A'],          # Option A\n",
    "            'B': mmlu_dataset['B'],          # Option B\n",
    "            'C': mmlu_dataset['C'],          # Option C\n",
    "            'D': mmlu_dataset['D'],          # Option D\n",
    "            'target': mmlu_dataset['target'] # The correct answer (e.g., 'A', 'B', 'C', 'D')\n",
    "        })\n",
    "        \n",
    "        # Store the DataFrame in the dictionary, with the category as the key\n",
    "        category_dataframes[category] = df_category\n",
    "        \n",
    "    return category_dataframes\n",
    "\n",
    "# Run the experiment for a category in the MMLU dataset\n",
    "def run_experiment_for_mmlu_category(data, bootstrapping, handle, model, tokenizer, max_new_tokens):\n",
    "    latencies = []\n",
    "    energy_per_token = []\n",
    "    energy_per_flops = []\n",
    "    energy_per_task = []\n",
    "    throughputs = []\n",
    "    generated_texts = []\n",
    "    accuracies = []\n",
    "    flopslisttotal = []\n",
    "    energy_over_time = []\n",
    "    perplexities = []\n",
    "    power_over_time = []\n",
    "\n",
    "    for idx, row in data.iterrows():\n",
    "        # Construct the prompt\n",
    "        prompt = f\"Question: {row['input']}\\nA) {row['A']}\\nB) {row['B']}\\nC) {row['C']}\\nD) {row['D']}\\nAnswer:\"\n",
    "        #prompt = \"Hello, how are you my friend?\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")  # Ensure input is on the same device\n",
    "        text_latencies = []\n",
    "        text_energy_per_token = []\n",
    "        text_energy_per_flops = []\n",
    "        text_energy_per_task = []\n",
    "        text_throughput = []\n",
    "        text_generated = []\n",
    "        correct_predictions = 0  # To calculate accuracy\n",
    "        floplist = []\n",
    "        energy = []\n",
    "        power_inf = []\n",
    "        perplexity_prompt = []\n",
    "\n",
    "        for _ in range(bootstrapping):\n",
    "            energy_consumed, latency, flops, output, power_during_inference, perplexity = measure_energy_during_inference(\n",
    "                handle, model.generate, model, inputs, max_new_tokens=max_new_tokens\n",
    "            )\n",
    "            perplexity_prompt.append(perplexity)\n",
    "            power_inf.append(power_during_inference)\n",
    "            energy.append(energy_consumed)\n",
    "            text_latencies.append(latency)\n",
    "            output_tokens = output.size(-1) - inputs['input_ids'].size(-1)\n",
    "            energy_token = energy_consumed / output_tokens if output_tokens > 0 else 0\n",
    "            text_energy_per_token.append(energy_token)\n",
    "\n",
    "            energy_flop = energy_consumed / flops if flops > 0 else 0\n",
    "            text_energy_per_flops.append(energy_flop)\n",
    "            text_energy_per_task.append(energy_consumed)\n",
    "            throughput = output_tokens / latency if latency > 0 else 0\n",
    "            text_throughput.append(throughput)\n",
    "\n",
    "            # Decode the generated token\n",
    "            generated_text = tokenizer.decode(output[0][inputs['input_ids'].size(-1):], skip_special_tokens=True)\n",
    "            generated_text = generated_text.strip()\n",
    "            print(f\"generated text: {generated_text}\")\n",
    "            text_generated.append(generated_text)\n",
    "\n",
    "            floplist.append(flops)\n",
    "            \n",
    "            # Map the generated text to an option\n",
    "            mapped_answer = map_generated_text_to_option(generated_text)\n",
    "            print(f\"Generated answer: '{mapped_answer}' | Correct answer: '{row['target']}'\")\n",
    "            if mapped_answer == row['target']:\n",
    "                print(\"Adding to correct predictions\")\n",
    "                correct_predictions += 1\n",
    "\n",
    "        perplexities.append(perplexity_prompt)\n",
    "        power_over_time.append(power_inf)\n",
    "        energy_over_time.append(energy)\n",
    "        flopslisttotal.append(floplist)\n",
    "        accuracy = correct_predictions / bootstrapping\n",
    "        accuracies.append(accuracy)\n",
    "        latencies.append(text_latencies)\n",
    "        energy_per_token.append(text_energy_per_token)\n",
    "        energy_per_flops.append(text_energy_per_flops)\n",
    "        energy_per_task.append(text_energy_per_task)\n",
    "        throughputs.append(text_throughput)\n",
    "        generated_texts.append(text_generated)\n",
    "\n",
    "    overall_accuracy = np.mean(accuracies)\n",
    "    return latencies, energy_per_token, energy_per_flops, energy_per_task, throughputs, generated_texts, overall_accuracy, flopslisttotal, energy_over_time, power_over_time, perplexities\n",
    "\n",
    "# Collect metrics for each category\n",
    "def collect_metrics_for_categories(data_dict, categories, bootstrapping, model, tokenizer, max_new_tokens):\n",
    "    category_metrics = {}\n",
    "    handle = get_gpu_handle(gpu_index=0)\n",
    "\n",
    "    for category in categories:\n",
    "        print(f\"Processing category: {category}\")\n",
    "        data = data_dict[category]\n",
    "        latencies, energy_per_token, energy_per_flops, energy_per_task, throughputs, generated_texts, overall_accuracy, flopslisttotal, energy_over_time, power_over_time, perplexities = run_experiment_for_mmlu_category(\n",
    "            data, bootstrapping, handle, model, tokenizer, max_new_tokens\n",
    "        )\n",
    "\n",
    "        category_metrics[category] = {\n",
    "            \"latencies\": latencies,\n",
    "            \"energy_per_token\": energy_per_token,\n",
    "            \"energy_per_flops\": energy_per_flops,\n",
    "            \"energy_per_task\": energy_per_task,\n",
    "            \"throughput\": throughputs,\n",
    "            \"generated_texts\": generated_texts,\n",
    "            \"accuracy\": overall_accuracy,\n",
    "            \"flopstotal\": flopslisttotal,\n",
    "            \"energy_over_time\": energy_over_time,\n",
    "            \"power_over_time\": power_over_time,\n",
    "            \"perplexity\": perplexities\n",
    "        }\n",
    "\n",
    "    shutdown_nvml()  \n",
    "    return category_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = semanticdifferent # math computer_science health semanticdifferent\n",
    "categories = [math, economics, computer_science, natural_sciences, health, humanities, sociology, engineering]\n",
    "\n",
    "#category_text = \"math\"\n",
    "\n",
    "# Bootstrapping iterations\n",
    "bootstrapping = 3\n",
    "\n",
    "# max new output tokens\n",
    "max_new_tokens = 1\n",
    "\n",
    "initialize_nvml()\n",
    "\n",
    "# HF Access Token\n",
    "access_token = \"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\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = ['facebook/opt-125m'\n",
    "            #\"meta-llama/Llama-3.1-8B\" \n",
    "            #\"meta-llama/Llama-3.1-8B\"  \n",
    "            #\"facebook/opt-125m\"\n",
    "            #\"tiiuae/falcon-7b\"\n",
    "            #\"ProbeMedicalYonseiMAILab/medllama3-v20\"\n",
    "            #\"NTQAI/Nxcode-CQ-7B-orpo\"\n",
    "            #\"MathLLMs/MathCoder-L-7B\"\n",
    "        ]\n",
    "\n",
    "counter = 0\n",
    "Model_metrics_for_categories = []\n",
    "for category in categories:\n",
    "# Load MMLU data\n",
    "    #print(i)\n",
    "    counter +=1\n",
    "    data_dict = load_mmlu_data(category)\n",
    "    #print(data_dict.keys())\n",
    "    allmetrics = []\n",
    "    for models in model_name:\n",
    "        #model = AutoModelForCausalLM.from_pretrained(models, use_auth_token=access_token)\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(models, device_map=\"auto\", use_auth_token=access_token)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(models, use_auth_token=access_token)\n",
    "        flop_mmlu_metrics = collect_metrics_for_categories(data_dict, category, bootstrapping, model, tokenizer, max_new_tokens)\n",
    "        allmetrics.append(flop_mmlu_metrics)\n",
    "    Model_metrics_for_categories.append(allmetrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metrics(metrics, categories):\n",
    "    num_categories = len(categories)\n",
    "\n",
    "    # Prepare the data for plotting\n",
    "    avg_latencies = []\n",
    "    avg_perplexities = []\n",
    "    avg_energy_per_flops = []\n",
    "    avg_energy_per_token = []\n",
    "    avg_accuracy = []\n",
    "\n",
    "    # Populate the lists with average values or set to zero if the category is missing\n",
    "    for category in categories:\n",
    "        if category in metrics:\n",
    "            avg_latencies.append(np.mean(metrics[category].get(\"latencies\", [0])))\n",
    "            avg_perplexities.append(np.mean(metrics[category].get(\"perplexity\", [0])))\n",
    "            avg_energy_per_flops.append(np.mean(metrics[category].get(\"energy_per_flops\", [0])))\n",
    "            avg_energy_per_token.append(np.mean(metrics[category].get(\"energy_per_token\", [0])))\n",
    "            avg_accuracy.append(np.mean(metrics[category].get(\"accuracy\", [0])))\n",
    "        else:\n",
    "            avg_latencies.append(0)\n",
    "            avg_perplexities.append(0)\n",
    "            avg_energy_per_flops.append(0)\n",
    "            avg_energy_per_token.append(0)\n",
    "            avg_accuracy.append(0)\n",
    "\n",
    "    x = np.arange(num_categories)  # the label locations\n",
    "    width = 0.15  # the width of the bars\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "    # Plot latencies\n",
    "    bars1 = ax1.bar(x - 2 * width, avg_latencies, width, label='Average Latency (s)', color='b')\n",
    "    ax1.set_ylabel('Average Latency (s)', color='b')\n",
    "    ax1.tick_params(axis='y', labelcolor='b')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(categories)\n",
    "\n",
    "    # Create a second y-axis for perplexities\n",
    "    ax2 = ax1.twinx()\n",
    "    bars2 = ax2.bar(x - width, avg_perplexities, width, label='Average Perplexity', color='g')\n",
    "    ax2.set_ylabel('Average Perplexity', color='g')\n",
    "    ax2.tick_params(axis='y', labelcolor='g')\n",
    "\n",
    "    # Create a third y-axis for accuracy\n",
    "    ax3 = ax1.twinx()\n",
    "    bars3 = ax3.bar(x, avg_accuracy, width, label='Accuracy', color='grey')\n",
    "    ax3.spines['right'].set_position(('outward', 60))\n",
    "    ax3.set_ylabel('Accuracy', color='grey')\n",
    "    ax3.tick_params(axis='y', labelcolor='grey')\n",
    "\n",
    "    # Create a fourth y-axis for energy per FLOPs\n",
    "    ax4 = ax1.twinx()\n",
    "    bars4 = ax4.bar(x + width, avg_energy_per_flops, width, label='Energy per FLOP (Joules)', color='r')\n",
    "    ax4.spines['right'].set_position(('outward', 120))\n",
    "    ax4.set_ylabel('Energy per FLOP (Joules)', color='r')\n",
    "    ax4.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "    # Create a fifth y-axis for energy per token\n",
    "    ax5 = ax1.twinx()\n",
    "    bars5 = ax5.bar(x + 2 * width, avg_energy_per_token, width, label='Energy per Token (Joules)', color='purple')\n",
    "    ax5.spines['right'].set_position(('outward', 180))\n",
    "    ax5.set_ylabel('Energy per Token (Joules)', color='purple')\n",
    "    ax5.tick_params(axis='y', labelcolor='purple')\n",
    "\n",
    "    # Legend for all bars\n",
    "    fig.legend(loc='upper left', bbox_to_anchor=(0.1, 0.9))\n",
    "\n",
    "    plt.title(\"Metrics Across Categories\")\n",
    "    plt.show()\n",
    "\n",
    "# Call the plotting function\n",
    "plot_metrics(flop_mmlu_metrics, categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
