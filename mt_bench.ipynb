{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Patrick/projects/energy_per_token/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# MT_Bench dataset\n",
    "import numpy as np\n",
    "import transformers\n",
    "import accelerate\n",
    "#import vllm\n",
    "import bitsandbytes\n",
    "#from vllm import LLM, SamplingParams\n",
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib\n",
    "from collections import Counter\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['generic', 'knowledge', 'roleplay', 'common-sense', 'fermi',\n",
       "       'counterfactual', 'coding', 'math', 'writing'], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_dataset(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Example Usage\n",
    "file_path = \"./question.jsonl\"\n",
    "# bootstrapping = 2 \n",
    "df_mtconversation = load_dataset(file_path)\n",
    "df_mtconversation.category.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Patrick/projects/energy_per_token/venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7f82eb58e9b0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/Patrick/projects/energy_per_token/venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n",
      "/home/Patrick/projects/energy_per_token/venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:796: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing category: generic\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import threading\n",
    "import torch\n",
    "import pynvml\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.profiler import profile, ProfilerActivity\n",
    "\n",
    "# Specify the GPU device you want to use\n",
    "device = \"cuda:0\"  # Change this to your preferred GPU\n",
    "\n",
    "# Initialize NVML for power measurement\n",
    "def initialize_nvml():\n",
    "    pynvml.nvmlInit()\n",
    "\n",
    "def shutdown_nvml():\n",
    "    pynvml.nvmlShutdown()\n",
    "\n",
    "def get_gpu_handle(gpu_index=0):\n",
    "    return pynvml.nvmlDeviceGetHandleByIndex(gpu_index)\n",
    "\n",
    "def start_power_monitoring(handle, interval_sec=0.1):\n",
    "    power_readings = []\n",
    "    running = True\n",
    "\n",
    "    def monitor():\n",
    "        while running:\n",
    "            power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0  # Convert from mW to W\n",
    "            timestamp = time.time()\n",
    "            power_readings.append((timestamp, power))\n",
    "            time.sleep(interval_sec)\n",
    "\n",
    "    thread = threading.Thread(target=monitor)\n",
    "    thread.start()\n",
    "\n",
    "    def stop():\n",
    "        nonlocal running\n",
    "        running = False\n",
    "        thread.join()\n",
    "\n",
    "    return power_readings, stop\n",
    "\n",
    "\n",
    "# Measure energy consumed during inference and FLOPs\n",
    "def measure_energy_during_inference(handle, inference_function, model, inputs, max_new_tokens=50):\n",
    "    # Start power monitoring\n",
    "    power_readings, stop_monitoring = start_power_monitoring(handle, interval_sec=0.05)\n",
    "    \n",
    "    \n",
    "    # Start time for inference\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Measure FLOPs using PyTorch profiler\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], with_flops=True,record_shapes=False) as prof:\n",
    "        with torch.no_grad():\n",
    "            result = inference_function(inputs['input_ids'], max_new_tokens=max_new_tokens, do_sample=False )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Stop power monitoring\n",
    "    stop_monitoring()\n",
    "\n",
    "    # Filter power readings during inference\n",
    "    power_during_inference = [p for t, p in power_readings if start_time <= t <= end_time]\n",
    "\n",
    "    \n",
    "    # Calculate average power and energy consumed\n",
    "    if power_during_inference:\n",
    "        avg_power = sum(power_during_inference) / len(power_during_inference)\n",
    "        elapsed_time = end_time - start_time\n",
    "        energy_consumed = avg_power * elapsed_time\n",
    "    else:\n",
    "        avg_power = 0\n",
    "        energy_consumed = 0\n",
    "        elapsed_time = end_time - start_time\n",
    "    #print(\"prof keys flops table\")\n",
    "    #print(prof.key_averages().table(sort_by=\"flops\", row_limit=10)) \n",
    "    # Calculate FLOPs\n",
    "    flops = sum([event.flops for event in prof.key_averages() if event.flops is not None])\n",
    "\n",
    "    return energy_consumed, elapsed_time, flops, result\n",
    "\n",
    "# Measure energy consumed during inference and FLOPs\n",
    "\n",
    "\n",
    "# Calculate perplexity for generated text\n",
    "def calculate_perplexity(model, input_text, tokenizer):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)  # Ensure input is on the same device\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "        perplexity = torch.exp(loss)\n",
    "    return perplexity.item()\n",
    "\n",
    "# Run the experiment for a list of texts\n",
    "def run_experiment_for_texts(texts, bootstrapping, handle, model, tokenizer):\n",
    "    latencies = []\n",
    "    energy_per_token = []\n",
    "    energy_per_flops = []\n",
    "    energy_per_task = []\n",
    "    throughputs = []\n",
    "    generated_texts = []\n",
    "    perplexities = []\n",
    "\n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\").to(device)  # Ensure input is on the same device\n",
    "        text_latencies = []\n",
    "        text_energy_per_token = []\n",
    "        text_energy_per_flops = []\n",
    "        text_energy_per_task = []\n",
    "        text_throughput = []\n",
    "        text_generated = []\n",
    "        text_perplexities = []\n",
    "\n",
    "        for _ in range(bootstrapping):\n",
    "            energy_consumed, latency, flops, output = measure_energy_during_inference(\n",
    "                handle, model.generate, model, inputs, max_new_tokens=200\n",
    "            )\n",
    "            text_latencies.append(latency)\n",
    "\n",
    "            #print(\"output:\", output)\n",
    "            output_tokens = output.size(-1)\n",
    "            energy_token = energy_consumed / output_tokens if output_tokens > 0 else 0\n",
    "            text_energy_per_token.append(energy_token)\n",
    "\n",
    "            # Energy per FLOPs calculation\n",
    "\n",
    "            print(\"text_energy_per_token:\", text_energy_per_token)\n",
    "            print(\"output_tokens:\", output_tokens)\n",
    "            print(\"flop:\", flops)\n",
    "            print(\"energy_consumed: \",energy_consumed)\n",
    "            energy_flop = energy_consumed / flops #if flops > 0 else 0\n",
    "            text_energy_per_flops.append(energy_flop)\n",
    "\n",
    "            # Energy per task (full inference energy)\n",
    "            text_energy_per_task.append(energy_consumed)\n",
    "\n",
    "            throughput = output_tokens / latency\n",
    "            text_throughput.append(throughput)\n",
    "\n",
    "            perplexity = calculate_perplexity(model, text, tokenizer)\n",
    "            text_perplexities.append(perplexity)\n",
    "\n",
    "            generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            filtered_generated_text = generated_text.replace(text, \"\").strip()\n",
    "            text_generated.append(filtered_generated_text)\n",
    "\n",
    "        latencies.append(text_latencies)\n",
    "        energy_per_token.append(text_energy_per_token)\n",
    "        energy_per_flops.append(text_energy_per_flops)\n",
    "        energy_per_task.append(text_energy_per_task)\n",
    "        throughputs.append(text_throughput)\n",
    "        generated_texts.append(text_generated)\n",
    "        perplexities.append(text_perplexities)\n",
    "\n",
    "    return latencies, energy_per_token, energy_per_flops, energy_per_task, throughputs, generated_texts, perplexities\n",
    "\n",
    "# Collect metrics for each category\n",
    "def collect_metrics_for_categories(df, categories, bootstrapping, model, tokenizer):\n",
    "    category_metrics = {}\n",
    "    handle = get_gpu_handle(gpu_index=0)\n",
    "\n",
    "    for category in categories:\n",
    "        print(f\"Processing category: {category}\")\n",
    "        texts = filter_texts_by_category(df, category)\n",
    "        latencies, energy_per_token, energy_per_flops, energy_per_task, throughputs, generated_texts, perplexities = run_experiment_for_texts(\n",
    "            texts, bootstrapping, handle, model, tokenizer\n",
    "        )\n",
    "\n",
    "        category_metrics[category] = {\n",
    "            \"latencies\": latencies,\n",
    "            \"energy_per_token\": energy_per_token,\n",
    "            \"energy_per_flops\": energy_per_flops,\n",
    "            \"energy_per_task\": energy_per_task,\n",
    "            \"throughput\": throughputs,\n",
    "            \"generated_texts\": generated_texts,\n",
    "            \"perplexities\": perplexities\n",
    "        }\n",
    "\n",
    "    shutdown_nvml()  \n",
    "    return category_metrics\n",
    "\n",
    "def filter_texts_by_category(df, category):\n",
    "    return df[df['category'] == category]['text'].values\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Example Usage\n",
    "file_path = \"./question.jsonl\"\n",
    "bootstrapping = 4\n",
    "max_new_tokens = 50\n",
    "df_mtconversation = load_dataset(file_path)\n",
    "\n",
    "#categories = [ 'common-sense']\n",
    "categories = ['generic', 'knowledge', 'roleplay', 'common-sense', 'fermi',\n",
    "       'counterfactual', 'coding', 'math', 'writing']\n",
    "\n",
    "initialize_nvml()\n",
    "\n",
    "# HF Access Token\n",
    "access_token = \"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\"\n",
    "\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = ['facebook/opt-125m'\n",
    "            #\"meta-llama/Llama-3.1-8B\" \n",
    "            #\"meta-llama/Llama-3.1-8B\"  \n",
    "            #\"facebook/opt-125m\"\n",
    "            #\"tiiuae/falcon-7b\"\n",
    "            #\"ProbeMedicalYonseiMAILab/medllama3-v20\"\n",
    "            #\"NTQAI/Nxcode-CQ-7B-orpo\"\n",
    "            #\"MathLLMs/MathCoder-L-7B\"\n",
    "        ]\n",
    "\n",
    "counter = 0\n",
    "allmetrics = []\n",
    "\n",
    "for models in model_name:\n",
    "    #model = AutoModelForCausalLM.from_pretrained(models, use_auth_token=access_token)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(models, device_map=\"auto\", use_auth_token=access_token)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(models, use_auth_token=access_token)\n",
    "    metrics = collect_metrics_for_categories(df_mtconversation, categories, bootstrapping, model, tokenizer)\n",
    "    allmetrics.append(metrics)\n",
    "        \n",
    "# (Optionally, you can visualize the collected metrics here)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the metrics\n",
    "def plot_metrics(metrics, categories):\n",
    "    num_categories = len(categories)\n",
    "\n",
    "    # Prepare the data for plotting\n",
    "    avg_latencies = []\n",
    "    avg_perplexities = []\n",
    "    avg_energy_per_flops = []\n",
    "    avg_energy_per_token = []\n",
    "    avg_energy_per_task = []\n",
    "\n",
    "    for category in categories:\n",
    "        if category in metrics:\n",
    "            avg_latencies.append(np.mean(metrics[category][\"latencies\"]))\n",
    "            avg_perplexities.append(np.mean(metrics[category][\"perplexities\"]))\n",
    "            avg_energy_per_flops.append(np.mean(metrics[category][\"energy_per_flops\"]))\n",
    "            avg_energy_per_token.append(np.mean(metrics[category][\"energy_per_token\"]))\n",
    "            avg_energy_per_task.append(np.mean(metrics[category][\"energy_per_task\"]))\n",
    "        else:\n",
    "            avg_latencies.append(0)\n",
    "            avg_perplexities.append(0)\n",
    "            avg_energy_per_flops.append(0)\n",
    "            avg_energy_per_token.append(0)\n",
    "            avg_energy_per_task.append(0)\n",
    "\n",
    "    x = np.arange(num_categories)  # the label locations\n",
    "    width = 0.15  # the width of the bars\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "    # Plot latencies\n",
    "    bars1 = ax1.bar(x - 2*width, avg_latencies, width, label='Average Latency (s)', color='b')\n",
    "    ax1.set_ylabel('Average Latency (s)', color='b')\n",
    "    ax1.tick_params(axis='y', labelcolor='b')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(categories)\n",
    "\n",
    "    # Create a second y-axis for perplexities\n",
    "    ax2 = ax1.twinx()\n",
    "    bars2 = ax2.bar(x - width, avg_perplexities, width, label='Average Perplexity', color='g')\n",
    "    ax2.set_ylabel('Average Perplexity', color='g')\n",
    "    ax2.tick_params(axis='y', labelcolor='g')\n",
    "\n",
    "    # Create a third y-axis for energy per FLOPs\n",
    "    ax3 = ax1.twinx()\n",
    "    bars3 = ax3.bar(x, avg_energy_per_flops, width, label='Energy per FLOP (Joules)', color='r')\n",
    "    ax3.spines['right'].set_position(('outward', 60))  # move the third y-axis to the right\n",
    "    ax3.set_ylabel('Energy per FLOP (Joules)', color='r')\n",
    "    ax3.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "    # Create a fourth y-axis for energy per token\n",
    "    ax4 = ax1.twinx()\n",
    "    bars4 = ax4.bar(x + width, avg_energy_per_token, width, label='Energy per Token (Joules)', color='purple')\n",
    "    ax4.spines['right'].set_position(('outward', 120))  # move the fourth y-axis to the right\n",
    "    ax4.set_ylabel('Energy per Token (Joules)', color='purple')\n",
    "    ax4.tick_params(axis='y', labelcolor='purple')\n",
    "\n",
    "\n",
    "plot_metrics(metrics, categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total Dataframe in a row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
